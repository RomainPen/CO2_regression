{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "155e905a-c234-4036-a367-b287e9f20abf",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1/ Import packages and data :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8306503e-ef2e-45ff-b969-bf43875eef1a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Import package "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d385014-50f5-4d5b-9b0d-8df47c3a4759",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option(\"display.min_rows\", 10)\n",
    "pd.set_option(\"display.max_column\", 1000)\n",
    "import os\n",
    "from unidecode import unidecode\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import chi2_contingency, spearmanr\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.anova import anova_lm\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import RFE, RFECV\n",
    "import sys\n",
    "from pre_processing import pre_processing\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "#sys.path.append(\"pre_processing.py\")\n",
    "from statistics import mean\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import catboost\n",
    "from catboost import CatBoostRegressor\n",
    "import optuna\n",
    "import xgboost\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "import joblib\n",
    "import yaml\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35125d7d-e8d7-4459-ab0a-d201620398a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6512bd-ed4c-4912-800a-3ede420a7fb7",
   "metadata": {},
   "source": [
    "## Collect data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba7151d-2ff2-4050-9a44-a9a4f87453b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/train.csv\", encoding=\"utf-8\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5c5bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename col : if necessary\n",
    "# df.columns = [name.split(\" \")[0] for name in df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616df0cd-1ed5-4255-bb41-0058ec097d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target : Ewltp (g/km)\n",
    "target = \"Ewltp (g/km)\""
   ]
  },
  {
   "cell_type": "raw",
   "id": "e4d25b19-0193-4593-aef6-b54ad0c5fb4c",
   "metadata": {},
   "source": [
    "Suite à la selection de var, on s'attend à ce que : \n",
    "Tan <=> [Mp, Mh, Man, MMS, T, Va, Mk, Cn, Ct, Cr, r, Enedc (g/km), W (mm), At1 (mm), At2 (mm), Ft, Fm, ec (cm3), ep (KW), z (Wh/km), IT, Ernedc (g/km), Erwltp (g/km), De, Vf, Electric range (km)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce34ddd-fc00-4049-8971-cc8b3f2c23f1",
   "metadata": {},
   "source": [
    "# 2/ Data cleaning :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc40c393-053c-47db-9a4d-3f66731a3d5c",
   "metadata": {},
   "source": [
    "## Basic treatment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504fd26d-7eb1-4400-b492-a9b906e633b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect useless col \n",
    "\n",
    "## Detect useless col after empirical analysis :\n",
    "useless_columns = [\"ID\", \"Vf\" ,\"De\", \"Ernedc (g/km)\", \"MMS\", \"Mp\", \"Mk\", \"Man\", \"Cn\", \"Date of registration\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b265b242-7566-4a59-8601-5f5c63745e87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Detect columns that contain only one value :\n",
    "for i in list(df.columns) :\n",
    "    if df[i].nunique()==1 :\n",
    "        useless_columns.append(i)\n",
    "        \n",
    "useless_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73d60181-89c3-4584-8c0b-5d606f6811cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def basic_treatment(df :pd.DataFrame, useless_columns : list, drop_duplicate=True) -> pd.DataFrame:\n",
    "    \"\"\"Perform basic data treatment on a DataFrame.\n",
    "\n",
    "        This function performs basic data treatment on a given DataFrame:\n",
    "        1. Removes duplicate rows.\n",
    "        2. Drops specified useless columns.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): The input DataFrame to be treated.\n",
    "            useless_columns (list): A list of column names to be removed from the DataFrame.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: A DataFrame with duplicate rows removed, specified columns dropped,\n",
    "            and string values converted to lowercase.\n",
    "    \"\"\"\n",
    "    \n",
    "    # step 1: drop duplicate\n",
    "    if drop_duplicate==True :\n",
    "        df = df.drop_duplicates()\n",
    "        df = df.reset_index(drop = True)\n",
    "\n",
    "    # step 2 : drop useless col \n",
    "    df = df.drop(useless_columns, axis=1)\n",
    "\n",
    "    # step 3 : lowercase caracter \n",
    "    df = df.applymap(lambda s : s.lower() if (type(s) == str and pd.isna(s)==False) else s) #map or applymap\n",
    "\n",
    "    # step 4 : drop white space \n",
    "    df = df.applymap(lambda s : s.strip() if (isinstance(s, str) and pd.isna(s)==False) else s) \n",
    "\n",
    "    # step 5 : drop multiple(double, triple) space \n",
    "    df = df.applymap(lambda s:s.replace(\"  \", \" \") if (isinstance(s, str) and pd.isna(s)==False) else s) \n",
    "\n",
    "    # step 6 : replace \" \" by \"_\" \n",
    "    df = df.applymap(lambda s:s.replace(\" \", \"_\") if (isinstance(s, str) and pd.isna(s)==False) else s) \n",
    "\n",
    "    # step 7 : remove accent \n",
    "    df = df.applymap(lambda s: unidecode(s) if (isinstance(s, str) and pd.isna(s)==False) else s) \n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533d9b32-52ba-478f-add8-1da548bff5f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df = basic_treatment(df=df, useless_columns=useless_columns, drop_duplicate=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a94a3f4-0f5c-4657-a32e-2b21f220db3e",
   "metadata": {},
   "source": [
    "## Data filter (useless)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3e5f6a-650b-4f48-b21b-6d3556dc93bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "62a770ab-e1b4-45b4-921d-c4c515f75e26",
   "metadata": {},
   "source": [
    "## Data transformation (useless)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5401847-e73d-41eb-9dbb-5623bc3162b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4108c7df-a238-41ac-bc5a-8ca776bb1010",
   "metadata": {},
   "source": [
    "## Check and change type of columns (all good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a6d9aa-28f5-49c0-bf15-7f617c54e8c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in df.columns :\n",
    "    print(f'{i} : {df[i].dtypes}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48184ba2-5f7e-47b0-8c41-021721481888",
   "metadata": {},
   "source": [
    "## Check and handle abnormal features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4bb2d5-02a6-4894-a9e5-6ae43c118490",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Analyze abnormal values (simple):\n",
    "\n",
    "#Num col :\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9fa39b-f4aa-4584-ad50-7e4db3fab979",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cat col : \n",
    "for i in df.columns :\n",
    "    if df[i].dtypes == object:\n",
    "        print(f'{i} : {df[i].unique()} \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bed591-40d6-4229-86ab-434f0bf8da43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treat anomalies (simple) :\n",
    "# Num : No anormal values\n",
    "# Cat : No anormal values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0011457a-3f29-4723-ae72-896e0b741bb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c44ea4-c033-43dc-a717-4c0d0919f2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze and treat abnormal values (deeper) :\n",
    "# Analyser et vérifier les incoherences dans la base train et test (Voir si les incoherences sont présentes dans les 2 bases ou pas :\n",
    "\n",
    "\n",
    "# Verif incoherance : \"Fuel consumption \" \n",
    "# Solution : Supprimer les lignes incohérentes, car il y avait des voitures électrique qui consommaient du Fuel, ce qui est incohérent.\n",
    "df = df[~((df['Ft'] == 'electric') & (df['Fuel consumption '].notna()))]\n",
    "df = df.reset_index(drop = True)\n",
    "\n",
    "# Verif incoherance : \"z (Wh/km)\" \n",
    "# Solution : Corriger les lignes incohérentes. Ajouter \"/electric\" à la col \"Ft\" si : z (Wh/km) != NaN, Fuel consumption != NaN, et Ft ne contient pas \"electric\"\n",
    "df.loc[(df['z (Wh/km)'].notna()) & (df['Fuel consumption '].notna() & ~(df[\"Ft\"].str.contains(\"electric\"))), \"Ft\"] += \"/electric\"\n",
    "\n",
    "# Verif incoherance : \"Electric range (km)\" \n",
    "# Solution : Corriger les lignes incohérentes. Même solution que pour \"z (Wh/km)\"\n",
    "# PAS DE CODE A FAIRE\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d38dee-a0f2-4629-b003-e4a81ba69f2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d14e9b-2c93-4e53-b3ac-021bf395a3e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#temp :\n",
    "# Save df :\n",
    "df.to_pickle(\"./data/train_clean.pkl\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9c7f0c32-cdc3-467a-a129-2d6e1b3ff6fc",
   "metadata": {
    "tags": []
   },
   "source": [
    "#temp :\n",
    "df = pd.read_pickle(\"./data/train_clean.pkl\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37883156-88fd-487c-b10e-aa1755067ace",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "babcf39f-f94d-4191-a70e-d416f633afc5",
   "metadata": {},
   "source": [
    "## Check and handle missing features (NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f48422-408f-4c6d-8a76-e632d4ba752d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Analyze Non-NaN :\n",
    "# Define the possible groupby variables : ['Cr', 'Ft', 'Mh']\n",
    "# Before choosing the groupby col, Check if the groupby var has not NaN in x_test.\n",
    "\n",
    "def analyse_non_nan(df : pd.DataFrame):\n",
    "    \"\"\"Analyse the columns that contains NaN value\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): database\n",
    "        \n",
    "    Print : \"column name\" : \"type of column\" | Number of NaN : \"nb\"\n",
    "    \"\"\"\n",
    "    for i in df.columns :\n",
    "        if df[i].isna().any()==False :\n",
    "            print(f'{i} : {df[i].dtypes}')\n",
    "\n",
    "analyse_non_nan(df=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48587acf-3e80-4b3c-8c6f-127a169c3144",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Analyze NaN :\n",
    "def analyse_nan(df : pd.DataFrame):\n",
    "    \"\"\"Analyse the columns that contains NaN value\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): database\n",
    "        \n",
    "    Print : \"column name\" : \"type of column\" | Number of NaN : \"nb\"\n",
    "    \"\"\"\n",
    "    for i in df.columns :\n",
    "        if df[i].isna().any() :\n",
    "            print(f'{i} : {df[i].dtypes} | Number of NaN : {df[i].isna().sum()}')\n",
    "\n",
    "analyse_nan(df=df)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d126b5be-3bbb-4700-a736-5558f1533fcf",
   "metadata": {},
   "source": [
    "Do for all columns, even if there is no NaN (except the groupby col) :\n",
    "\n",
    "Groupby col : \n",
    "\"Ft\"\n",
    "\n",
    "Col to drop : (good)\n",
    "Enedc (g/km)\n",
    "Erwltp (g/km)\n",
    "\n",
    "Imputation method fix : (good)\n",
    "\"Country\" : \"unknown\",\n",
    "\"z (Wh/km)\" : 0 if \"Ft\"!= \"electric\"/\"hybrid\",\n",
    "\"Fuel consumption \": 0 if \"Ft\"== \"electric\",\n",
    "\"Electric range (km)\": 0 if \"Ft\"!= \"electric\"/\"hybrid\",\n",
    "\n",
    "Imputation method non-fix :\n",
    "\"VFN\" : mode,\n",
    "\"Tan\" : mode,\n",
    "\"T\" : mode,\n",
    "\"Va\" : mode,\n",
    "\"Ve\" : mode,\n",
    "\"Ct\" : mode,\n",
    "\"m (kg)\" : median,\n",
    "\"Mt\" : median,\n",
    "\"W (mm)\": median,\n",
    "\"At1 (mm)\": median,\n",
    "\"At2 (mm)\": median,\n",
    "\"Fm \" : mode,\n",
    "\"ec (cm3)\": median,\n",
    "\"ep (KW)\": median,\n",
    "\"z (Wh/km)\" : median if \"Ft\"== \"electric\"/\"hybrid\",\n",
    "\"IT\": mode,\n",
    "\"Fuel consumption \": median if \"Ft\"!= \"electric\",\n",
    "\"Electric range (km)\": median if \"Ft\"== \"electric\"/\"hybrid\"\n",
    "'Cr':\"mode\" \n",
    "'Mh':\"mode\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845a89c3-21a0-4e5b-b703-7b31b27dea1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Delete columns which contains more than 50% of NaN or useless :\n",
    "\n",
    "Col_to_drop = [\"Enedc (g/km)\", \"Erwltp (g/km)\"]\n",
    "df = df.drop(Col_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e50575a-e690-403c-82bc-d371b3d11024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute by fix value :\n",
    "# df = df.dropna(subset=[\"\"]).reset_index(drop=True)\n",
    "# df[\"\"] = df[\"\"].fillna()\n",
    "\n",
    "\n",
    "def fillna_fix_value(df, fillna_value):\n",
    "    \n",
    "    # Recreate the new dict :\n",
    "    fillna_value = {key: value for key, value in fillna_value.items() if key in list(df.columns)}\n",
    "    \n",
    "    for i in list(fillna_value.keys()) :\n",
    "        # Cat col :\n",
    "        if df[i].dtypes==object :\n",
    "            if type(fillna_value[i])==str :\n",
    "                df[i] = df[i].fillna(fillna_value[i])\n",
    "            else :\n",
    "                print(f\"{i} must be a 'str' !\")\n",
    "                break\n",
    "        \n",
    "        # Num col :\n",
    "        elif df[i].dtypes==float or df[i].dtypes==int :\n",
    "            if type(fillna_value[i])==float or type(fillna_value[i])==int :\n",
    "                if i==\"z (Wh/km)\":\n",
    "                    # \"z (Wh/km)\" : 0 if \"Ft\"!= \"electric\"/\"hybrid\"('petrol/electric', 'diesel/electric') :\n",
    "                    df.loc[~((df[\"Ft\"].str.contains(\"electric\"))),i] = df.loc[~((df[\"Ft\"].str.contains(\"electric\"))),i].fillna(fillna_value[i])\n",
    "                \n",
    "                elif i==\"Fuel consumption \":\n",
    "                    # \"Fuel consumption \"= 0 if \"Ft\"= \"electric\" :\n",
    "                    df.loc[df[\"Ft\"]==\"electric\",i] = df.loc[df[\"Ft\"]==\"electric\",i].fillna(fillna_value[i])\n",
    "                \n",
    "                else :    \n",
    "                    df[i] = df[i].fillna(fillna_value[i])\n",
    "            else :\n",
    "                print(f\"{i} must be a 'float' or 'int' !\")\n",
    "                break\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5c5590-f0ff-4bfd-afbe-b4ea0aab9fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionnary of imputationby fix value (Num and Cat) :\n",
    "dict_imputation_fix = {\"Country\" : \"unknown\", \"z (Wh/km)\": 0,\n",
    "                       \"Fuel consumption \": 0,\"Electric range (km)\": 0}\n",
    "\n",
    "df = fillna_fix_value(df=df, fillna_value=dict_imputation_fix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9a1693-d62d-40dc-b174-43c4f3fd1bb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "939e15ac-01ed-46ae-bd45-9733dcac529c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Impute by non-fix value (create the function for the next imputation): \n",
    "\n",
    "def fillna_non_fix(x_train, x_test, fillna_method, groupby_col, display_groupby_col=False):\n",
    "    \"\"\"Suppose that the columns name are the same in train and test.\n",
    "        Suppose that the groupby col of the x_train has not NaN value.\n",
    "\n",
    "    Args:\n",
    "        x_train (_type_): _description_\n",
    "        x_test (_type_): _description_\n",
    "        fillna_method (_type_): _description_\n",
    "        groupby_col (_type_): _description_\n",
    "        groupby_col_comparison (bool, optional): _description_. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"    \n",
    "    # Recreate the new dict :\n",
    "    fillna_method = {key: value for key, value in fillna_method.items() if key in list(x_train.columns)}\n",
    "    \n",
    "    # Check values of keys :\n",
    "    for i in list(fillna_method.keys()) :  \n",
    "        if fillna_method[i] not in [\"mode\",\"median\",\"mean\"] :\n",
    "            print (f\"{i} must be imputed by mean, median or mean\")\n",
    "            return x_train, x_test\n",
    "\n",
    "    # While loop :\n",
    "    j = len(groupby_col)  \n",
    "    while j>=0:\n",
    "        \n",
    "        if j > 0:\n",
    "            try:\n",
    "                for i in list(fillna_method.keys()) : \n",
    "                    grouped = x_train.groupby(groupby_col[:j])\n",
    "                    \n",
    "                    if fillna_method[i]==\"mode\" : \n",
    "                        mode = grouped[i].agg(lambda x: x.mode().iloc[0] if not x.mode().empty else np.nan)\n",
    "                    elif fillna_method[i]==\"median\" : \n",
    "                        mode = grouped[i].median()\n",
    "                    elif fillna_method[i]==\"mean\" : \n",
    "                        mode = grouped[i].mean() \n",
    "                    \n",
    "                    x_test[i] = x_test.groupby(groupby_col[:j])[i].transform(lambda x: x.fillna(mode[x.name]))\n",
    "                    x_train[i] = x_train.groupby(groupby_col[:j])[i].transform(lambda x: x.fillna(mode[x.name]))\n",
    "                    \n",
    "                    if x_train[i].isna().any()==True or x_test[i].isna().any()==True :\n",
    "                        if fillna_method[i]==\"mode\" : \n",
    "                            mode = x_train[i].mode()[0]\n",
    "                        elif fillna_method[i]==\"median\" : \n",
    "                            mode = x_train[i].median()   \n",
    "                        elif fillna_method[i]==\"mean\" : \n",
    "                            mode = x_train[i].mean() \n",
    "\n",
    "                        x_test[i] = x_test[i].fillna(mode)\n",
    "                        x_train[i] = x_train[i].fillna(mode)\n",
    "                        \n",
    "                \n",
    "                if display_groupby_col==True:\n",
    "                    print(groupby_col[:j])\n",
    "                \n",
    "                j=-1\n",
    "                \n",
    "            except :\n",
    "                j-=1\n",
    "    \n",
    "        elif j==0: \n",
    "            for i in list(fillna_method.keys()) :     \n",
    "                if fillna_method[i]==\"mode\" : \n",
    "                    mode = x_train[i].mode()[0]\n",
    "                elif fillna_method[i]==\"median\" : \n",
    "                    mode = x_train[i].median()   \n",
    "                elif fillna_method[i]==\"mean\" : \n",
    "                    mode = x_train[i].mean() \n",
    "        \n",
    "                x_test[i] = x_test[i].fillna(mode)\n",
    "                x_train[i] = x_train[i].fillna(mode)\n",
    "            \n",
    "            j=-1\n",
    "        \n",
    "    return x_train, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f1ecaf-ee61-452b-ab08-8abd9f6d16d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#* Pour le moment, faire seulement groupby 1 variable :\n",
    "\n",
    "# Avant de selectionner la col pour le groubpy, on doit verifier en 2 etapes :\n",
    "\"\"\"\n",
    "col_for_groupby = \"Ft\"\n",
    "# 1/ Verifier les valeurs .unique() avant de choisir le variable groupby :\n",
    "for i in [x_val1, x_val2, x_test]:\n",
    "    if all(element in set(x_train[col_for_groupby].unique()) for element in set(i[col_for_groupby].unique()))== False:\n",
    "        print(f\"Cannot use the column {col_for_groupby} for groupby\")\n",
    "        break\n",
    "    else :\n",
    "        print(all(element in set(x_train[col_for_groupby].unique()) for element in set(i[col_for_groupby].unique())))\n",
    "\n",
    "# 2/ Verifier si la variable ne possède pas de NaN \n",
    "for i in [x_train, x_val1, x_val2, x_test]:\n",
    "    if i[col_for_groupby].isna().any()== True:\n",
    "        print(f\"Cannot use the column {col_for_groupby} for groupby\")\n",
    "        break\n",
    "    else :\n",
    "        print(i[col_for_groupby].isna().any())\n",
    "\"\"\"\n",
    "\n",
    "# Groupby_col : ['Ft'], []\n",
    "# Attention : Before choosing the Groupby variable. \n",
    "# Verify for each variable, if :\n",
    "# - Categorical\n",
    "# - Number of unique value is <= 30\n",
    "# Verify for each groupby combinaison, if :\n",
    "# - The number of unique combinaison is <= 80\n",
    "# - Same unique value in train, val1, val2, test\n",
    "# - Same unique combinaison in val1, val2, test\n",
    "\n",
    "# Verifier si l'imputation train/val est la même que train/val2 et que train/test. \n",
    "# voir si ca impute bien avec les mêmes combinaisons de groupby.\n",
    "\n",
    "\n",
    "\n",
    "# Fillna all groupby(Ft) :\n",
    "groupby_col = ['Ft']\n",
    "dict_imputation_non_fix = {\"VFN\": \"mode\", \"T\": \"mode\", \"Tan\": \"mode\",\n",
    "                            \"Va\": \"mode\",\"Ve\": \"mode\",\"Ct\": \"mode\",\n",
    "                            \"m (kg)\": \"median\",\"Mt\": \"median\",\"W (mm)\": \"median\",\n",
    "                            \"At1 (mm)\": \"median\",\"At2 (mm)\": \"median\",\"Fm\": \"mode\",\n",
    "                            \"ec (cm3)\": \"median\",\"ep (KW)\": \"median\", \"z (Wh/km)\" : \"median\",\n",
    "                            \"IT\": \"mode\",\"Fuel consumption \": \"median\", \"Electric range (km)\":\"median\", 'Cr':\"mode\", 'Mh':\"mode\"}\n",
    "\n",
    "#x_train, x_test = fillna_non_fix(x_train=x_train, x_test=x_test, fillna_method=dict_imputation_non_fix, groupby_col=groupby_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c396723-76ea-403a-bc9a-9861744fdf91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "921cb38f-10e8-4aff-b428-57a97aa6e1e8",
   "metadata": {},
   "source": [
    "# 3/ Split data as train/val :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d5f792-603b-4ab6-9ec0-ad5cec95fe27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Split :\n",
    "\n",
    "num_bins = 10  # Number of bins for stratification\n",
    "y_bins = pd.cut(df[target], bins=num_bins, labels=False)\n",
    "df_train, df_val = train_test_split(df, test_size=0.25, stratify=y_bins, random_state=42)\n",
    "\n",
    "#reset_index :\n",
    "df_train.reset_index(drop = True, inplace=True)\n",
    "df_val.reset_index(drop = True, inplace=True)\n",
    "\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378480dd-5efd-495d-b079-1aae9e592963",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "53ccc1ee-789e-4f49-a5cd-9f0b6f8e7b3f",
   "metadata": {},
   "source": [
    "# 4/ Feature engineering :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e43b44-edf5-4361-a78c-251e4a933e1c",
   "metadata": {},
   "source": [
    "## Feature creation (useless)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec9f63b-cead-4ce9-aeed-490553250286",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "df4dc40d-308c-49a3-acd5-d61aff6cb4bd",
   "metadata": {},
   "source": [
    "## Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6158d303-4786-48eb-b5c7-9aa9adc266b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2.1/ Drop columns used for features creation (option) \n",
    "## 2.2/ Drop columns with low variance (num) and have the same value more than 99% of time (cat)\n",
    "## 2.3/ Select with correlation method\n",
    "## 2.4/ Select with RFE method (option)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75916ce7-08e8-4405-962e-fcc575aae95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1/ Drop columns used for features creation (option) : useless\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039bcad1-06ed-454d-9484-562296f665e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 2.2/ Drop columns with low variance (num) and have the same value more than 99.5% of time (cat) :\n",
    "\n",
    "def drop_col_with_same_value(df_train, df_test, target):\n",
    "    col_to_drop = [col for col in df_train.columns if (df_train[col].value_counts().iloc[0]/df_train.shape[0] >= 0.995 and col !=target)]\n",
    "    df_train = df_train.drop(col_to_drop, axis=1)\n",
    "    df_test = df_test.drop(col_to_drop, axis=1)  \n",
    "    return df_train, df_test\n",
    "\n",
    "df_train , df_val = drop_col_with_same_value(df_train=df_train, df_test=df_val, target=target)\n",
    "\n",
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7a5fa8-7f7d-4b4f-8627-388be41e5873",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c123d7-e338-4cc2-961a-f215de5c3be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3/ Select with correlation method : "
   ]
  },
  {
   "cell_type": "raw",
   "id": "bc4a9b86-7d39-4e2e-b084-f25c6c3f918f",
   "metadata": {},
   "source": [
    "Correlation computing\n",
    "\n",
    "# Step 1 : Compute corr\n",
    "Num-Num :\n",
    "- Test de Spearman\n",
    "\n",
    "Cat-Cat :\n",
    "- Cramer-V\n",
    "\n",
    "Num-Cat :\n",
    "- Eta carré (η²) ou coefficient de contingence (fonctionne que si la taille de l'échantillon est très élevé)\n",
    "- Test de Kruskal-Wallis H Test (si p-value < 0.05, alors il y a une correlation significative)\n",
    "\n",
    "\n",
    "\n",
    "# Step 2 : Select features\n",
    "Si : \n",
    "- Regression : alors, Target = Num\n",
    "- Classification : alors, Target = Cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7637a8c3-49c2-4846-927f-1f7d52baef72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate Cramer's V\n",
    "def cramers_v(x, y):\n",
    "    \n",
    "    # Créer un tableau de contingence\n",
    "    contingency_table = pd.crosstab(x, y)\n",
    "    \n",
    "    # Effectuer le test du chi-carré\n",
    "    chi2_stat, p_value, dof, expected_freq = chi2_contingency(contingency_table)\n",
    "    \n",
    "    # Calculer le coefficient de Cramér-V\n",
    "    n = contingency_table.sum().sum()\n",
    "    min_dim = min(contingency_table.shape) - 1\n",
    "    cramer_v = np.sqrt(chi2_stat / (n * min_dim))\n",
    "    \n",
    "    return cramer_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e4e295-504c-4474-ace2-34dd98cf5fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate ANOVA eta_carre\n",
    "\n",
    "def eta_carre(x, y) :\n",
    "    #x=Cat col\n",
    "    #y=Num col\n",
    "    \n",
    "    #Rename y serie name :\n",
    "    y = y.copy()\n",
    "    y.name = y.name.split()[0]\n",
    "    \n",
    "    # Replace \"specific character\" by \"_\" of x value :\n",
    "    # Define the characters to be replaced\n",
    "    characters_to_replace = ['-', '/', '*', '.', '?', ')', '(']\n",
    "    # Create a regex pattern to match any of the characters to be replaced\n",
    "    regex_pattern = '|'.join(map(re.escape, characters_to_replace))\n",
    "    # Apply the replacement using regex\n",
    "    x = x.apply(lambda s: re.sub(regex_pattern, '_', s) if isinstance(s, str) and not pd.isna(s) else s)\n",
    "     \n",
    "    \n",
    "    # Convertir la variable qualitative en variables indicatrices (dummies)\n",
    "    data_dummies = pd.get_dummies(x)\n",
    "    new_col_name = {}\n",
    "    for i in data_dummies.columns :\n",
    "        new_col_name[i] = f'{x.name}_{i}'\n",
    "    data_dummies = data_dummies.rename(columns=new_col_name)\n",
    "    \n",
    "    # Fusionner les données dummies avec le jeu de données original\n",
    "    data = pd.concat([y, data_dummies], axis=1)\n",
    "\n",
    "    # Modèle linéaire\n",
    "    formula = f\"{y.name} ~\"\n",
    "    for i in data_dummies.columns:\n",
    "        formula+= f' {i} +'\n",
    "    formula=formula[:-2]\n",
    "    model = ols(formula, data=data).fit()\n",
    "\n",
    "    # ANOVA\n",
    "    result = anova_lm(model, typ=2)\n",
    "\n",
    "    # Calcul de l'Eta carré\n",
    "    eta_squared = result['sum_sq'][0] / (result['sum_sq'][0] + result['sum_sq'][1])\n",
    "\n",
    "    return eta_squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d706f0-7a75-46ef-844c-be475f8c649f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Features_selection : \n",
    "# Ne pas supprimer les col du groupby : ['Ft']\n",
    "\n",
    "def Features_selection(df_train, df_test, target, prediction_type, groupby_col=groupby_col, threshold=0.8):\n",
    "    \n",
    "    # 0/ Build list for each col type :\n",
    "    list_number_col = df_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    # Num list :\n",
    "    # Numeric and > 2 :\n",
    "    list_num_col = [col for col in list_number_col if ((df_train[col].nunique() > 2) & (col!=target) & (col not in groupby_col))]\n",
    "\n",
    "    # Cat list :\n",
    "    # Numeric and <= 2 :\n",
    "    list_binary_col = [col for col in list_number_col if ((df_train[col].nunique() <= 2) & (col!=target) & (col not in groupby_col))]\n",
    "    # Categorical col :\n",
    "    list_cat_col = df_train.select_dtypes(include=['object']).columns.tolist()\n",
    "    list_cat_col = [col for col in list_cat_col if ((col!=target) & (df_train[col].nunique() <= 100) & (col not in groupby_col))] + list_binary_col\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # 1/ Num-Num :\n",
    "    print(\"Num-Num\")\n",
    "    df_train_num=df_train[list_num_col]\n",
    "    \n",
    "    # A/ corr matrix :\n",
    "    corr_matrix_num = df_train_num.corr(numeric_only=True, method=\"spearman\")\n",
    "    \n",
    "    # B/ Find pairs of columns with correlation above the threshold :\n",
    "    highly_correlated_pairs_num = []\n",
    "    for i in range(len(corr_matrix_num.columns)):\n",
    "        for j in range(i):\n",
    "            if abs(corr_matrix_num.iloc[i, j]) > threshold:\n",
    "                colname_i = corr_matrix_num.columns[i]\n",
    "                colname_j = corr_matrix_num.columns[j]\n",
    "                highly_correlated_pairs_num.append((colname_i, colname_j, corr_matrix_num.iloc[i, j]))\n",
    "    highly_correlated_pairs_num = sorted(highly_correlated_pairs_num, key=lambda x: x[2], reverse=True)\n",
    "    \n",
    "    # C/ Get correlation between explicative num and target :\n",
    "    dict_corr_target_num = {}\n",
    "    if prediction_type==\"regression\":\n",
    "        for i in list_num_col :\n",
    "            correlation, p_value = spearmanr(df_train.dropna(subset=[i]).reset_index(drop=True)[i], df_train.dropna(subset=[i]).reset_index(drop=True)[target])\n",
    "            dict_corr_target_num[i] = correlation\n",
    "    \n",
    "    elif prediction_type==\"classification\":\n",
    "        for i in list_num_col :\n",
    "            correlation = eta_carre(df_train.dropna(subset=[i]).reset_index(drop=True)[target], df_train.dropna(subset=[i]).reset_index(drop=True)[i])\n",
    "            dict_corr_target_num[i] = correlation\n",
    "    \n",
    "    # D/ Get set of col to drop :\n",
    "    col_to_drop_num = set()\n",
    "    for pair in highly_correlated_pairs_num :\n",
    "        corr_target_0 = dict_corr_target_num[pair[0]]\n",
    "        corr_target_1 = dict_corr_target_num[pair[1]]\n",
    "        \n",
    "        if corr_target_0 > corr_target_1 :\n",
    "            col_to_drop_num.add(pair[1])\n",
    "            \n",
    "        elif corr_target_0 <= corr_target_1 :\n",
    "            col_to_drop_num.add(pair[0])\n",
    "    \n",
    "    # E/ Drop num col :\n",
    "    df_train = df_train.drop(list(col_to_drop_num), axis=1)\n",
    "    df_test = df_test.drop(list(col_to_drop_num), axis=1)\n",
    "    print(list(col_to_drop_num))\n",
    "    print(\"End Num-Num\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # 2/ Cat-Cat :\n",
    "    print(\"Cat-Cat\")\n",
    "    \n",
    "    # A/ Corr matrix : \n",
    "    # List of categorical columns\n",
    "    categorical_columns = list_cat_col\n",
    "    # Initialize an empty matrix\n",
    "    cramer_matrix = np.zeros((len(categorical_columns), len(categorical_columns)))\n",
    "    corr_matrix_cat = pd.DataFrame(cramer_matrix, index=categorical_columns, columns=categorical_columns)\n",
    "    # Calculate Cramer's V for each pair of categorical columns\n",
    "    for i in range(len(categorical_columns)):\n",
    "        for j in range(i+1, len(categorical_columns)):\n",
    "            col1 = df_train.dropna(subset=[categorical_columns[i], categorical_columns[j]]).reset_index(drop=True)[categorical_columns[i]]\n",
    "            col2 = df_train.dropna(subset=[categorical_columns[i], categorical_columns[j]]).reset_index(drop=True)[categorical_columns[j]]\n",
    "            corr_matrix_cat.loc[categorical_columns[j], categorical_columns[i]] = cramers_v(col1, col2)\n",
    "    \n",
    "    # B/ Find pairs of columns with correlation above the threshold :\n",
    "    highly_correlated_pairs_cat = []\n",
    "    for i in categorical_columns :\n",
    "        for j in categorical_columns :\n",
    "            if abs(corr_matrix_cat.loc[j, i]) > threshold:\n",
    "                highly_correlated_pairs_cat.append((i, j, corr_matrix_cat.loc[j, i]))\n",
    "    highly_correlated_pairs_cat = sorted(highly_correlated_pairs_cat, key=lambda x: x[2], reverse=True)\n",
    "    \n",
    "    \n",
    "    # C/ Get correlation between explicative cat and target :\n",
    "    dict_corr_target_cat = {}\n",
    "    if prediction_type==\"regression\":\n",
    "        for i in corr_matrix_cat :\n",
    "            correlation = eta_carre(df_train.dropna(subset=[i]).reset_index(drop=True)[i], df_train.dropna(subset=[i]).reset_index(drop=True)[target])\n",
    "            dict_corr_target_cat[i] = correlation\n",
    "        \n",
    "    elif prediction_type==\"classification\":\n",
    "        for i in corr_matrix_cat :\n",
    "            correlation = cramers_v(df_train.dropna(subset=[i]).reset_index(drop=True)[i], df_train.dropna(subset=[i]).reset_index(drop=True)[target])\n",
    "            dict_corr_target_cat[i] = correlation\n",
    "      \n",
    "    # D/ Get set of col to drop :\n",
    "    col_to_drop_cat = set()\n",
    "    for pair in highly_correlated_pairs_cat :\n",
    "        corr_target_0 = dict_corr_target_cat[pair[0]]\n",
    "        corr_target_1 = dict_corr_target_cat[pair[1]]\n",
    "        \n",
    "        if corr_target_0 > corr_target_1 :\n",
    "            col_to_drop_cat.add(pair[1])\n",
    "            \n",
    "        elif corr_target_0 <= corr_target_1 :\n",
    "            col_to_drop_cat.add(pair[0])\n",
    "    \n",
    "    # E/ Drop num col :\n",
    "    df_train = df_train.drop(list(col_to_drop_cat), axis=1)\n",
    "    df_test = df_test.drop(list(col_to_drop_cat), axis=1)\n",
    "    print(list(col_to_drop_cat))\n",
    "    print(\"End Cat-Cat\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # 3/ Num-Cat : \n",
    "    print(\"Num-Cat\")\n",
    "    # A*/ Build list for each col type :\n",
    "    list_number_col = df_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    # Num list :\n",
    "    # Numeric and > 2 :\n",
    "    list_num_col = [col for col in list_number_col if ((df_train[col].nunique() > 2) & (col!=target) & (col not in groupby_col))]\n",
    "\n",
    "    # Cat list :\n",
    "    # Numeric and <= 2 :\n",
    "    list_binary_col = [col for col in list_number_col if ((df_train[col].nunique() <= 2) & (col!=target) & (col not in groupby_col))]\n",
    "    # Categorical col :\n",
    "    list_cat_col = df_train.select_dtypes(include=['object']).columns.tolist()\n",
    "    list_cat_col = [col for col in list_cat_col if ((col!=target) & (df_train[col].nunique() <= 100) & (col not in groupby_col))] + list_binary_col\n",
    "    \n",
    "\n",
    "    # A/ Corr matrix :\n",
    "    # List of categorical columns\n",
    "    numerical_columns = list_num_col\n",
    "    categorical_columns = list_cat_col\n",
    "    # Initialize an empty matrix\n",
    "    eta_matrix = np.zeros((len(numerical_columns), len(categorical_columns)))\n",
    "    # Calculate eta for each pair of num-categorical columns\n",
    "    for i in range(len(categorical_columns)):\n",
    "        for j in range(len(numerical_columns)):\n",
    "            cat_col1 = df_train.dropna(subset=[categorical_columns[i], numerical_columns[j]]).reset_index(drop=True)[categorical_columns[i]]\n",
    "            num_col2 = df_train.dropna(subset=[categorical_columns[i], numerical_columns[j]]).reset_index(drop=True)[numerical_columns[j]]\n",
    "            eta_matrix[j, i] = eta_carre(x=cat_col1, y=num_col2)  \n",
    "    # Create a DataFrame from the matrix\n",
    "    corr_matrix_num_cat = pd.DataFrame(eta_matrix, index=numerical_columns, columns=categorical_columns)\n",
    "\n",
    "    # B/ Find pairs of columns with correlation above the threshold :\n",
    "    highly_correlated_pairs_num_cat = []\n",
    "    for i in numerical_columns:\n",
    "        for j in categorical_columns:\n",
    "            if abs(corr_matrix_num_cat.loc[i, j]) > threshold:\n",
    "                highly_correlated_pairs_num_cat.append((i, j, corr_matrix_num_cat.loc[i, j]))\n",
    "    highly_correlated_pairs_num_cat = sorted(highly_correlated_pairs_num_cat, key=lambda x: x[2], reverse=True)\n",
    "    \n",
    "    # C/ Get correlation between explicative Num-cat and target :\n",
    "    dict_corr_target_num_cat = {}\n",
    "    if prediction_type==\"regression\":\n",
    "        # Num-Target :\n",
    "        for i in numerical_columns :\n",
    "            correlation, p_value = spearmanr(df_train.dropna(subset=[i]).reset_index(drop=True)[i], df_train.dropna(subset=[i]).reset_index(drop=True)[target])\n",
    "            dict_corr_target_num_cat[i] = correlation\n",
    "            \n",
    "        # Cat-Target :\n",
    "        for i in categorical_columns :\n",
    "            correlation = eta_carre(df_train.dropna(subset=[i]).reset_index(drop=True)[i], df_train.dropna(subset=[i]).reset_index(drop=True)[target])\n",
    "            dict_corr_target_num_cat[i] = correlation\n",
    "        \n",
    "    elif prediction_type==\"classification\":\n",
    "        # Num-Target :\n",
    "        for i in numerical_columns :\n",
    "            correlation = eta_carre(df_train.dropna(subset=[i]).reset_index(drop=True)[target], df_train.dropna(subset=[i]).reset_index(drop=True)[i])\n",
    "            dict_corr_target_num_cat[i] = correlation\n",
    "        \n",
    "        # Cat-Target :\n",
    "        for i in categorical_columns :\n",
    "            correlation = cramers_v(df_train.dropna(subset=[i]).reset_index(drop=True)[i], df_train.dropna(subset=[i]).reset_index(drop=True)[target])\n",
    "            dict_corr_target_num_cat[i] = correlation\n",
    "    \n",
    "    # D/ Get set of col to drop :\n",
    "    col_to_drop_num_cat = set()\n",
    "    for pair in highly_correlated_pairs_num_cat :\n",
    "        corr_target_0 = dict_corr_target_num_cat[pair[0]]\n",
    "        corr_target_1 = dict_corr_target_num_cat[pair[1]]\n",
    "        \n",
    "        if corr_target_0 > corr_target_1 :\n",
    "            col_to_drop_num_cat.add(pair[1])\n",
    "            \n",
    "        elif corr_target_0 <= corr_target_1 :\n",
    "            col_to_drop_num_cat.add(pair[0])\n",
    "    \n",
    "    # E/ Drop num col :\n",
    "    df_train = df_train.drop(list(col_to_drop_num_cat), axis=1)\n",
    "    df_test = df_test.drop(list(col_to_drop_num_cat), axis=1)\n",
    "    print(list(col_to_drop_num_cat))\n",
    "    print(\"End Num-Cat\")\n",
    "    \n",
    "    return df_train, df_test\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d01e9ed6-eccc-44fc-a479-0fc4d4065eda",
   "metadata": {
    "tags": []
   },
   "source": [
    "%%time\n",
    "\n",
    "df_train, df_val = Features_selection(df_train=df_train, df_test=df_val, target=target, prediction_type=\"regression\", groupby_col=groupby_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0a881a-f0f3-4980-9733-39a1492e68f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29d3de2-f368-40f9-b1c1-2187033d4b3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 2.4/ RFE\n",
    "\n",
    "def Feature_selection_RFE(df_train=df_train, df_test=df_val, target=target, fillna_method=dict_imputation_non_fix, groupby_col=groupby_col, display_selected_features=False) :\n",
    "    \n",
    "    # Create x, y :\n",
    "    X_train = df_train.drop([target], axis=1)\n",
    "    Y_train = df_train[target]\n",
    "    X_test = df_test.drop([target], axis=1)\n",
    "    Y_test = df_test[target]\n",
    "    \n",
    "    # fillna :\n",
    "    X_train, X_test = fillna_non_fix(x_train=X_train, x_test=X_test, fillna_method=fillna_method, groupby_col=groupby_col)\n",
    "\n",
    "    # encoding/scaling :\n",
    "    #numeric and > 2 :\n",
    "    list_cont_col = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    list_cont_col = [col for col in list_cont_col if X_train[col].nunique() > 2]\n",
    "    #numeric and <= 2 :\n",
    "    list_binary_col = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    list_binary_col = [col for col in list_binary_col if X_train[col].nunique() <= 2]\n",
    "    #categorical col :\n",
    "    list_cat_col_TE = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "    \n",
    "    pre_process = pre_processing()\n",
    "    X_train = pre_process.pre_processing(df=X_train, train=True, categorical_var_OHE=[],\n",
    "                                         categorical_var_OrdinalEncoding={}, categorical_var_TE=list_cat_col_TE, \n",
    "                                         target=Y_train, continious_var=list_cont_col, encoding_type_cont=StandardScaler())\n",
    "\n",
    "    X_test = pre_process.pre_processing(df=X_test, train=False, categorical_var_OHE=[],\n",
    "                                     categorical_var_OrdinalEncoding={}, categorical_var_TE=list_cat_col_TE, \n",
    "                                     target=Y_train, continious_var=list_cont_col, encoding_type_cont=StandardScaler())\n",
    "    \n",
    "    \n",
    "    # Features selection :\n",
    "    rfe_score = {\"nb_var\" : 1000, \"score_train\": 1000, \"best_score_test\" : 1000}\n",
    "\n",
    "    for i in tqdm(range(1,len(X_train.columns)+1)) :\n",
    "        model = LinearRegression()\n",
    "        selector = RFE(model, n_features_to_select=i, step=1)\n",
    "        selector.fit(X_train, Y_train)\n",
    "\n",
    "        X_train_new = X_train[list(selector.get_feature_names_out())]\n",
    "        X_test_new = X_test[list(selector.get_feature_names_out())]\n",
    "        model.fit(X_train_new, Y_train)\n",
    "\n",
    "        score_train = mean_absolute_error(Y_train, model.predict(X_train_new))\n",
    "        score_test = mean_absolute_error(Y_test, model.predict(X_test_new))\n",
    "        \n",
    "        if score_test < rfe_score[\"best_score_test\"] :\n",
    "            rfe_score[\"nb_var\"] = i\n",
    "            rfe_score[\"score_train\"] = score_train\n",
    "            rfe_score[\"best_score_test\"] = score_test\n",
    "            rfe_score[\"selected_features\"] = list(selector.get_feature_names_out())\n",
    "    \n",
    "    if display_selected_features==True:\n",
    "        print(f\"RFE_score : {rfe_score}\")\n",
    "    \n",
    "    df_train_new = pd.concat([df_train[rfe_score[\"selected_features\"]], df_train[target]], axis=1)\n",
    "    df_test_new = pd.concat([df_test[rfe_score[\"selected_features\"]], df_test[target]], axis=1)\n",
    "    \n",
    "    return df_train_new, df_test_new\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8622af-3f0d-4d0b-bcf7-c1a10aaf27ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df_train, df_val = Feature_selection_RFE(df_train=df_train, df_test=df_val, target=target, fillna_method=dict_imputation_non_fix, \n",
    "                                         groupby_col=groupby_col, display_selected_features=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ea587e-3321-4070-99fa-329d762840ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "display(df_train.shape)\n",
    "display(df_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0abfd0f-6a57-460e-a610-2c8f9f95d96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5adc31d-05c3-46c8-bfd0-471dd38e3cec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2b4d9c7e-e3be-4727-9892-f0f7f1968552",
   "metadata": {
    "tags": []
   },
   "source": [
    "#  __________________________________ Beginning : ________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4a64e6-ff30-42f9-b83a-963a92aacb82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "3a3830d0-42c7-403c-bad6-fdf04e41a031",
   "metadata": {
    "tags": []
   },
   "source": [
    "#temp :\n",
    "# Save df :\n",
    "df_train.to_pickle(\"./data/df_train.pkl\")\n",
    "df_val.to_pickle(\"./data/df_val.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ee22f1f-ec5b-4c2c-866c-71669b1cbf32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#temp :\n",
    "df_train = pd.read_pickle(\"./data/df_train.pkl\")\n",
    "df_val = pd.read_pickle(\"./data/df_val.pkl\")\n",
    "\n",
    "target = \"Ewltp (g/km)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a7053c-d085-4062-8e60-e71801bb7a1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1a753857-9f3a-4268-849d-091dc2352a6e",
   "metadata": {},
   "source": [
    "## Row selection (useless)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5715a73b-3881-41fb-ba26-b8cd2813287d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c1671651-0553-4a0b-964b-db02dc617b06",
   "metadata": {},
   "source": [
    "## Create x and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24ee653a-fb5f-4182-93bf-b137ed51e9fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_x_y(df_train, df_test, target):\n",
    "    x_train = df_train.drop([target], axis=1).copy()\n",
    "    y_train = df_train[target].copy()\n",
    "\n",
    "    x_test = df_test.drop([target], axis=1).copy()\n",
    "    y_test = df_test[target].copy()\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "x_train, y_train, x_val, y_val = create_x_y(df_train=df_train, df_test=df_val, target=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc53a9df-66e8-4aa0-b6a0-efe74a5a8846",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1935f4f1-5ea4-4c83-93a6-a4e459722658",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 5/ Modelling :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb6d7af-7657-4201-acf8-95623d5cf4e3",
   "metadata": {},
   "source": [
    "## Display Data treatment methods before modelling "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b72f46-343b-4bcb-ae05-c5e4cad4b33e",
   "metadata": {},
   "source": [
    "#### A/ Fillna non-fix method"
   ]
  },
  {
   "cell_type": "raw",
   "id": "153a9193-83c5-42db-ab38-4c96d99f02e0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Impute by non-fix value : \n",
    "\n",
    "def fillna_non_fix(x_train, x_test, fillna_method, groupby_col, display_groupby_col=False):\n",
    "    \"\"\"Suppose that the columns name are the same in train and test.\n",
    "        Suppose that the groupby col of the x_train has not NaN value.\n",
    "\n",
    "    Args:\n",
    "        x_train (_type_): _description_\n",
    "        x_test (_type_): _description_\n",
    "        fillna_method (_type_): _description_\n",
    "        groupby_col (_type_): _description_\n",
    "        groupby_col_comparison (bool, optional): _description_. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"    \n",
    "    # Recreate the new dict :\n",
    "    fillna_method = {key: value for key, value in fillna_method.items() if key in list(x_train.columns)}\n",
    "    \n",
    "    # Check values of keys :\n",
    "    for i in list(fillna_method.keys()) :  \n",
    "        if fillna_method[i] not in [\"mode\",\"median\",\"mean\"] :\n",
    "            print (f\"{i} must be imputed by mean, median or mean\")\n",
    "            return x_train, x_test\n",
    "\n",
    "    # While loop :\n",
    "    j = len(groupby_col)  \n",
    "    while j>=0:\n",
    "        \n",
    "        if j > 0:\n",
    "            try:\n",
    "                for i in list(fillna_method.keys()) : \n",
    "                    grouped = x_train.groupby(groupby_col[:j])\n",
    "                    \n",
    "                    if fillna_method[i]==\"mode\" : \n",
    "                        mode = grouped[i].agg(lambda x: x.mode().iloc[0] if not x.mode().empty else np.nan)\n",
    "                    elif fillna_method[i]==\"median\" : \n",
    "                        mode = grouped[i].median()\n",
    "                    elif fillna_method[i]==\"mean\" : \n",
    "                        mode = grouped[i].mean() \n",
    "                    \n",
    "                    x_test[i] = x_test.groupby(groupby_col[:j])[i].transform(lambda x: x.fillna(mode[x.name]))\n",
    "                    x_train[i] = x_train.groupby(groupby_col[:j])[i].transform(lambda x: x.fillna(mode[x.name]))\n",
    "                    \n",
    "                    if x_train[i].isna().any()==True or x_test[i].isna().any()==True :\n",
    "                        if fillna_method[i]==\"mode\" : \n",
    "                            mode = x_train[i].mode()[0]\n",
    "                        elif fillna_method[i]==\"median\" : \n",
    "                            mode = x_train[i].median()   \n",
    "                        elif fillna_method[i]==\"mean\" : \n",
    "                            mode = x_train[i].mean() \n",
    "\n",
    "                        x_test[i] = x_test[i].fillna(mode)\n",
    "                        x_train[i] = x_train[i].fillna(mode)\n",
    "                        \n",
    "                \n",
    "                if display_groupby_col==True:\n",
    "                    print(groupby_col[:j])\n",
    "                \n",
    "                j=-1\n",
    "                \n",
    "            except :\n",
    "                j-=1\n",
    "    \n",
    "        elif j==0: \n",
    "            for i in list(fillna_method.keys()) :     \n",
    "                if fillna_method[i]==\"mode\" : \n",
    "                    mode = x_train[i].mode()[0]\n",
    "                elif fillna_method[i]==\"median\" : \n",
    "                    mode = x_train[i].median()   \n",
    "                elif fillna_method[i]==\"mean\" : \n",
    "                    mode = x_train[i].mean() \n",
    "        \n",
    "                x_test[i] = x_test[i].fillna(mode)\n",
    "                x_train[i] = x_train[i].fillna(mode)\n",
    "            \n",
    "            j=-1\n",
    "        \n",
    "    return x_train, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6cc064c-88de-4caa-976a-694bfa9be163",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Impute by non-fix value : \n",
    "\n",
    "# Fillna all groupby(Ft) :\n",
    "groupby_col = ['Ft']\n",
    "dict_imputation_non_fix = {\"VFN\": \"mode\", \"T\": \"mode\", \"Tan\": \"mode\",\n",
    "                            \"Va\": \"mode\",\"Ve\": \"mode\",\"Ct\": \"mode\",\n",
    "                            \"m (kg)\": \"median\",\"Mt\": \"median\",\"W (mm)\": \"median\",\n",
    "                            \"At1 (mm)\": \"median\",\"At2 (mm)\": \"median\",\"Fm\": \"mode\",\n",
    "                            \"ec (cm3)\": \"median\",\"ep (KW)\": \"median\", \"z (Wh/km)\" : \"median\",\n",
    "                            \"IT\": \"mode\",\"Fuel consumption \": \"median\", \"Electric range (km)\":\"median\", 'Cr':\"mode\", 'Mh':\"mode\"}\n",
    "\n",
    "#x_train, x_test = fillna_non_fix(x_train=x_train, x_test=x_test, fillna_method=dict_imputation_non_fix, groupby_col=groupby_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa81343d-8bab-495b-b00a-3a2b4a6ffb0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8b5a0241-3b10-4235-a7d0-72ed133cbe48",
   "metadata": {},
   "source": [
    "#### B/ Encoding/Scaling method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e1374d1-d527-497d-87ce-ec58b45a5887",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['m (kg)', 'Mt', 'W (mm)', 'At1 (mm)', 'At2 (mm)', 'ec (cm3)', 'ep (KW)', 'z (Wh/km)', 'Fuel consumption ', 'Electric range (km)']\n",
      "[]\n",
      "['Ct', 'Cr', 'Ft', 'Fm']\n",
      "['VFN', 'T', 'Va', 'Ve']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#numeric and > 2 :\n",
    "list_cont_col = x_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "list_cont_col = [col for col in list_cont_col if x_train[col].nunique() > 2]\n",
    "\n",
    "#numeric and <= 2 :\n",
    "list_binary_col = x_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "list_binary_col = [col for col in list_binary_col if x_train[col].nunique() <= 2]\n",
    "\n",
    "#categorical col :\n",
    "list_cat_col = x_train.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "list_cat_col_OHE = [col for col in list_cat_col if x_train[col].nunique() <= 30]\n",
    "list_cat_col_TE =  [col for col in list_cat_col if x_train[col].nunique() > 30]\n",
    "\n",
    "# Check if all columns are taken :\n",
    "print(list_cont_col)\n",
    "print(list_binary_col)\n",
    "print(list_cat_col_OHE)\n",
    "print(list_cat_col_TE)\n",
    "len(list_cont_col) + len(list_binary_col) + len(list_cat_col_OHE) + len(list_cat_col_TE) == x_train.shape[1]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "91bd28b1-f3ff-47f2-9463-cc38fc0b0707",
   "metadata": {
    "tags": []
   },
   "source": [
    "pre_process = pre_processing()\n",
    "            \n",
    "X_train = pre_process.pre_processing(df=x_train, train=True, categorical_var_OHE=list_cat_col_OHE,\n",
    "                        categorical_var_OrdinalEncoding={}, categorical_var_TE=list_cat_col_TE, target=y_train,\n",
    "                        continious_var=[], encoding_type_cont=MinMaxScaler())\n",
    "\n",
    "X_val = pre_process.pre_processing(df=x_val, train=False, categorical_var_OHE=list_cat_col_OHE,\n",
    "                        categorical_var_OrdinalEncoding={}, categorical_var_TE=list_cat_col_TE, target=y_train,\n",
    "                        continious_var=[], encoding_type_cont=MinMaxScaler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0917a0e-6892-4985-aa1e-11e67f4ebbfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "31ff33b1-8f3a-41ed-9f5b-f9977f71a0fb",
   "metadata": {},
   "source": [
    "## Model testing (CV method)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218e4cee-a136-4224-9ea1-8bc4370a1cf1",
   "metadata": {},
   "source": [
    "#### 0/ Evaluation metric "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b9c7a29-a6ab-4643-a33c-8f20ecc8f6bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# MAE_score_CV :\n",
    "\n",
    "def MAE_score_CV(x_train, y_train, model, fillna_method, groupby_col, list_cat_col_OHE=None, list_cat_col_TE=None, list_cont_col=None, cv=5, random_state=42, encoding=False, display_cv=False):\n",
    "    \n",
    "    num_bins = 8  # Number of bins for stratification\n",
    "    y_bins = pd.cut(y_train, bins=num_bins, labels=False)\n",
    "    skf = StratifiedKFold(n_splits=cv, shuffle=True, random_state=random_state) #StratifiedKFold or #KFold\n",
    "    \n",
    "    list_score = []\n",
    "    dict_score = {}\n",
    "\n",
    "    i=0\n",
    "    for train_idx, valid_idx in skf.split(x_train, y_bins):\n",
    "        X_train, X_val = x_train.loc[train_idx], x_train.loc[valid_idx]\n",
    "        Y_train, Y_val = y_train.loc[train_idx], y_train.loc[valid_idx]\n",
    "        \n",
    "        #reset_index() :\n",
    "        X_train = X_train.reset_index(drop = True)\n",
    "        Y_train = Y_train.reset_index(drop = True)\n",
    "        X_val = X_val.reset_index(drop = True)\n",
    "        Y_val = Y_val.reset_index(drop = True)\n",
    "        \n",
    "        # fillna :\n",
    "        X_train, X_val = fillna_non_fix(x_train=X_train, x_test=X_val, fillna_method=fillna_method, groupby_col=groupby_col)\n",
    "\n",
    "        # encoding :\n",
    "        if encoding==True :\n",
    "            pre_process = pre_processing()\n",
    "            X_train = pre_process.pre_processing(df=X_train, train=True, categorical_var_OHE=list_cat_col_OHE,\n",
    "                                                 categorical_var_OrdinalEncoding={}, categorical_var_TE=list_cat_col_TE, \n",
    "                                                 target=Y_train, continious_var=[], encoding_type_cont=MinMaxScaler())\n",
    "\n",
    "            X_val = pre_process.pre_processing(df=X_val, train=False, categorical_var_OHE=list_cat_col_OHE,\n",
    "                                             categorical_var_OrdinalEncoding={}, categorical_var_TE=list_cat_col_TE, \n",
    "                                             target=Y_train, continious_var=[], encoding_type_cont=MinMaxScaler())\n",
    "\n",
    "        # training on train set :\n",
    "        model.fit(X_train, Y_train)\n",
    "        \n",
    "        #score :\n",
    "        score_train = mean_absolute_error(Y_train, model.predict(X_train))\n",
    "        score_val = mean_absolute_error(Y_val, model.predict(X_val))\n",
    "        \n",
    "        dict_score[f\"cv {str(i)}\"]= {\"train\" : score_train, \"val\" : score_val}\n",
    "        list_score.append(score_val)\n",
    "        \n",
    "        #temp :\n",
    "        #print(f\"CV {str(i)} : score_train = {score_train} | score_val : {score_val}\")\n",
    "            \n",
    "        i+=1\n",
    "\n",
    "    if display_cv :\n",
    "        display(dict_score)\n",
    "        \n",
    "    return mean(list_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4f8313-6aaf-47f8-8983-c650a27e6f70",
   "metadata": {},
   "source": [
    "### A/ Catboost (to review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e485199e-5662-4a14-8386-934205ef6f94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Naiv modelling :\n",
    "\n",
    "model = CatBoostRegressor(iterations=150, # Number of boosting iterations\n",
    "                                    depth=6, # Depth of the tree\n",
    "                                    learning_rate=0.1, # Step size shrinkage\n",
    "                                    loss_function='MAE', \n",
    "                                    eval_metric='MAE',\n",
    "                                    cat_features=list_cat_col_OHE + list_cat_col_TE, # Indices of categorical features\n",
    "                                    silent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c88ca3-49d7-462b-9e3a-4b19e7663ec1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "MAE_score_CV(x_train=x_train, y_train=y_train, model=model, fillna_method=dict_imputation_non_fix, groupby_col=groupby_col, list_cat_col_OHE=None, \n",
    "             list_cat_col_TE=None, list_cont_col=None, cv=5, random_state=42, encoding=False, display_cv=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1cf520-91c9-4dd0-aae3-0b92fdbe26bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c10e2f-cf5e-4e8a-be66-37ac512152e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Bayesian Optimisation (optuna) :\n",
    "\n",
    "def objective(trial):\n",
    "    #hyperparameters :\n",
    "    learning_rate = trial.suggest_float(name='learning_rate', low=0.01, high=1)\n",
    "    colsample_bylevel = trial.suggest_float(name=\"colsample_bylevel\", low=0.3, high=1)\n",
    "    depth = trial.suggest_int(name=\"depth\", low=1, high=4)\n",
    "    reg_lambda = trial.suggest_float(name=\"reg_lambda\", low=0.01, high=10)\n",
    "    iterations = trial.suggest_int(name=\"iterations\", low=5, high=300)\n",
    "    random_strength = trial.suggest_float(name=\"random_strength\", low=0, high=10)\n",
    "    bagging_temperature = trial.suggest_float(name=\"bagging_temperature\", low=0, high=10)\n",
    "    \n",
    "\n",
    "    # instanciate :\n",
    "    # train on df_train :\n",
    "    model = CatBoostRegressor(silent=True, loss_function='MAE', # Use 'MultiClass' for multi-class classification\n",
    "                            eval_metric='MAE',\n",
    "                            cat_features=list_cat_col_OHE + list_cat_col_TE, \n",
    "                            learning_rate=learning_rate,\n",
    "                            colsample_bylevel=colsample_bylevel, depth=depth, reg_lambda=reg_lambda,\n",
    "                            iterations=iterations, random_strength=random_strength, bagging_temperature=bagging_temperature)\n",
    "\n",
    "    # score :\n",
    "    score = MAE_score_CV(x_train=x_train, y_train=y_train, model=model, fillna_method=dict_imputation_non_fix, groupby_col=groupby_col, list_cat_col_OHE=None, \n",
    "             list_cat_col_TE=None, list_cont_col=None, cv=3, random_state=42, encoding=False, display_cv=False)\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f202f65-8e46-4b22-8f88-162813cea234",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trial = study.best_trial\n",
    "print('score : {}'.format(trial.value)) # replace scoring='accuracy' by \"recall\"  #or auc\n",
    "print(\"Best hyperparameters: {}\".format(trial.params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab68cf2e-9e53-40a3-8577-35220136baa6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5d8644-f5da-45a0-92ad-951dc0556500",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Modelling with best hyperparameters : \n",
    "\n",
    "best_cat_model = CatBoostRegressor(silent=True, loss_function='MAE', # Use 'MultiClass' for multi-class classification\n",
    "                                    eval_metric='MAE',\n",
    "                                    cat_features=list_cat_col_OHE + list_cat_col_TE, \n",
    "                                    learning_rate=(trial.params)[\"learning_rate\"],\n",
    "                                    colsample_bylevel=(trial.params)[\"colsample_bylevel\"], depth=(trial.params)[\"depth\"], \n",
    "                                    reg_lambda=(trial.params)['reg_lambda'],\n",
    "                                    iterations=(trial.params)[\"iterations\"], random_strength=(trial.params)[\"random_strength\"], \n",
    "                                    bagging_temperature=(trial.params)[\"bagging_temperature\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846e8638-5b1a-4031-9bbc-814a6e78f567",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "MAE_score_CV(x_train=x_train, y_train=y_train, model=best_cat_model, fillna_method=dict_imputation_non_fix, groupby_col=groupby_col, list_cat_col_OHE=None, \n",
    "             list_cat_col_TE=None, list_cont_col=None, cv=5, random_state=42, encoding=False, display_cv=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e090cc2c-c8a7-49bf-87cd-1ac62bf454f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cba823c0-98fc-4090-a885-e2522a7ccf2d",
   "metadata": {},
   "source": [
    "### B/ Xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c454ef-eb50-4ac2-b077-7589cfb3414a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Naiv modelling :\n",
    "\n",
    "model = XGBRegressor(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840ef40f-3463-4a58-a6bb-6523b28ec2ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "MAE_score_CV(x_train=x_train, y_train=y_train, model=model, fillna_method=dict_imputation_non_fix, groupby_col=groupby_col, list_cat_col_OHE=list_cat_col_OHE, \n",
    "             list_cat_col_TE=list_cat_col_TE, list_cont_col=[], cv=2, random_state=42, encoding=True, display_cv=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911d9fd9-2131-4c10-a12f-849076a77506",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7555a0-c282-48c8-9194-671b6522f787",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Bayesian Optimisation (optuna) :\n",
    "\n",
    "def objective(trial):\n",
    "    #hyperparameters :\n",
    "    max_depth = trial.suggest_int('max_depth', 0, 50, step=2) #profondeur\n",
    "    learning_rate = trial.suggest_categorical('learning_rate', [0.01,0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1]) \n",
    "    colsample_bytree = trial.suggest_float('colsample_bytree', 0.1, 1, step=0.1) #min leaf of each tree\n",
    "    n_estimators = trial.suggest_int('n_estimators', 1,1001, step=50) #nb of tree\n",
    "    \n",
    "    # instanciate :\n",
    "    # train on train set :\n",
    "    model = XGBRegressor(random_state=42, n_jobs=-1, max_depth=max_depth, learning_rate=learning_rate, colsample_bytree=colsample_bytree,\n",
    "                        n_estimators=n_estimators)  #, tree_method='gpu_hist', predictor=\"gpu_predictor\"\n",
    "\n",
    "    # score :\n",
    "    score = MAE_score_CV(x_train=x_train, y_train=y_train, model=model, fillna_method=dict_imputation_non_fix, groupby_col=groupby_col, list_cat_col_OHE=list_cat_col_OHE, \n",
    "             list_cat_col_TE=list_cat_col_TE, list_cont_col=[], cv=5, random_state=42, encoding=True, display_cv=False)\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a08bb25-2de5-427b-aa98-ee945e3c9b1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trial = study.best_trial\n",
    "print('score : {}'.format(trial.value)) \n",
    "print(\"Best hyperparameters: {}\".format(trial.params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0a8354-2dae-46a2-9b86-b11dc751c8c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04da2f4d-4748-4883-aae1-2f0f89fed654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelling with best hyperparameters : \n",
    "\n",
    "best_xgb_model =  XGBRegressor(random_state=42, n_jobs=-1, max_depth=(trial.params)[\"max_depth\"], learning_rate=(trial.params)[\"learning_rate\"], \n",
    "                                  colsample_bytree=(trial.params)[\"colsample_bytree\"], n_estimators=(trial.params)[\"n_estimators\"]) #, tree_method='gpu_hist'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494b6c41-43d8-4b20-bf19-29057f38bbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "MAE_score_CV(x_train=x_train, y_train=y_train, model=best_xgb_model, fillna_method=dict_imputation_non_fix, groupby_col=groupby_col, list_cat_col_OHE=list_cat_col_OHE, \n",
    "             list_cat_col_TE=list_cat_col_TE, list_cont_col=[], cv=5, random_state=42, encoding=True, display_cv=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e17b71-ac11-42ea-a5d9-1ba8f4390c10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c481356d-ca0d-48f8-91fc-002e2e56b304",
   "metadata": {},
   "source": [
    "## Model testing (train/val1/val2 method)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596b1dcd-4b9c-4342-ba4d-32b55212abe2",
   "metadata": {},
   "source": [
    "#### 0*/ Data treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8c037da-3b5c-4787-9732-0424130d68fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split x_val/x_val2 :\n",
    "target = \"Ewltp (g/km)\"\n",
    "\n",
    "num_bins = 8  # Number of bins for stratification\n",
    "y_bins = pd.cut(df_val[target], bins=num_bins, labels=False)\n",
    "df_val1, df_val2 = train_test_split(df_val, test_size=0.5, stratify=y_bins, random_state=42)\n",
    "\n",
    "#reset_index :\n",
    "df_val1 = df_val1.reset_index(drop = True)\n",
    "df_val2 = df_val2.reset_index(drop = True)\n",
    "\n",
    "#create x_val, y_val, x_val2, y_val2 : \n",
    "x_val1, y_val1, x_val2, y_val2 = create_x_y(df_train=df_val1, df_test=df_val2, target=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d60c5a19-1eca-43a2-bed1-3172c3194a96",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n"
     ]
    }
   ],
   "source": [
    "# Fillna :\n",
    "x_train_imp, x_val1_imp = fillna_non_fix(x_train=x_train, x_test=x_val1, fillna_method=dict_imputation_non_fix, groupby_col=groupby_col)\n",
    "\n",
    "# Encoding/Scaling :\n",
    "pre_process = pre_processing()\n",
    "x_train_process = pre_process.pre_processing(df=x_train_imp, train=True, categorical_var_OHE=list_cat_col_OHE,\n",
    "                                     categorical_var_OrdinalEncoding={}, categorical_var_TE=list_cat_col_TE, \n",
    "                                     target=y_train, continious_var=[], encoding_type_cont=MinMaxScaler())\n",
    "\n",
    "x_val1_process = pre_process.pre_processing(df=x_val1_imp, train=False, categorical_var_OHE=list_cat_col_OHE,\n",
    "                                 categorical_var_OrdinalEncoding={}, categorical_var_TE=list_cat_col_TE, \n",
    "                                 target=y_train, continious_var=[], encoding_type_cont=MinMaxScaler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a3f631-630a-4893-a286-f77fae2df009",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3ecbde65-0748-46c8-86f5-c00f2870bcb6",
   "metadata": {},
   "source": [
    "### A*/ Catboost (to review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac84c283-b152-405c-a83f-3aeab5eef960",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Naiv modelling :\n",
    "\n",
    "# Instanciate :\n",
    "model = CatBoostRegressor(iterations=150, # Number of boosting iterations\n",
    "                                    depth=6, # Depth of the tree\n",
    "                                    learning_rate=0.1, # Step size shrinkage\n",
    "                                    loss_function='MAE', \n",
    "                                    eval_metric='MAE',\n",
    "                                    cat_features=list_cat_col_OHE + list_cat_col_TE, # Indices of categorical features\n",
    "                                    silent=True)\n",
    "\n",
    "# Training :\n",
    "model.fit(x_train_imp, y_train)\n",
    "\n",
    "# MAE score :\n",
    "print(f\"MAE on train : {mean_absolute_error(y_train, model.predict(x_train_imp))}\")\n",
    "print(f\"MAE on val : {mean_absolute_error(y_val1, model.predict(x_val1_imp))}\") #y_val1, x_val1_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061f2fe8-7742-48d5-bcc4-5cd42c36a5a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46380d7-f6c3-4c6f-bd15-b50aad70000d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Bayesian Optimisation (optuna) :\n",
    "\n",
    "def objective(trial):\n",
    "    #hyperparameters :\n",
    "    learning_rate = trial.suggest_float(name='learning_rate', low=0.01, high=1)\n",
    "    colsample_bylevel = trial.suggest_float(name=\"colsample_bylevel\", low=0.3, high=1)\n",
    "    depth = trial.suggest_int(name=\"depth\", low=1, high=4)\n",
    "    reg_lambda = trial.suggest_float(name=\"reg_lambda\", low=0.01, high=10)\n",
    "    iterations = trial.suggest_int(name=\"iterations\", low=5, high=300)\n",
    "    random_strength = trial.suggest_float(name=\"random_strength\", low=0, high=10)\n",
    "    bagging_temperature = trial.suggest_float(name=\"bagging_temperature\", low=0, high=10)\n",
    "    \n",
    "\n",
    "    # instanciate :\n",
    "    # train on df_train :\n",
    "    model = CatBoostRegressor(silent=True, loss_function='MAE', \n",
    "                            eval_metric='MAE',\n",
    "                            cat_features=list_cat_col_OHE + list_cat_col_TE, \n",
    "                            learning_rate=learning_rate,\n",
    "                            colsample_bylevel=colsample_bylevel, depth=depth, reg_lambda=reg_lambda,\n",
    "                            iterations=iterations, random_strength=random_strength, bagging_temperature=bagging_temperature)\n",
    "    \n",
    "    # Training :\n",
    "    model.fit(x_train_imp, y_train)\n",
    "    \n",
    "    # score :\n",
    "    score = mean_absolute_error(y_val1, model.predict(x_val1_imp))\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888f90ec-5028-4c5a-b235-5a35591f30ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Catboost_trial = study.best_trial\n",
    "print('score : {}'.format(trial.value)) \n",
    "print(\"Best hyperparameters: {}\".format(trial.params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d659f11-d539-4d80-a6f3-aca8894fefd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0f51ee-00ea-4f7b-889f-e1e0226c4d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelling with best hyperparameters : \n",
    "\n",
    "best_cat_model = CatBoostRegressor(silent=True, loss_function='MAE', # Use 'MultiClass' for multi-class classification\n",
    "                                    eval_metric='MAE',\n",
    "                                    cat_features=list_cat_col_OHE + list_cat_col_TE, \n",
    "                                    learning_rate=(Catboost_trial.params)[\"learning_rate\"],\n",
    "                                    colsample_bylevel=(Catboost_trial.params)[\"colsample_bylevel\"], depth=(Catboost_trial.params)[\"depth\"], \n",
    "                                    reg_lambda=(Catboost_trial.params)['reg_lambda'],\n",
    "                                    iterations=(Catboost_trial.params)[\"iterations\"], random_strength=(Catboost_trial.params)[\"random_strength\"], \n",
    "                                    bagging_temperature=(Catboost_trial.params)[\"bagging_temperature\"])\n",
    "\n",
    "# Training :\n",
    "best_cat_model.fit(x_train_imp, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d397a691-a3ff-45d9-943f-1f3f2dbb6637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model temporary :\n",
    "joblib.dump(value = best_cat_model, filename = './MODEL/Temp/train_val1_val2/best_cat_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aae61b9-63a8-417b-9f83-eef1a92ae9b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "784d5bde-e26b-49be-9c39-0675845dc932",
   "metadata": {},
   "source": [
    "### B*/ Xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c50002-01f9-4bf0-b73f-6a22c1752b98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Naiv modelling :\n",
    "\n",
    "# Instanciate :\n",
    "model = XGBRegressor(random_state=42, objective='reg:squarederror', booster= 'gbtree', eval_metric='mae', n_jobs=-1)\n",
    "\n",
    "# Training :\n",
    "model.fit(x_train_process, y_train)\n",
    "\n",
    "# MAE score :\n",
    "print(f\"MAE on train : {mean_absolute_error(y_train, model.predict(x_train_process))}\")\n",
    "print(f\"MAE on val : {mean_absolute_error(y_val1, model.predict(x_val1_process))}\") #y_val1, x_val1_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d010248a-a96f-4fd2-901d-adc75f63e4ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f1e8ef-6b54-4983-89af-1fe3887ffc9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Bayesian Optimisation (optuna) :\n",
    "\n",
    "def objective(trial):\n",
    "    #hyperparameters :\n",
    "    learning_rate = trial.suggest_float('learning_rate', 0.1, 0.4, step=0.001) \n",
    "    n_estimators = trial.suggest_int('n_estimators', 50, 2010, step=20) #nb of tree\n",
    "    max_depth = trial.suggest_int('max_depth', 3, 40, step=1) #profondeur\n",
    "    colsample_bytree = trial.suggest_float('colsample_bytree', 0.5, 1, step=0.1) #min leaf of each tree\n",
    "    \n",
    "    subsample = trial.suggest_float('subsample', 0.5, 1.0, step=0.1)\n",
    "    gamma= trial.suggest_float('gamma', 0, 5, step=0.1)\n",
    "    reg_alpha= trial.suggest_float('reg_alpha', 0, 1, step=0.1)\n",
    "    reg_lambda= trial.suggest_float('reg_lambda', 0, 1, step=0.1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # instanciate :\n",
    "    # train on train set :\n",
    "    model = XGBRegressor(random_state=42, n_jobs=-1, objective='reg:squarederror', booster= 'gbtree', eval_metric='mae', \n",
    "                         learning_rate=learning_rate, n_estimators=n_estimators, max_depth=max_depth, colsample_bytree=colsample_bytree, \n",
    "                         subsample=subsample, gamma=gamma, reg_alpha=reg_alpha, reg_lambda=reg_lambda)  #, tree_method='gpu_hist', predictor=\"gpu_predictor\"\n",
    "\n",
    "    # Training :\n",
    "    model.fit(x_train_process, y_train)\n",
    "    \n",
    "    # score :\n",
    "    score = mean_absolute_error(y_val1, model.predict(x_val1_process))\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9748b5ec-7f1e-4ef1-a3ad-1708c5953e43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "XGB_trial = study.best_trial\n",
    "print('score : {}'.format(XGB_trial.value)) \n",
    "print(\"Best hyperparameters: {}\".format(XGB_trial.params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5df6054-e0cb-444d-a312-29e3b7368034",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06894611-1084-468a-9660-cec1b74c160b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelling with best hyperparameters : \n",
    "\n",
    "best_xgb_model = XGBRegressor(random_state=42, n_jobs=-1, objective='reg:squarederror', booster= 'gbtree', eval_metric='mae', \n",
    "                               learning_rate=(XGB_trial.params)[\"learning_rate\"], n_estimators=(XGB_trial.params)[\"n_estimators\"], \n",
    "                               max_depth=(XGB_trial.params)[\"max_depth\"], colsample_bytree=(XGB_trial.params)[\"colsample_bytree\"],\n",
    "                               subsample=(XGB_trial.params)[\"subsample\"], gamma=(XGB_trial.params)[\"gamma\"], reg_alpha=(XGB_trial.params)[\"reg_alpha\"], \n",
    "                               reg_lambda=(XGB_trial.params)[\"reg_lambda\"]) #, tree_method='gpu_hist', predictor=\"gpu_predictor\"\n",
    "\n",
    "# Training :\n",
    "best_xgb_model.fit(x_train_process, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37eb2e2-5270-494d-974e-a4b89feac85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model temporary :\n",
    "joblib.dump(value = best_xgb_model, filename = './MODEL/Temp/train_val1_val2/best_xgb_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84aa2cd0-3619-4427-bedf-bb3b6826c560",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c070d3bc-38e8-4112-99d6-b2fce1d40edf",
   "metadata": {},
   "source": [
    "### C*/ Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c3a4f1-959e-4865-8a3c-06f932ae805d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Naiv modelling :\n",
    "\n",
    "# Instanciate :\n",
    "model = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "\n",
    "# Training :\n",
    "model.fit(x_train_process, y_train)\n",
    "\n",
    "# MAE score :\n",
    "print(f\"MAE on train : {mean_absolute_error(y_train, model.predict(x_train_process))}\")\n",
    "print(f\"MAE on val : {mean_absolute_error(y_val1, model.predict(x_val1_process))}\") #y_val1, x_val1_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e75476-b291-4522-86af-acf11cc75daf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4973debd-038d-4fe3-8fba-c1b98afef40b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Bayesian Optimisation (optuna) :\n",
    "\n",
    "def objective(trial):\n",
    "    \n",
    "    #hyperparameters :\n",
    "    max_depth = trial.suggest_int('max_depth', 2, 50, step=2) #profondeur\n",
    "    max_features = trial.suggest_categorical('max_features', [\"log2\",\"sqrt\",None]) \n",
    "    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 10, step=1) #min leaf of each tree\n",
    "    max_leaf_nodes = trial.suggest_int('max_leaf_nodes', 1, 10, step=1) #max leaf of each tree\n",
    "    min_samples_split = trial.suggest_int('min_samples_split', 4, 20, step=1)\n",
    "    n_estimators = trial.suggest_int('n_estimators', 40,2000, step=20) #nb of tree\n",
    "\n",
    "    \n",
    "    # instanciate :\n",
    "    # train on train set :\n",
    "    model = RandomForestRegressor(random_state=42, n_jobs=-1, max_depth=max_depth, max_features=max_features, min_samples_leaf=min_samples_leaf, \n",
    "                                max_leaf_nodes=max_leaf_nodes, min_samples_split=min_samples_split, n_estimators=n_estimators)\n",
    "\n",
    "    # Training :\n",
    "    model.fit(x_train_process, y_train)\n",
    "    \n",
    "    # score :\n",
    "    score = mean_absolute_error(y_val1, model.predict(x_val1_process))\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efba3f43-7caf-4f57-b6c3-5584b23a58ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rf_trial = study.best_trial\n",
    "print('score : {}'.format(rf_trial.value)) \n",
    "print(\"Best hyperparameters: {}\".format(rf_trial.params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7db0f3e-8b82-4cef-8975-c50fcc6254d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46af1a78-eab0-4112-a9c7-9816933214a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelling with best hyperparameters : \n",
    "\n",
    "best_rf_model = RandomForestRegressor(random_state=42, n_jobs=-1, max_depth=(rf_trial.params)[\"max_depth\"], max_features=(rf_trial.params)[\"max_features\"], \n",
    "                                      min_samples_leaf=(rf_trial.params)[\"min_samples_leaf\"], max_leaf_nodes=(rf_trial.params)[\"max_leaf_nodes\"], \n",
    "                                      min_samples_split=(rf_trial.params)[\"min_samples_split\"], n_estimators=(rf_trial.params)[\"n_estimators\"])\n",
    "\n",
    "# Training :\n",
    "best_rf_model.fit(x_train_process, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99dcbad1-a97c-4a13-9ee6-99b773803034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model temporary :\n",
    "joblib.dump(value = best_rf_model, filename = './MODEL/Temp/train_val1_val2/best_rf_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce9c3ab-9280-48e2-b664-736c12b102a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "20000eb1-670f-4c25-8226-aa725e98502f",
   "metadata": {},
   "source": [
    "### D*/ LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811ea3f2-2708-4dca-986a-5841ccd5523e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Naiv modelling :\n",
    "\n",
    "# Instanciate :\n",
    "model = LGBMRegressor(random_state=42, n_jobs=-1)\n",
    "\n",
    "# Training :\n",
    "model.fit(x_train_process, y_train)\n",
    "\n",
    "# MAE score :\n",
    "print(f\"MAE on train : {mean_absolute_error(y_train, model.predict(x_train_process))}\")\n",
    "print(f\"MAE on val : {mean_absolute_error(y_val1, model.predict(x_val1_process))}\") #y_val1, x_val1_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3838be-a3cd-4a09-ba63-d52b754af891",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847f3290-6dde-4b28-9956-5309f695118e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Bayesian Optimisation (optuna) :\n",
    "\n",
    "def objective(trial):\n",
    "    max_depth = trial.suggest_categorical('max_depth', [5, 6, 7, 8, 9, 10, 12, 14, 16, 18, 20, 23, 25, 28, 30, 40, 50, None]) #profondeur\n",
    "    learning_rate = trial.suggest_float('learning_rate', 0.1, 0.4, step=0.001) \n",
    "    n_estimators = trial.suggest_int('n_estimators', 40,2000, step=20) #nb of tree\n",
    "    boosting_type = trial.suggest_categorical('boosting_type', [\"gbdt\", \"dart\"])\n",
    "    num_leaves = trial.suggest_int('num_leaves', 10,200,step=5)\n",
    "    #feature_fraction = trial.suggest_float('feature_fraction', 0.1,0.999)\n",
    "    subsample = trial.suggest_float('subsample', 0.1,0.999)\n",
    "    reg_alpha = trial.suggest_float('reg_alpha', 0.001,0.999)\n",
    "    reg_lambda = trial.suggest_float('reg_lambda', 0.001,0.999)\n",
    "\n",
    "    model = LGBMRegressor(random_state=42, max_depth=max_depth, learning_rate=learning_rate, n_estimators=n_estimators, num_leaves=num_leaves,\n",
    "                                    boosting_type=boosting_type, subsample=subsample, reg_alpha=reg_alpha, reg_lambda=reg_lambda)\n",
    "    \n",
    "    # Training :\n",
    "    model.fit(x_train_process, y_train)\n",
    "    \n",
    "    # score :\n",
    "    score = mean_absolute_error(y_val1, model.predict(x_val1_process))\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057f0877-80cc-47d5-99f7-e00ee347f63c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "LGBM_trial = study.best_trial\n",
    "print('score : {}'.format(LGBM_trial.value)) \n",
    "print(\"Best hyperparameters: {}\".format(LGBM_trial.params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695927f3-6349-4023-884a-7cb8da187fa5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8655e6db-9731-4d5a-886a-f890c84ed830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelling with best hyperparameters : \n",
    "\n",
    "best_lgbm_model = RandomForestRegressor(random_state=42, n_jobs=-1, max_depth=(LGBM_trial.params)['max_depth'], \n",
    "                                      learning_rate=(LGBM_trial.params)['learning_rate'], \n",
    "                                      n_estimators=(LGBM_trial.params)['n_estimators'], num_leaves=(LGBM_trial.params)['num_leaves'], \n",
    "                                      boosting_type=(LGBM_trial.params)['boosting_type'], subsample=(LGBM_trial.params)['subsample'], \n",
    "                                      reg_alpha=(LGBM_trial.params)['reg_alpha'], reg_lambda=(LGBM_trial.params)['reg_lambda'])\n",
    "                                  \n",
    "# Training :\n",
    "best_lgbm_model.fit(x_train_process, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3cba59-8b59-4319-9cb7-ea2dbccfd20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model temporary :\n",
    "joblib.dump(value = best_lgbm_model, filename = './MODEL/Temp/train_val1_val2/best_lgbm_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f854f65-a25b-435b-b058-f568d7d3195f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cab714b7-47de-4f14-ade8-7b240a24cbd5",
   "metadata": {},
   "source": [
    "## Combinaison testing (train/val1/val2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3799c3-5a32-4746-ab7e-cf689ece5308",
   "metadata": {},
   "source": [
    "#### 0/ Import all model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18ad958-8d6e-4897-bf25-762d72146847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all model :\n",
    "best_cat_model = joblib.load(filename = './MODEL/Temp/train_val1_val2/best_cat_model.pkl')\n",
    "best_xgb_model = joblib.load(filename = './MODEL/Temp/train_val1_val2/best_xgb_model.pkl')\n",
    "best_rf_model = joblib.load(filename = './MODEL/Temp/train_val1_val2/best_rf_model.pkl')\n",
    "best_lgbm_model = joblib.load(filename = './MODEL/Temp/train_val1_val2/best_lgbm_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715d90bc-2457-4df2-b8c5-cf084f79af83",
   "metadata": {},
   "source": [
    "### A/ Comb 1 (xgb, rf, lgbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733f9208-caa4-4a64-8e47-955a592502d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combinaison method 1 :\n",
    "\n",
    "def model_predict_comb1(x_test, dict_coeff):\n",
    "    \n",
    "    y_test_pred_xgb = best_xgb_model.predict(x_test)\n",
    "    y_test_pred_rf = best_rf_model.predict(x_test)\n",
    "    y_test_pred_lgbm = best_lgbm_model.predict(x_test)\n",
    "\n",
    "    y_test_pred = y_test_pred_xgb * dict_coeff[\"XGB\"] + y_test_pred_rf * dict_coeff[\"RF\"] + y_test_pred_lgbm * dict_coeff[\"LGBM\"]\n",
    "    \n",
    "    return y_test_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5880a1e1-7495-4c14-9caf-40ebac5b32c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d648fe83-0599-44d1-ba9a-6525584ba61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naiv modelling :\n",
    "dict_coeff = {\"XGB\" :0.7, \"RF\":0.15, \"LGBM\":0.15}\n",
    "y_val1_pred_comb = model_predict_comb1(x_test=x_val1_process, dict_coeff=dict_coeff)\n",
    "print(mean_absolute_error(y_val1, y_val1_pred_comb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fe3acf-cea4-4cca-a6d6-9d618be5162f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bc037f-48cf-4601-934e-e576aca494f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Bayesian Optimisation (optuna) :\n",
    "\n",
    "def objective(trial):\n",
    "    xgb_coeff = trial.suggest_float('xgb_coeff', 0, 1, step=0.001)\n",
    "    rf_coeff = trial.suggest_float('rf_coeff', 0, 1, step=0.001)\n",
    "    lgbm_coeff = trial.suggest_float('lgbm_coeff', 0, 1, step=0.001)\n",
    "    \n",
    "    # Build dict coeff :\n",
    "    dict_coeff = {\"XGB\" : xgb_coeff, \"RF\":rf_coeff, \"LGBM\":lgbm_coeff}\n",
    "\n",
    "    # Predict :\n",
    "    y_val1_pred_comb = model_predict_comb1(x_test=x_val1_process, dict_coeff=dict_coeff)\n",
    "    \n",
    "    # score :\n",
    "    score = mean_absolute_error(y_val1, y_val1_pred_comb)\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d6baab-baf2-4fcf-b294-28828f709a63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "comb_trial = study.best_trial\n",
    "print('score : {}'.format(comb_trial.value)) \n",
    "print(\"Best hyperparameters: {}\".format(comb_trial.params))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "488eb3db-d2a7-4f00-9fee-b6c19c11c46b",
   "metadata": {},
   "source": [
    "score : 2.9162520992211833\n",
    "Best hyperparameters: {'xgb_coeff': 0.613, 'rf_coeff': 0.114, 'lgbm_coeff': 0.273}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cd344a-45b7-41f3-ae05-399ab61bd409",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a834cf61-bbfc-43c5-b96c-00b0e8887d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the best dict_coeff :\n",
    "dict_coeff = {\"XGB\" :(comb_trial.params)[\"xgb_coeff\"], \"RF\": (comb_trial.params)['rf_coeff'] , \"LGBM\": (comb_trial.params)['lgbm_coeff']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66831415-d67d-4ec4-b24b-6508800268c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dict and model combinaison :\n",
    "\n",
    "# dict : \n",
    "with open(r'./MODEL/Temp/Combinaison_model/Comb1/dict_coeff.yaml', 'w') as file:\n",
    "    documents = yaml.dump(dict_coeff, file)\n",
    "\n",
    "# Model the model used for this comb model : \n",
    "joblib.dump(value = best_xgb_model, filename = './MODEL/Temp/Combinaison_model/Comb1/best_xgb_model.pkl')\n",
    "joblib.dump(value = best_rf_model, filename = './MODEL/Temp/Combinaison_model/Comb1/best_rf_model.pkl')\n",
    "joblib.dump(value = best_lgbm_model, filename = './MODEL/Temp/Combinaison_model/Comb1/best_lgbm_model.pkl')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3d425ce3-4c17-465b-a61a-d4744eaec518",
   "metadata": {
    "tags": []
   },
   "source": [
    "# If the best model is this combinaison, then use this to read the dict_coeff file :\n",
    "# read dict_coeff : \n",
    "with open(r'./MODEL/Temp/Combinaison_model/Comb1/dict_coeff.yaml') as file:\n",
    "    dict_coeff = yaml.load(file, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501b2154-3f6c-4ebd-9564-9dba0d633170",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "03fe402e-ad26-4dc5-b1cf-33e6e30391ea",
   "metadata": {},
   "source": [
    "# 6/ Choose, Train, Valid, Save the best model : "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d490f06-5c47-41f4-bd03-88541280a187",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Choose the model"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7e38ab75-1eee-4241-afda-f3ad6e0819e6",
   "metadata": {},
   "source": [
    "XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f19923a-7e62-4930-bc5e-cfd05138a8c4",
   "metadata": {},
   "source": [
    "## Treat x_train/x_val1/x_val2 for the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "681f0ee4-4035-4781-be71-26db3ed65257",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n"
     ]
    }
   ],
   "source": [
    "# Fillna :\n",
    "\n",
    "# x_train and x_val1 :\n",
    "x_train_imp, x_val1_imp = fillna_non_fix(x_train=x_train, x_test=x_val1, fillna_method=dict_imputation_non_fix, groupby_col=groupby_col)\n",
    "\n",
    "# x_val2 :\n",
    "_ , x_val2_imp = fillna_non_fix(x_train=x_train, x_test=x_val2, fillna_method=dict_imputation_non_fix, groupby_col=groupby_col)\n",
    "\n",
    "\n",
    "\n",
    "# Encoding/Scaling :\n",
    "pre_process = pre_processing()\n",
    "\n",
    "# x_train :\n",
    "x_train_process = pre_process.pre_processing(df=x_train_imp, train=True, categorical_var_OHE=list_cat_col_OHE,\n",
    "                                     categorical_var_OrdinalEncoding={}, categorical_var_TE=list_cat_col_TE, \n",
    "                                     target=y_train, continious_var=[], encoding_type_cont=MinMaxScaler())\n",
    "\n",
    "# x_val1 :\n",
    "x_val1_process = pre_process.pre_processing(df=x_val1_imp, train=False, categorical_var_OHE=list_cat_col_OHE,\n",
    "                                 categorical_var_OrdinalEncoding={}, categorical_var_TE=list_cat_col_TE, \n",
    "                                 target=y_train, continious_var=[], encoding_type_cont=MinMaxScaler())\n",
    "\n",
    "# x_val2 :\n",
    "x_val2_process = pre_process.pre_processing(df=x_val2_imp, train=False, categorical_var_OHE=list_cat_col_OHE,\n",
    "                                 categorical_var_OrdinalEncoding={}, categorical_var_TE=list_cat_col_TE, \n",
    "                                 target=y_train, continious_var=[], encoding_type_cont=MinMaxScaler())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e432cbe2-4933-4eab-9627-deb5d49a8fe1",
   "metadata": {},
   "source": [
    "## Load the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6ec9c60-4c4c-4708-9e9a-bfc421c283bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#load the best regression model :\n",
    "best_model_reg = joblib.load(filename = './MODEL/Temp/train_val1_val2/best_xgb_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d630a88-5e73-43df-9ea9-db33fe3df2b4",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3da8b6e7-4283-4dac-a749-f67f42c6c55d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE on train : 2.6303748991392304\n",
      "MAE on val1 : 2.9301284525904654\n",
      "MAE on val2 : 2.938298250240637\n"
     ]
    }
   ],
   "source": [
    "# MAE\n",
    "print(f\"MAE on train : {mean_absolute_error(y_train, best_model_reg.predict(x_train_process))}\")\n",
    "print(f\"MAE on val1 : {mean_absolute_error(y_val1, best_model_reg.predict(x_val1_process))}\")\n",
    "print(f\"MAE on val2 : {mean_absolute_error(y_val2, best_model_reg.predict(x_val2_process))}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0f6bc991-14d4-42bf-8231-19f0b18d7bf3",
   "metadata": {},
   "source": [
    "MAE on train : 2.6303748991392304\n",
    "MAE on val1 : 2.9301284525904654\n",
    "MAE on val2 : 2.938298250240637"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea26d771-a1ed-4947-a230-05bb06b447e4",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4460afcc-64c8-437b-9049-cfc95d4d2f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model :\n",
    "joblib.dump(value = best_model_reg, filename = './MODEL/best_model/best_model_reg.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be8f204-b8fb-4b7d-a6fa-289a914bafe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model :\n",
    "best_model_reg = joblib.load(filename = './MODEL/best_model/best_model_reg.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cff70a1-904c-4729-95cc-381e7d38c173",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5b6a0a76-1c18-4c11-8cad-1f4b015f49ff",
   "metadata": {},
   "source": [
    "# 7/ Analyse output : (useless)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47006fe5-4a83-454a-9fbc-553bc154ec88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "89c811f8-10eb-46a3-aee7-97391d8cb7f0",
   "metadata": {},
   "source": [
    "# 8/ Choose best threshold (option) : (useless)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8755077a-6698-4ccd-b756-365910cedbc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "efcb1fd0-0d29-48e4-8309-1be28a61c052",
   "metadata": {},
   "source": [
    "# 9/ Feature impact analysis (model interpretation) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62cfdc6-b6c8-48a8-8d71-60316d1cdeb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identification des variables les plus importantes :\n",
    "def Features_importance(model) -> pd.DataFrame :\n",
    "    \"\"\"\n",
    "    Calculate and return feature importance scores as a DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing feature names and their importance scores in percentage.\n",
    "    \"\"\"\n",
    "    df_features_importance = (pd.DataFrame({'Features': model.feature_names_in_,\n",
    "            'Features importance (in %)': (model.feature_importances_)*100}))\n",
    "    \n",
    "    return df_features_importance.sort_values(by='Features importance (in %)', ascending=False)\n",
    "\n",
    "Features_importance(model=best_model_reg).head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2f0537-3799-4203-b50a-0732e362b4d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6890f69-0cb1-4735-9e24-4520272948e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP values\n",
    "\n",
    "#General :\n",
    "# compute the SHAP values for the linear model\n",
    "explainer = shap.TreeExplainer(best_model_reg)\n",
    "shap_values = explainer.shap_values(x_val2_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7449112d-9f12-4d73-95ed-aa09f28a8915",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, x_val2_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22abf7c-afb4-40a7-9c76-629e73f950a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b8c590c3-9b84-4a77-a4ad-651796d45418",
   "metadata": {},
   "source": [
    "# 10/ Deployement :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5ddd6f-e1d3-45f7-a158-9a520e05cbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1/ Open data\n",
    "\n",
    "# - read_csv :\n",
    "x_test = pd.read_csv(\"./data/test.csv\", encoding=\"utf-8\")\n",
    "\n",
    "# - Take the ID (option) :\n",
    "ID = x_test[\"ID\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9322aed-f79c-4ce6-8182-917cd97a1ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2/ Data cleaning \n",
    "\n",
    "\n",
    "# - Basic treatment :\n",
    "useless_columns = ['ID', 'Vf', 'De','Ernedc (g/km)','MMS','Mp','Mk','Man','Cn','Date of registration','r','Status']\n",
    "x_test = basic_treatment(df=x_test, useless_columns=useless_columns, drop_duplicate=False)\n",
    "\n",
    "\n",
    "# - Data filter : (useless)\n",
    "\n",
    "\n",
    "# - Data transformation : (useless)\n",
    "\n",
    "\n",
    "# - Check/change col type : (good)\n",
    "\n",
    "\n",
    "# - Handle abnormal values : \n",
    "# Verif incoherance : \"z (Wh/km)\" \n",
    "# Solution : Corriger les lignes incohérentes. Ajouter \"/electric\" à la col \"Ft\" si : z (Wh/km) != NaN, Fuel consumption != NaN, et Ft ne contient pas \"electric\"\n",
    "x_test.loc[(x_test['z (Wh/km)'].notna()) & (x_test['Fuel consumption '].notna() & ~(x_test[\"Ft\"].str.contains(\"electric\"))), \"Ft\"] += \"/electric\"\n",
    "\n",
    "\n",
    "# - Impute NaN (delete col, fillna_fix) :\n",
    "# A/ Delete columns which contains more than 50% of NaN or useless :\n",
    "Col_to_drop = [\"Enedc (g/km)\", \"Erwltp (g/km)\"]\n",
    "x_test = x_test.drop(Col_to_drop, axis=1)\n",
    "\n",
    "#B/ # Impute by fix value :\n",
    "dict_imputation_fix = {\"Country\" : \"unknown\", \"z (Wh/km)\": 0,\"Fuel consumption \": 0, \"Electric range (km)\": 0}\n",
    "x_test = fillna_fix_value(df=x_test, fillna_value=dict_imputation_fix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c8ef82-9da1-49d5-b3ee-cfc0d2bb5321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3/ Feature eng \n",
    "\n",
    "\n",
    "# - Feature creation : (useless)\n",
    "\n",
    "\n",
    "# - Feature selection (x_test = x_test[list(x_train.columns)]) :\n",
    "x_test = x_test[list(x_train.columns)]\n",
    "\n",
    "\n",
    "# - Row selection : (useless)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ac4672-cab3-4059-8e25-2708b86ee4a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 4/ Prediction\n",
    "\n",
    "\n",
    "# - Impute NaN (fillna_nonfix) :\n",
    "groupby_col = ['Ft']\n",
    "dict_imputation_non_fix = {\"VFN\": \"mode\",\"T\": \"mode\", \"Tan\": \"mode\", \"Va\": \"mode\",\"Ve\": \"mode\",\"Ct\": \"mode\",\"m (kg)\": \"median\",\"Mt\": \"median\",\"W (mm)\": \"median\",\"At1 (mm)\": \"median\",\"At2 (mm)\": \"median\",\"Fm\": \"mode\",\n",
    "                            \"ec (cm3)\": \"median\",\"ep (KW)\": \"median\", \"z (Wh/km)\" : \"median\",\"IT\": \"mode\",\"Fuel consumption \": \"median\", \"Electric range (km)\":\"median\", 'Cr':\"mode\", 'Mh':\"mode\"}\n",
    "x_train_imp, x_test_imp = fillna_non_fix(x_train=x_train, x_test=x_test, fillna_method=dict_imputation_non_fix, groupby_col=groupby_col)\n",
    "\n",
    "\n",
    "# - Encoding/Scaling :\n",
    "pre_process = pre_processing()\n",
    "x_train_pro = pre_process.pre_processing(df=x_train_imp, train=True, categorical_var_OHE=list_cat_col_OHE,\n",
    "                                     categorical_var_OrdinalEncoding={}, categorical_var_TE=list_cat_col_TE, \n",
    "                                     target=y_train, continious_var=[], encoding_type_cont=MinMaxScaler())\n",
    "\n",
    "x_test_pro = pre_process.pre_processing(df=x_test_imp, train=False, categorical_var_OHE=list_cat_col_OHE,\n",
    "                                 categorical_var_OrdinalEncoding={}, categorical_var_TE=list_cat_col_TE, \n",
    "                                 target=y_train, continious_var=[], encoding_type_cont=MinMaxScaler())\n",
    "\n",
    "\n",
    "# - model.predict(x_test) :\n",
    "y_test_pred = best_model_reg.predict(x_test_pro)\n",
    "\n",
    "\n",
    "# - Save the prediction (submission.csv) :\n",
    "submission = pd.DataFrame({'ID': ID, 'Ewltp (g/km)': y_test_pred})\n",
    "submission.to_csv(r'submission.csv', index=False)\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf2a4f6-1abd-4412-a1d8-fca8b794ad38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m113",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m113"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
