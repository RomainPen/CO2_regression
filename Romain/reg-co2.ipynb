{"metadata":{"environment":{"kernel":"conda-root-py","name":"workbench-notebooks.m113","type":"gcloud","uri":"gcr.io/deeplearning-platform-release/workbench-notebooks:m113"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7931178,"sourceType":"datasetVersion","datasetId":4661860},{"sourceId":8011069,"sourceType":"datasetVersion","datasetId":4661711}],"dockerImageVersionId":30673,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1/ Import packages and data :","metadata":{"tags":[]}},{"cell_type":"markdown","source":"## Import package","metadata":{"tags":[]}},{"cell_type":"code","source":"!cp /kaggle/input/package2/pre_processing.py /kaggle/working\nfrom pre_processing import pre_processing","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\npd.set_option(\"display.min_rows\", 10)\npd.set_option(\"display.max_column\", 1000)\nimport os\nfrom unidecode import unidecode\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom scipy.stats import chi2_contingency, spearmanr\nfrom statsmodels.formula.api import ols\nfrom statsmodels.stats.anova import anova_lm\nimport re\nfrom tqdm import tqdm\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.feature_selection import RFE, RFECV\nimport sys\n#sys.path.append(\"pre_processing.py\")\n#from pre_processing import pre_processing\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom statistics import mean\nfrom sklearn.model_selection import StratifiedKFold, KFold\nfrom sklearn.metrics import mean_absolute_error\nimport catboost\nfrom catboost import CatBoostRegressor, Pool\nimport optuna\nimport xgboost\nfrom xgboost import XGBRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom lightgbm import LGBMRegressor\nimport joblib\nimport yaml\nimport shap\n\nfrom category_encoders import CountEncoder","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-04-08T19:31:17.889630Z","iopub.execute_input":"2024-04-08T19:31:17.890145Z","iopub.status.idle":"2024-04-08T19:31:26.429771Z","shell.execute_reply.started":"2024-04-08T19:31:17.890112Z","shell.execute_reply":"2024-04-08T19:31:26.428649Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/dask/dataframe/_pyarrow_compat.py:23: UserWarning: You are using pyarrow version 11.0.0 which is known to be insecure. See https://www.cve.org/CVERecord?id=CVE-2023-47248 for further details. Please upgrade to pyarrow>=14.0.1 or install pyarrow-hotfix to patch your current version.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"os.getcwd()","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Collect data ","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/data-co2/train.csv\", encoding=\"utf-8\")\ndf","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"raw","source":"Suite à la selection de var, on s'attend à ce que : \nTan <=> [Mp, Mh, Man, MMS, T, Va, Mk, Cn, Ct, Cr, r, Enedc (g/km), W (mm), At1 (mm), At2 (mm), Ft, Fm, ec (cm3), ep (KW), z (Wh/km), IT, Ernedc (g/km), Erwltp (g/km), De, Vf, Electric range (km)]","metadata":{}},{"cell_type":"markdown","source":"# 2/ Data cleaning :","metadata":{}},{"cell_type":"markdown","source":"## Basic treatment ","metadata":{}},{"cell_type":"code","source":"# Rename col : if necessary\n# df.columns = [name.split(\" \")[0] for name in df.columns]\n# df.columns = df.columns.str.replace(\" \", \"_\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# target : Ewltp (g/km)\ntarget = \"Ewltp (g/km)\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Detect useless col \n\n## Detect useless col after empirical analysis :\n# ['Date of registration','r','Status','ID']\n# [\"ID\", \"Vf\" ,\"De\", \"Ernedc (g/km)\", \"MMS\", \"Mp\", \"Mk\", \"Man\", \"Cn\", \"Date of registration\"]\nuseless_columns = {'Date of registration','r','Status','ID'}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Detect columns that contain only one value :\nfor i in list(df.columns) :\n    if df[i].nunique()==1 :\n        useless_columns.add(i)\n        \nuseless_columns","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def basic_treatment(df :pd.DataFrame, useless_columns : list, drop_duplicate=True) -> pd.DataFrame:\n    \"\"\"Perform basic data treatment on a DataFrame.\n\n        This function performs basic data treatment on a given DataFrame:\n        1. Removes duplicate rows.\n        2. Drops specified useless columns.\n\n        Args:\n            df (pd.DataFrame): The input DataFrame to be treated.\n            useless_columns (set): A set of column names to be removed from the DataFrame.\n\n        Returns:\n            pd.DataFrame: A DataFrame with duplicate rows removed, specified columns dropped,\n            and string values converted to lowercase.\n    \"\"\"\n    \n    # step 1: drop duplicate\n    if drop_duplicate==True :\n        df = df.drop_duplicates()\n        df = df.reset_index(drop = True)\n\n    # step 2 : drop useless col \n    df = df.drop(list(useless_columns), axis=1)\n\n    # step 3 : lowercase caracter \n    #df = df.applymap(lambda s : s.lower() if (type(s) == str and pd.isna(s)==False) else s) #map or applymap\n\n    # step 4 : drop white space \n    df = df.applymap(lambda s : s.strip() if (isinstance(s, str) and pd.isna(s)==False) else s) \n\n    # step 5 : drop multiple(double, triple) space \n    df = df.applymap(lambda s:s.replace(\"  \", \" \") if (isinstance(s, str) and pd.isna(s)==False) else s) \n\n    # step 6 : replace \" \" by \"_\" \n    df = df.applymap(lambda s:s.replace(\" \", \"_\") if (isinstance(s, str) and pd.isna(s)==False) else s) \n\n    # step 7 : remove accent \n    #df = df.applymap(lambda s: unidecode(s) if (isinstance(s, str) and pd.isna(s)==False) else s) \n\n    return df\n","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\ndf = basic_treatment(df=df, useless_columns=useless_columns, drop_duplicate=True)\ndf","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data filter (useless)","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data transformation (useless)","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Check and change type of columns (all good)","metadata":{}},{"cell_type":"code","source":"for i in df.columns :\n    print(f'{i} : {df[i].dtypes}')","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Check and handle abnormal features","metadata":{}},{"cell_type":"code","source":"# Analyze abnormal values (simple):\n\n#Num col :\ndf.describe()","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Cat col : \nfor i in df.columns :\n    if df[i].dtypes == object:\n        display(f'{i} : {set(df[i].unique())}')","metadata":{"tags":[],"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Treat anomalies (simple) :\n# Num : No anormal values\n# Cat : No anormal values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Analyze and treat abnormal values (deeper) :\n# Analyser et vérifier les incoherences dans la base train et test (Voir si les incoherences sont présentes dans les 2 bases ou pas) :\n\n\n# Verif incoherance : \"Fuel consumption \" \n# Solution : Supprimer les lignes incohérentes, car il y avait des voitures électrique qui consommaient du Fuel, ce qui est incohérent.\ndf = df[~((df['Ft'] == 'ELECTRIC') & (df['Fuel consumption '].notna()))]\ndf = df.reset_index(drop = True)\n\n# Verif incoherance : \"z (Wh/km)\" \n# Solution : Corriger les lignes incohérentes. Ajouter \"/electric\" à la col \"Ft\" si : z (Wh/km) != NaN, Fuel consumption != NaN, et Ft ne contient pas \"electric\"\ndf.loc[(df['z (Wh/km)'].notna()) & (df['Fuel consumption '].notna() & ~(df[\"Ft\"].str.contains(\"ELECTRIC\"))), \"Ft\"] += \"/ELECTRIC\"\n\n# Verif incoherance : \"Electric range (km)\" \n# Solution : Corriger les lignes incohérentes. Même solution que pour \"z (Wh/km)\"\n# PAS DE CODE A FAIRE car tout est bon\n\ndf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#temp :\n# Save df :\n#df.to_pickle(\"./train_clean.pkl\")","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"raw","source":"#temp :\ndf = pd.read_pickle(\"./train_clean.pkl\")\ndf","metadata":{"tags":[]}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Check and handle missing features (NaN)","metadata":{}},{"cell_type":"code","source":"# Analyze Non-NaN :\n# Define the possible groupby variables : ['Cr', 'Ft', 'Mh']\n# Before choosing the groupby col, Check if the groupby var has not NaN in x_test.\n\ndef analyse_non_nan(df : pd.DataFrame):\n    \"\"\"Analyse the columns that contains NaN value\n\n    Args:\n        df (pd.DataFrame): database\n        \n    Print : \"column name\" : \"type of column\" | Number of NaN : \"nb\"\n    \"\"\"\n    for i in df.columns :\n        if df[i].isna().any()==False :\n            print(f'{i} : {df[i].dtypes}')\n\nanalyse_non_nan(df=df)","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Analyze NaN :\ndef analyse_nan(df : pd.DataFrame):\n    \"\"\"Analyse the columns that contains NaN value\n\n    Args:\n        df (pd.DataFrame): database\n        \n    Print : \"column name\" : \"type of column\" | Number of NaN : \"nb\"\n    \"\"\"\n    for i in df.columns :\n        if df[i].isna().any() :\n            print(f'{i} : {df[i].dtypes} | Number of NaN (%): {(df[i].isna().sum()/df.shape[0])*100}')\n\nanalyse_nan(df=df)","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list(df.columns)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"raw","source":"Do for all columns, even if there is no NaN in the train set(except the groupby col) :\n\nTarget : \nEwltp (g/km)\n\nGroupby col : \n\"Ft\"\n\nCol to drop : (good)\nMMS\nEnedc (g/km)\nErwltp (g/km)\nDe\nVf\n\nImputation method fix : (good)\n1st/ Imputate the potential groupby col (put them first in the dict) :\n    \"Ft\" : \"unknown\",\n    \"Country\" : \"unknown\",\n2nd/ Imputate the other col : \n\"z (Wh/km)\" : 0 if \"Ft\"!= \"electric\"/\"hybrid\",\n\"Fuel consumption \": 0 if \"Ft\"== \"electric\",\n\"Electric range (km)\": 0 if \"Ft\"!= \"electric\"/\"hybrid\",\n\nImputation method non-fix :\n1st/ Imputate the potential groupby col (put them first in the dict) :\n    'Mh':\"mode\"\n    \"Man\":mode,\n    \"Cr\" : mode,\n2nd/ Imputate the other col : \n\"VFN\" : mode,\n\"Mp\" : mode,\n\"Tan\" : mode,\n\"T\" : mode,\n\"Va\" : mode,\n\"Ve\" : mode,\n\"Mk\" : mode,\n\"Cn\" : mode,\n\"Ct\" : mode,\n\"m (kg)\" : median,\n\"Mt\" : median,\n\"Ernedc (g/km)\" : median,\n\"W (mm)\": median,\n\"At1 (mm)\": median,\n\"At2 (mm)\": median,\n\"Fm \" : mode,\n\"ec (cm3)\": median,\n\"ep (KW)\": median,\n\"z (Wh/km)\" : median if \"Ft\"== \"electric\"/\"hybrid\",\n\"IT\": mode,\n\"Fuel consumption \": median if \"Ft\"!= \"electric\",\n\"Electric range (km)\": median if \"Ft\"== \"electric\"/\"hybrid\"","metadata":{}},{"cell_type":"code","source":"# Delete columns which contains more than 80% of NaN and/or useless :\n\nCol_to_drop = [\"MMS\", \"Enedc (g/km)\", \"Erwltp (g/km)\", \"De\", \"Vf\"]\ndf = df.drop(Col_to_drop, axis=1)","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Impute by fix value :\n# df = df.dropna(subset=[\"\"]).reset_index(drop=True)\n# df[\"\"] = df[\"\"].fillna()\n\n\ndef fillna_fix_value(df, fillna_value):\n    \n    # Recreate the new dict :\n    fillna_value = {key: value for key, value in fillna_value.items() if key in list(df.columns)}\n    \n    for i in list(fillna_value.keys()) :\n        # Cat col :\n        if df[i].dtypes==object :\n            if type(fillna_value[i])==str :\n                df[i] = df[i].fillna(fillna_value[i])\n            else :\n                print(f\"{i} must be a 'str' !\")\n                break\n        \n        # Num col :\n        elif df[i].dtypes==float or df[i].dtypes==int :\n            if type(fillna_value[i])==float or type(fillna_value[i])==int :\n                if i==\"z (Wh/km)\":\n                    # \"z (Wh/km)\" : 0 if \"Ft\"!= \"electric\"/\"hybrid\"('petrol/electric', 'diesel/electric') :\n                    df.loc[~((df[\"Ft\"].str.contains(\"ELECTRIC\"))),i] = df.loc[~((df[\"Ft\"].str.contains(\"ELECTRIC\"))),i].fillna(fillna_value[i])\n                \n                elif i==\"Fuel consumption \":\n                    # \"Fuel consumption \"= 0 if \"Ft\"= \"electric\" :\n                    df.loc[df[\"Ft\"]==\"ELECTRIC\",i] = df.loc[df[\"Ft\"]==\"ELECTRIC\",i].fillna(fillna_value[i])\n                \n                else :    \n                    df[i] = df[i].fillna(fillna_value[i])\n            else :\n                print(f\"{i} must be a 'float' or 'int' !\")\n                break\n    return df\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dictionnary of imputationby fix value (Num and Cat) :\ndict_imputation_fix = {\"Ft\" : \"UNKNOWN\", \"Country\" : \"UNKNOWN\", \"z (Wh/km)\": 0,\n                       \"Fuel consumption \": 0,\"Electric range (km)\": 0}\n\ndf = fillna_fix_value(df=df, fillna_value=dict_imputation_fix)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# To review : Check with the case where the \"groupby\" group has \"NaN\" value, See how this function react","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Impute by non-fix value (create the function for the next imputation): \n\ndef fillna_non_fix(x_train, x_test, fillna_method, groupby_col, display_groupby_col=False):\n    \"\"\"Suppose that the columns name are the same in train and test.\n        Suppose that the groupby col of the x_train has not NaN value.\n\n    Args:\n        x_train (_type_): _description_\n        x_test (_type_): _description_\n        fillna_method (_type_): _description_\n        groupby_col (_type_): _description_\n        groupby_col_comparison (bool, optional): _description_. Defaults to False.\n\n    Returns:\n        _type_: _description_\n    \"\"\"    \n    # Recreate the new dict :\n    fillna_method = {key: value for key, value in fillna_method.items() if key in list(x_train.columns)}\n    \n    # Check values of keys :\n    for i in list(fillna_method.keys()) :  \n        if fillna_method[i] not in [\"mode\",\"median\",\"mean\"] :\n            print (f\"{i} must be imputed by mean, median or mean\")\n            return x_train, x_test\n\n    # While loop :\n    j = len(groupby_col)  \n    while j>=0:\n        \n        if j > 0:\n            try:\n                for i in list(fillna_method.keys()) : \n                    grouped = x_train.groupby(groupby_col[:j])\n                    \n                    if fillna_method[i]==\"mode\" : \n                        mode = grouped[i].agg(lambda x: x.mode().iloc[0] if not x.mode().empty else np.nan)\n                    elif fillna_method[i]==\"median\" : \n                        mode = grouped[i].median()\n                    elif fillna_method[i]==\"mean\" : \n                        mode = grouped[i].mean() \n                    \n                    x_test[i] = x_test.groupby(groupby_col[:j])[i].transform(lambda x: x.fillna(mode[x.name]))\n                    x_train[i] = x_train.groupby(groupby_col[:j])[i].transform(lambda x: x.fillna(mode[x.name]))\n                    \n                    if x_train[i].isna().any()==True or x_test[i].isna().any()==True :\n                        if fillna_method[i]==\"mode\" : \n                            mode = x_train[i].mode()[0]\n                        elif fillna_method[i]==\"median\" : \n                            mode = x_train[i].median()   \n                        elif fillna_method[i]==\"mean\" : \n                            mode = x_train[i].mean() \n\n                        x_test[i] = x_test[i].fillna(mode)\n                        x_train[i] = x_train[i].fillna(mode)\n                        \n                \n                if display_groupby_col==True:\n                    print(groupby_col[:j])\n                \n                j=-1\n                \n            except :\n                j-=1\n    \n        elif j==0: \n            for i in list(fillna_method.keys()) :     \n                if fillna_method[i]==\"mode\" : \n                    mode = x_train[i].mode()[0]\n                elif fillna_method[i]==\"median\" : \n                    mode = x_train[i].median()   \n                elif fillna_method[i]==\"mean\" : \n                    mode = x_train[i].mean() \n        \n                x_test[i] = x_test[i].fillna(mode)\n                x_train[i] = x_train[i].fillna(mode)\n            \n            j=-1\n        \n    return x_train, x_test","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-04-08T19:31:26.431364Z","iopub.execute_input":"2024-04-08T19:31:26.431959Z","iopub.status.idle":"2024-04-08T19:31:26.448864Z","shell.execute_reply.started":"2024-04-08T19:31:26.431907Z","shell.execute_reply":"2024-04-08T19:31:26.447864Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"#* Pour le moment, faire seulement groupby 1 variable :\n\n# Avant de selectionner la col pour le groubpy, on doit verifier en 2 etapes :\n\"\"\"\ncol_for_groupby = \"Ft\"\n# 1/ Verifier les valeurs .unique() avant de choisir le variable groupby :\nfor i in [x_val1, x_val2, x_test]:\n    if all(element in set(x_train[col_for_groupby].unique()) for element in set(i[col_for_groupby].unique()))== False:\n        print(f\"Cannot use the column {col_for_groupby} for groupby\")\n        break\n    else :\n        print(all(element in set(x_train[col_for_groupby].unique()) for element in set(i[col_for_groupby].unique())))\n\n# 2/ Verifier si la variable ne possède pas de NaN dans la base train et test, si c'est le cas, imputer par \"unknown\"\nfor i in [x_train, x_val1, x_val2, x_test]:\n    if i[col_for_groupby].isna().any()== True:\n        print(f\"Cannot use the column {col_for_groupby} for groupby\")\n        break\n    else :\n        print(i[col_for_groupby].isna().any())\n\"\"\"\n\n# Groupby_col : ['Ft'], []\n# Attention : Before choosing the Groupby variable. \n# Verify for each variable, if :\n# - Categorical\n# - Number of unique value is <= 30\n# Verify for each groupby combinaison, if :\n# - The number of unique combinaison is <= 80\n# - Same unique value in train, val1, val2, test\n# - Same unique combinaison in val1, val2, test\n\n# Verifier si l'imputation train/val est la même que train/val2 et que train/test. \n# voir si ca impute bien avec les mêmes combinaisons de groupby.\n\n\n\n# Fillna all groupby(Ft) :\n# groupby_col = ['Ft']\n# dict_imputation_non_fix = {\"Mh\": \"mode\",\"Man\": \"mode\", \"Cr\": \"mode\",\n#                            \"VFN\": \"mode\",\"Mp\": \"mode\",\n#                             \"MMS\": \"median\",\"Tan\": \"mode\",\n#                             \"T\": \"mode\",\"Va\": \"mode\",\n#                             \"Ve\": \"mode\",\"Mk\": \"mode\",\n#                             \"Cn\": \"mode\",\"Ct\": \"mode\",\"m (kg)\": \"median\",\n#                             \"Mt\": \"median\",\"Ernedc (g/km)\": \"median\",\n#                             \"W (mm)\": \"median\",\"At1 (mm)\": \"median\",\n#                             \"At2 (mm)\": \"median\",\"Fm\": \"mode\",\n#                             \"ec (cm3)\": \"median\",\"ep (KW)\": \"median\",\n#                             \"z (Wh/km)\": \"median\",\"IT\": \"mode\",\n#                             \"Fuel consumption \": \"median\",\n#                             \"Electric range (km)\": \"median\"}\n\n\n#x_train, x_test = fillna_non_fix(x_train=x_train, x_test=x_test, fillna_method=dict_imputation_non_fix, groupby_col=groupby_col)","metadata":{"execution":{"iopub.status.busy":"2024-04-08T19:31:26.450947Z","iopub.execute_input":"2024-04-08T19:31:26.451297Z","iopub.status.idle":"2024-04-08T19:31:26.467803Z","shell.execute_reply.started":"2024-04-08T19:31:26.451273Z","shell.execute_reply":"2024-04-08T19:31:26.466875Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"'\\ncol_for_groupby = \"Ft\"\\n# 1/ Verifier les valeurs .unique() avant de choisir le variable groupby :\\nfor i in [x_val1, x_val2, x_test]:\\n    if all(element in set(x_train[col_for_groupby].unique()) for element in set(i[col_for_groupby].unique()))== False:\\n        print(f\"Cannot use the column {col_for_groupby} for groupby\")\\n        break\\n    else :\\n        print(all(element in set(x_train[col_for_groupby].unique()) for element in set(i[col_for_groupby].unique())))\\n\\n# 2/ Verifier si la variable ne possède pas de NaN dans la base train et test, si c\\'est le cas, imputer par \"unknown\"\\nfor i in [x_train, x_val1, x_val2, x_test]:\\n    if i[col_for_groupby].isna().any()== True:\\n        print(f\"Cannot use the column {col_for_groupby} for groupby\")\\n        break\\n    else :\\n        print(i[col_for_groupby].isna().any())\\n'"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3/ Split data as train/val :","metadata":{}},{"cell_type":"code","source":"df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Split :\n\nnum_bins = 10  # Number of bins for stratification\ny_bins = pd.cut(df[target], bins=num_bins, labels=False)\ndf_train, df_val = train_test_split(df, test_size=0.2, stratify=y_bins, random_state=42)\n\n#reset_index :\ndf_train.reset_index(drop = True, inplace=True)\ndf_val.reset_index(drop = True, inplace=True)\n\ndf_train","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4/ Feature engineering :","metadata":{}},{"cell_type":"markdown","source":"## Feature creation (useless)","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature selection","metadata":{}},{"cell_type":"code","source":"## 2.1/ Drop columns used for features creation (option) \n## 2.2/ Drop columns with low variance (num) and have the same value more than 99% of time (cat)\n## 2.3/ Select with correlation method\n## 2.4/ Select with RFE method (option)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 2.1/ Drop columns used for features creation (option) : useless\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 2.2/ Drop columns with low variance (num) and have the same value more than 99.5% of time (cat) :\n\ndef drop_col_with_same_value(df_train, df_test, target):\n    try : \n        col_to_drop = [col for col in df_train.columns if (df_train[col].value_counts().iloc[0]/df_train.shape[0] >= 0.995 and col !=target)]\n    except IndexError :\n        col_to_drop = []\n        for col in df_train.columns :\n            try : \n                if (df_train[col].value_counts().iloc[0]/df_train.shape[0] >= 0.995 and col !=target) :\n                    col_to_drop.append(col)\n            except IndexError :\n                col_to_drop.append(col)\n\n    df_train = df_train.drop(col_to_drop, axis=1)\n    df_test = df_test.drop(col_to_drop, axis=1)  \n    return df_train, df_test\n\n\ndf_train , df_val = drop_col_with_same_value(df_train=df_train, df_test=df_val, target=target)\n\ndf_train.columns","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 2.3/ Select with correlation method : ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"raw","source":"Correlation computing\n\n# Step 1 : Compute corr\nNum-Num :\n- Test de Spearman\n\nCat-Cat :\n- Cramer-V\n\nNum-Cat :\n- Eta carré (η²) ou coefficient de contingence (fonctionne que si la taille de l'échantillon est très élevé)\n- Test de Kruskal-Wallis H Test (si p-value < 0.05, alors il y a une correlation significative)\n\n\n\n# Step 2 : Select features\nSi : \n- Regression : alors, Target = Num\n- Classification : alors, Target = Cat","metadata":{}},{"cell_type":"code","source":"# Function to calculate Cramer's V\n# def cramers_v(x, y):\n    \n#     # Créer un tableau de contingence\n#     contingency_table = pd.crosstab(x, y)\n    \n#     # Effectuer le test du chi-carré\n#     chi2_stat, p_value, dof, expected_freq = chi2_contingency(contingency_table)\n    \n#     # Calculer le coefficient de Cramér-V\n#     n = contingency_table.sum().sum()\n#     min_dim = min(contingency_table.shape) - 1\n#     cramer_v = np.sqrt(chi2_stat / (n * min_dim))\n    \n#     return cramer_v","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Function to calculate ANOVA eta_carre\n\n# def eta_carre(x, y) :\n#     #x=Cat col\n#     #y=Num col\n    \n#     #Rename y serie name :\n#     y = y.copy()\n#     y.name = y.name.split()[0]\n    \n#     # Replace \"specific character\" by \"_\" of x value :\n#     # Define the characters to be replaced\n#     characters_to_replace = ['-', '/', '*', '.', '?', ')', '(']\n#     # Create a regex pattern to match any of the characters to be replaced\n#     regex_pattern = '|'.join(map(re.escape, characters_to_replace))\n#     # Apply the replacement using regex\n#     x = x.apply(lambda s: re.sub(regex_pattern, '_', s) if isinstance(s, str) and not pd.isna(s) else s)\n     \n    \n#     # Convertir la variable qualitative en variables indicatrices (dummies)\n#     data_dummies = pd.get_dummies(x)\n#     new_col_name = {}\n#     for i in data_dummies.columns :\n#         new_col_name[i] = f'{x.name}_{i}'\n#     data_dummies = data_dummies.rename(columns=new_col_name)\n    \n#     # Fusionner les données dummies avec le jeu de données original\n#     data = pd.concat([y, data_dummies], axis=1)\n\n#     # Modèle linéaire\n#     formula = f\"{y.name} ~\"\n#     for i in data_dummies.columns:\n#         formula+= f' {i} +'\n#     formula=formula[:-2]\n#     model = ols(formula, data=data).fit()\n\n#     # ANOVA\n#     result = anova_lm(model, typ=2)\n\n#     # Calcul de l'Eta carré\n#     eta_squared = result['sum_sq'][0] / (result['sum_sq'][0] + result['sum_sq'][1])\n\n#     return eta_squared","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Features_selection : \n# # Ne pas supprimer les col du groupby : ['Ft']\n\n# def Features_selection(df_train, df_test, target, prediction_type, groupby_col=groupby_col, threshold=0.8):\n    \n#     # 0/ Build list for each col type :\n#     list_number_col = df_train.select_dtypes(include=[np.number]).columns.tolist()\n    \n#     # Num list :\n#     # Numeric and > 2 :\n#     list_num_col = [col for col in list_number_col if ((df_train[col].nunique() > 2) & (col!=target) & (col not in groupby_col))]\n\n#     # Cat list :\n#     # Numeric and <= 2 :\n#     list_binary_col = [col for col in list_number_col if ((df_train[col].nunique() <= 2) & (col!=target) & (col not in groupby_col))]\n#     # Categorical col :\n#     list_cat_col = df_train.select_dtypes(include=['object']).columns.tolist()\n#     list_cat_col = [col for col in list_cat_col if ((col!=target) & (df_train[col].nunique() <= 100) & (col not in groupby_col))] + list_binary_col\n    \n    \n    \n    \n#     # 1/ Num-Num :\n#     print(\"Num-Num\")\n#     df_train_num=df_train[list_num_col]\n    \n#     # A/ corr matrix :\n#     corr_matrix_num = df_train_num.corr(numeric_only=True, method=\"spearman\")\n    \n#     # B/ Find pairs of columns with correlation above the threshold :\n#     highly_correlated_pairs_num = []\n#     for i in range(len(corr_matrix_num.columns)):\n#         for j in range(i):\n#             if abs(corr_matrix_num.iloc[i, j]) > threshold:\n#                 colname_i = corr_matrix_num.columns[i]\n#                 colname_j = corr_matrix_num.columns[j]\n#                 highly_correlated_pairs_num.append((colname_i, colname_j, corr_matrix_num.iloc[i, j]))\n#     highly_correlated_pairs_num = sorted(highly_correlated_pairs_num, key=lambda x: x[2], reverse=True)\n    \n#     # C/ Get correlation between explicative num and target :\n#     dict_corr_target_num = {}\n#     if prediction_type==\"regression\":\n#         for i in list_num_col :\n#             correlation, p_value = spearmanr(df_train.dropna(subset=[i]).reset_index(drop=True)[i], df_train.dropna(subset=[i]).reset_index(drop=True)[target])\n#             dict_corr_target_num[i] = correlation\n    \n#     elif prediction_type==\"classification\":\n#         for i in list_num_col :\n#             correlation = eta_carre(df_train.dropna(subset=[i]).reset_index(drop=True)[target], df_train.dropna(subset=[i]).reset_index(drop=True)[i])\n#             dict_corr_target_num[i] = correlation\n    \n#     # D/ Get set of col to drop :\n#     col_to_drop_num = set()\n#     for pair in highly_correlated_pairs_num :\n#         corr_target_0 = dict_corr_target_num[pair[0]]\n#         corr_target_1 = dict_corr_target_num[pair[1]]\n        \n#         if corr_target_0 > corr_target_1 :\n#             col_to_drop_num.add(pair[1])\n            \n#         elif corr_target_0 <= corr_target_1 :\n#             col_to_drop_num.add(pair[0])\n    \n#     # E/ Drop num col :\n#     df_train = df_train.drop(list(col_to_drop_num), axis=1)\n#     df_test = df_test.drop(list(col_to_drop_num), axis=1)\n#     print(list(col_to_drop_num))\n#     print(\"End Num-Num\")\n    \n    \n    \n    \n    \n#     # 2/ Cat-Cat :\n#     print(\"Cat-Cat\")\n    \n#     # A/ Corr matrix : \n#     # List of categorical columns\n#     categorical_columns = list_cat_col\n#     # Initialize an empty matrix\n#     cramer_matrix = np.zeros((len(categorical_columns), len(categorical_columns)))\n#     corr_matrix_cat = pd.DataFrame(cramer_matrix, index=categorical_columns, columns=categorical_columns)\n#     # Calculate Cramer's V for each pair of categorical columns\n#     for i in range(len(categorical_columns)):\n#         for j in range(i+1, len(categorical_columns)):\n#             col1 = df_train.dropna(subset=[categorical_columns[i], categorical_columns[j]]).reset_index(drop=True)[categorical_columns[i]]\n#             col2 = df_train.dropna(subset=[categorical_columns[i], categorical_columns[j]]).reset_index(drop=True)[categorical_columns[j]]\n#             corr_matrix_cat.loc[categorical_columns[j], categorical_columns[i]] = cramers_v(col1, col2)\n    \n#     # B/ Find pairs of columns with correlation above the threshold :\n#     highly_correlated_pairs_cat = []\n#     for i in categorical_columns :\n#         for j in categorical_columns :\n#             if abs(corr_matrix_cat.loc[j, i]) > threshold:\n#                 highly_correlated_pairs_cat.append((i, j, corr_matrix_cat.loc[j, i]))\n#     highly_correlated_pairs_cat = sorted(highly_correlated_pairs_cat, key=lambda x: x[2], reverse=True)\n    \n    \n#     # C/ Get correlation between explicative cat and target :\n#     dict_corr_target_cat = {}\n#     if prediction_type==\"regression\":\n#         for i in corr_matrix_cat :\n#             correlation = eta_carre(df_train.dropna(subset=[i]).reset_index(drop=True)[i], df_train.dropna(subset=[i]).reset_index(drop=True)[target])\n#             dict_corr_target_cat[i] = correlation\n        \n#     elif prediction_type==\"classification\":\n#         for i in corr_matrix_cat :\n#             correlation = cramers_v(df_train.dropna(subset=[i]).reset_index(drop=True)[i], df_train.dropna(subset=[i]).reset_index(drop=True)[target])\n#             dict_corr_target_cat[i] = correlation\n      \n#     # D/ Get set of col to drop :\n#     col_to_drop_cat = set()\n#     for pair in highly_correlated_pairs_cat :\n#         corr_target_0 = dict_corr_target_cat[pair[0]]\n#         corr_target_1 = dict_corr_target_cat[pair[1]]\n        \n#         if corr_target_0 > corr_target_1 :\n#             col_to_drop_cat.add(pair[1])\n            \n#         elif corr_target_0 <= corr_target_1 :\n#             col_to_drop_cat.add(pair[0])\n    \n#     # E/ Drop num col :\n#     df_train = df_train.drop(list(col_to_drop_cat), axis=1)\n#     df_test = df_test.drop(list(col_to_drop_cat), axis=1)\n#     print(list(col_to_drop_cat))\n#     print(\"End Cat-Cat\")\n    \n    \n    \n    \n#     # 3/ Num-Cat : \n#     print(\"Num-Cat\")\n#     # A*/ Build list for each col type :\n#     list_number_col = df_train.select_dtypes(include=[np.number]).columns.tolist()\n    \n#     # Num list :\n#     # Numeric and > 2 :\n#     list_num_col = [col for col in list_number_col if ((df_train[col].nunique() > 2) & (col!=target) & (col not in groupby_col))]\n\n#     # Cat list :\n#     # Numeric and <= 2 :\n#     list_binary_col = [col for col in list_number_col if ((df_train[col].nunique() <= 2) & (col!=target) & (col not in groupby_col))]\n#     # Categorical col :\n#     list_cat_col = df_train.select_dtypes(include=['object']).columns.tolist()\n#     list_cat_col = [col for col in list_cat_col if ((col!=target) & (df_train[col].nunique() <= 100) & (col not in groupby_col))] + list_binary_col\n    \n\n#     # A/ Corr matrix :\n#     # List of categorical columns\n#     numerical_columns = list_num_col\n#     categorical_columns = list_cat_col\n#     # Initialize an empty matrix\n#     eta_matrix = np.zeros((len(numerical_columns), len(categorical_columns)))\n#     # Calculate eta for each pair of num-categorical columns\n#     for i in range(len(categorical_columns)):\n#         for j in range(len(numerical_columns)):\n#             cat_col1 = df_train.dropna(subset=[categorical_columns[i], numerical_columns[j]]).reset_index(drop=True)[categorical_columns[i]]\n#             num_col2 = df_train.dropna(subset=[categorical_columns[i], numerical_columns[j]]).reset_index(drop=True)[numerical_columns[j]]\n#             eta_matrix[j, i] = eta_carre(x=cat_col1, y=num_col2)  \n#     # Create a DataFrame from the matrix\n#     corr_matrix_num_cat = pd.DataFrame(eta_matrix, index=numerical_columns, columns=categorical_columns)\n\n#     # B/ Find pairs of columns with correlation above the threshold :\n#     highly_correlated_pairs_num_cat = []\n#     for i in numerical_columns:\n#         for j in categorical_columns:\n#             if abs(corr_matrix_num_cat.loc[i, j]) > threshold:\n#                 highly_correlated_pairs_num_cat.append((i, j, corr_matrix_num_cat.loc[i, j]))\n#     highly_correlated_pairs_num_cat = sorted(highly_correlated_pairs_num_cat, key=lambda x: x[2], reverse=True)\n    \n#     # C/ Get correlation between explicative Num-cat and target :\n#     dict_corr_target_num_cat = {}\n#     if prediction_type==\"regression\":\n#         # Num-Target :\n#         for i in numerical_columns :\n#             correlation, p_value = spearmanr(df_train.dropna(subset=[i]).reset_index(drop=True)[i], df_train.dropna(subset=[i]).reset_index(drop=True)[target])\n#             dict_corr_target_num_cat[i] = correlation\n            \n#         # Cat-Target :\n#         for i in categorical_columns :\n#             correlation = eta_carre(df_train.dropna(subset=[i]).reset_index(drop=True)[i], df_train.dropna(subset=[i]).reset_index(drop=True)[target])\n#             dict_corr_target_num_cat[i] = correlation\n        \n#     elif prediction_type==\"classification\":\n#         # Num-Target :\n#         for i in numerical_columns :\n#             correlation = eta_carre(df_train.dropna(subset=[i]).reset_index(drop=True)[target], df_train.dropna(subset=[i]).reset_index(drop=True)[i])\n#             dict_corr_target_num_cat[i] = correlation\n        \n#         # Cat-Target :\n#         for i in categorical_columns :\n#             correlation = cramers_v(df_train.dropna(subset=[i]).reset_index(drop=True)[i], df_train.dropna(subset=[i]).reset_index(drop=True)[target])\n#             dict_corr_target_num_cat[i] = correlation\n    \n#     # D/ Get set of col to drop :\n#     col_to_drop_num_cat = set()\n#     for pair in highly_correlated_pairs_num_cat :\n#         corr_target_0 = dict_corr_target_num_cat[pair[0]]\n#         corr_target_1 = dict_corr_target_num_cat[pair[1]]\n        \n#         if corr_target_0 > corr_target_1 :\n#             col_to_drop_num_cat.add(pair[1])\n            \n#         elif corr_target_0 <= corr_target_1 :\n#             col_to_drop_num_cat.add(pair[0])\n    \n#     # E/ Drop num col :\n#     df_train = df_train.drop(list(col_to_drop_num_cat), axis=1)\n#     df_test = df_test.drop(list(col_to_drop_num_cat), axis=1)\n#     print(list(col_to_drop_num_cat))\n#     print(\"End Num-Cat\")\n    \n#     return df_train, df_test\n","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"raw","source":"%%time\n\ndf_train, df_val = Features_selection(df_train=df_train, df_test=df_val, target=target, prediction_type=\"regression\", groupby_col=groupby_col)","metadata":{"tags":[]}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # 2.4/ RFE\n\n# def Feature_selection_RFE(df_train=df_train, df_test=df_val, target=target, fillna_method=dict_imputation_non_fix, groupby_col=groupby_col, display_selected_features=False) :\n    \n#     # Create x, y :\n#     X_train = df_train.drop([target], axis=1).copy()\n#     Y_train = df_train[target].copy()\n#     X_test = df_test.drop([target], axis=1).copy()\n#     Y_test = df_test[target].copy()\n    \n#     # fillna :\n#     X_train, X_test = fillna_non_fix(x_train=X_train, x_test=X_test, fillna_method=fillna_method, groupby_col=groupby_col)\n\n#     # encoding/scaling :\n#     #numeric and > 2 :\n#     list_cont_col = X_train.select_dtypes(include=[np.number]).columns.tolist()\n#     list_cont_col = [col for col in list_cont_col if X_train[col].nunique() > 2]\n#     #numeric and <= 2 :\n#     list_binary_col = X_train.select_dtypes(include=[np.number]).columns.tolist()\n#     list_binary_col = [col for col in list_binary_col if X_train[col].nunique() <= 2]\n#     #categorical col :\n#     list_cat_col = x_train.select_dtypes(exclude=[np.number]).columns.tolist()\n#     list_cat_col_TE = [col for col in list_cat_col if x_train[col].nunique() <= 20]\n#     list_cat_col_CE =  [col for col in list_cat_col if x_train[col].nunique() > 20]\n    \n#     pre_process = pre_processing()\n#     X_train = pre_process.pre_processing(df=X_train, train=True, categorical_var_OHE=[],\n#                                          categorical_var_OrdinalEncoding={}, categorical_var_TE=list_cat_col_TE, \n#                                          target=Y_train, continious_var=list_cont_col, encoding_type_cont=StandardScaler())\n\n#     X_test = pre_process.pre_processing(df=X_test, train=False, categorical_var_OHE=[],\n#                                      categorical_var_OrdinalEncoding={}, categorical_var_TE=list_cat_col_TE, \n#                                      target=Y_train, continious_var=list_cont_col, encoding_type_cont=StandardScaler())\n    \n#     # Count encoding : \n#     count_encoder = CountEncoder(cols=list_cat_col_CE)\n#     X_train = count_encoder.fit_transform(X_train)\n#     X_test = count_encoder.transform(X_test)\n    \n#     # Features selection :\n#     rfe_score = {\"nb_var\" : 1000, \"score_train\": 1000, \"best_score_test\" : 1000}\n\n#     for i in tqdm(range(1,len(X_train.columns)+1)) :\n#         model = LinearRegression()\n#         selector = RFE(model, n_features_to_select=i, step=1)\n#         selector.fit(X_train, Y_train)\n\n#         X_train_new = X_train[list(selector.get_feature_names_out())]\n#         X_test_new = X_test[list(selector.get_feature_names_out())]\n#         model.fit(X_train_new, Y_train)\n\n#         score_train = mean_absolute_error(Y_train, model.predict(X_train_new))\n#         score_test = mean_absolute_error(Y_test, model.predict(X_test_new))\n        \n#         if score_test < rfe_score[\"best_score_test\"] :\n#             rfe_score[\"nb_var\"] = i\n#             rfe_score[\"score_train\"] = score_train\n#             rfe_score[\"best_score_test\"] = score_test\n#             rfe_score[\"selected_features\"] = list(selector.get_feature_names_out())\n    \n#     if display_selected_features==True:\n#         print(f\"RFE_score : {rfe_score}\")\n    \n#     df_train_new = pd.concat([df_train[rfe_score[\"selected_features\"]], df_train[target]], axis=1)\n#     df_test_new = pd.concat([df_test[rfe_score[\"selected_features\"]], df_test[target]], axis=1)\n    \n#     return df_train_new, df_test_new\n    ","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# %%time\n\n# df_train, df_val = Feature_selection_RFE(df_train=df_train, df_test=df_val, target=target, fillna_method=dict_imputation_non_fix, \n#                                          groupby_col=groupby_col, display_selected_features=True)","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Row selection (useless)","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(df_train.shape)\ndisplay(df_val.shape)","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  __________________________________ Beginning : ________________________________","metadata":{"tags":[]}},{"cell_type":"code","source":"#temp :\n# Save df :\n# df_train.to_pickle(\"df_train.pkl\")\n# df_val.to_pickle(\"df_val.pkl\")","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#temp :\ndf_train = pd.read_pickle(\"/kaggle/input/data-co2/df_train.pkl\")\ndf_val = pd.read_pickle(\"/kaggle/input/data-co2/df_val.pkl\")\n\ntarget = \"Ewltp (g/km)\"","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-04-08T19:31:26.468820Z","iopub.execute_input":"2024-04-08T19:31:26.469083Z","iopub.status.idle":"2024-04-08T19:31:53.954829Z","shell.execute_reply.started":"2024-04-08T19:31:26.469061Z","shell.execute_reply":"2024-04-08T19:31:53.953950Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Create x and y","metadata":{}},{"cell_type":"code","source":"def create_x_y(df_train, df_test, target):\n    x_train = df_train.drop([target], axis=1).copy()\n    y_train = df_train[target].copy()\n\n    x_test = df_test.drop([target], axis=1).copy()\n    y_test = df_test[target].copy()\n    \n    return x_train, y_train, x_test, y_test\n\nx_train, y_train, x_val, y_val = create_x_y(df_train=df_train, df_test=df_val, target=target)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-04-08T19:31:53.956254Z","iopub.execute_input":"2024-04-08T19:31:53.956974Z","iopub.status.idle":"2024-04-08T19:32:03.583556Z","shell.execute_reply.started":"2024-04-08T19:31:53.956914Z","shell.execute_reply":"2024-04-08T19:32:03.582726Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"x_train.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5/ Modelling :","metadata":{"tags":[]}},{"cell_type":"markdown","source":"## Display Data treatment methods before modelling ","metadata":{}},{"cell_type":"markdown","source":"#### A/ Fillna non-fix method","metadata":{}},{"cell_type":"raw","source":"# Impute by non-fix value : \n\ndef fillna_non_fix(x_train, x_test, fillna_method, groupby_col, display_groupby_col=False):\n    \"\"\"Suppose that the columns name are the same in train and test.\n        Suppose that the groupby col of the x_train has not NaN value.\n\n    Args:\n        x_train (_type_): _description_\n        x_test (_type_): _description_\n        fillna_method (_type_): _description_\n        groupby_col (_type_): _description_\n        groupby_col_comparison (bool, optional): _description_. Defaults to False.\n\n    Returns:\n        _type_: _description_\n    \"\"\"    \n    # Recreate the new dict :\n    fillna_method = {key: value for key, value in fillna_method.items() if key in list(x_train.columns)}\n    \n    # Check values of keys :\n    for i in list(fillna_method.keys()) :  \n        if fillna_method[i] not in [\"mode\",\"median\",\"mean\"] :\n            print (f\"{i} must be imputed by mean, median or mean\")\n            return x_train, x_test\n\n    # While loop :\n    j = len(groupby_col)  \n    while j>=0:\n        \n        if j > 0:\n            try:\n                for i in list(fillna_method.keys()) : \n                    grouped = x_train.groupby(groupby_col[:j])\n                    \n                    if fillna_method[i]==\"mode\" : \n                        mode = grouped[i].agg(lambda x: x.mode().iloc[0] if not x.mode().empty else np.nan)\n                    elif fillna_method[i]==\"median\" : \n                        mode = grouped[i].median()\n                    elif fillna_method[i]==\"mean\" : \n                        mode = grouped[i].mean() \n                    \n                    x_test[i] = x_test.groupby(groupby_col[:j])[i].transform(lambda x: x.fillna(mode[x.name]))\n                    x_train[i] = x_train.groupby(groupby_col[:j])[i].transform(lambda x: x.fillna(mode[x.name]))\n                    \n                    if x_train[i].isna().any()==True or x_test[i].isna().any()==True :\n                        if fillna_method[i]==\"mode\" : \n                            mode = x_train[i].mode()[0]\n                        elif fillna_method[i]==\"median\" : \n                            mode = x_train[i].median()   \n                        elif fillna_method[i]==\"mean\" : \n                            mode = x_train[i].mean() \n\n                        x_test[i] = x_test[i].fillna(mode)\n                        x_train[i] = x_train[i].fillna(mode)\n                        \n                \n                if display_groupby_col==True:\n                    print(groupby_col[:j])\n                \n                j=-1\n                \n            except :\n                j-=1\n    \n        elif j==0: \n            for i in list(fillna_method.keys()) :     \n                if fillna_method[i]==\"mode\" : \n                    mode = x_train[i].mode()[0]\n                elif fillna_method[i]==\"median\" : \n                    mode = x_train[i].median()   \n                elif fillna_method[i]==\"mean\" : \n                    mode = x_train[i].mean() \n        \n                x_test[i] = x_test[i].fillna(mode)\n                x_train[i] = x_train[i].fillna(mode)\n            \n            j=-1\n        \n    return x_train, x_test","metadata":{"tags":[]}},{"cell_type":"code","source":"# Impute by non-fix value : \n\n# Fillna all groupby(Ft) :\ngroupby_col = ['Ft']\ndict_imputation_non_fix = {\"Mh\": \"mode\",\"Man\": \"mode\", \"Cr\": \"mode\",\n                           \"VFN\": \"mode\",\"Mp\": \"mode\",\n                            \"MMS\": \"median\",\"Tan\": \"mode\",\n                            \"T\": \"mode\",\"Va\": \"mode\",\n                            \"Ve\": \"mode\",\"Mk\": \"mode\",\n                            \"Cn\": \"mode\",\"Ct\": \"mode\",\"m (kg)\": \"median\",\n                            \"Mt\": \"median\",\"Ernedc (g/km)\": \"median\",\n                            \"W (mm)\": \"median\",\"At1 (mm)\": \"median\",\n                            \"At2 (mm)\": \"median\",\"Fm\": \"mode\",\n                            \"ec (cm3)\": \"median\",\"ep (KW)\": \"median\",\n                            \"z (Wh/km)\": \"median\",\"IT\": \"mode\",\n                            \"Fuel consumption \": \"median\",\n                            \"Electric range (km)\": \"median\"}\n\n#x_train, x_test = fillna_non_fix(x_train=x_train, x_test=x_test, fillna_method=dict_imputation_non_fix, groupby_col=groupby_col)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-04-08T19:32:03.584668Z","iopub.execute_input":"2024-04-08T19:32:03.584953Z","iopub.status.idle":"2024-04-08T19:32:03.591282Z","shell.execute_reply.started":"2024-04-08T19:32:03.584929Z","shell.execute_reply":"2024-04-08T19:32:03.590289Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### B/ Encoding/Scaling method","metadata":{}},{"cell_type":"code","source":"#numeric and > 2 :\nlist_cont_col = x_train.select_dtypes(include=[np.number]).columns.tolist()\nlist_cont_col = [col for col in list_cont_col if x_train[col].nunique() > 2]\n\n#numeric and <= 2 :\nlist_binary_col = x_train.select_dtypes(include=[np.number]).columns.tolist()\nlist_binary_col = [col for col in list_binary_col if x_train[col].nunique() <= 2]\n\n#categorical col :\nlist_cat_col = x_train.select_dtypes(exclude=[np.number]).columns.tolist()\nlist_cat_col_OHE = [col for col in list_cat_col if x_train[col].nunique() <= 70]\nlist_cat_col_TE = [] \nlist_cat_col_CE = []\nlist_cat_col_sup20 = [col for col in list_cat_col if x_train[col].nunique() > 70]\n\n# Check if all columns are taken :\nprint(list_cont_col)\nprint(list_binary_col)\nprint(list_cat_col_OHE)\nprint(list_cat_col_TE)\nprint(list_cat_col_CE)\nprint(list_cat_col_sup20)\nlen(list_cont_col) + len(list_binary_col) + len(list_cat_col_OHE) + len(list_cat_col_TE) + len(list_cat_col_CE) + len(list_cat_col_sup20) == x_train.shape[1]","metadata":{"execution":{"iopub.status.busy":"2024-04-08T19:32:03.592372Z","iopub.execute_input":"2024-04-08T19:32:03.592653Z","iopub.status.idle":"2024-04-08T19:32:23.536791Z","shell.execute_reply.started":"2024-04-08T19:32:03.592631Z","shell.execute_reply":"2024-04-08T19:32:23.535569Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"['m (kg)', 'Mt', 'W (mm)', 'At1 (mm)', 'At2 (mm)', 'ec (cm3)', 'ep (KW)', 'z (Wh/km)', 'Fuel consumption ', 'Electric range (km)']\n[]\n['Country', 'Mp', 'Ct', 'Cr', 'Ft', 'Fm']\n[]\n[]\n['VFN', 'Mh', 'Man', 'Tan', 'T', 'Va', 'Ve', 'Mk', 'Cn', 'IT']\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"raw","source":"pre_process = pre_processing()\n            \nX_train = pre_process.pre_processing(df=x_train, train=True, categorical_var_OHE=list_cat_col_OHE,\n                        categorical_var_OrdinalEncoding={}, categorical_var_TE=list_cat_col_TE, target=y_train,\n                        continious_var=[], encoding_type_cont=MinMaxScaler())\n\nX_val = pre_process.pre_processing(df=x_val, train=False, categorical_var_OHE=list_cat_col_OHE,\n                        categorical_var_OrdinalEncoding={}, categorical_var_TE=list_cat_col_TE, target=y_train,\n                        continious_var=[], encoding_type_cont=MinMaxScaler())\n                        \n\n# Count encoding : \nfrom category_encoders import CountEncoder\ncount_encoder = CountEncoder(cols=list_cat_col_CE)\nx_train_process = count_encoder.fit_transform(x_train)\nx_val1_process = count_encoder.transform(x_val)","metadata":{"tags":[]}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model testing (CV method)","metadata":{}},{"cell_type":"markdown","source":"#### 0/ Evaluation metric ","metadata":{}},{"cell_type":"code","source":"# # MAE_score_CV :\n\n# def MAE_score_CV(x_train, y_train, model, fillna_method, groupby_col, list_cat_col_OHE=None, list_cat_col_TE=None, list_cont_col=None, cv=5, random_state=42, encoding=False, display_cv=False):\n    \n#     num_bins = 8  # Number of bins for stratification\n#     y_bins = pd.cut(y_train, bins=num_bins, labels=False)\n#     skf = StratifiedKFold(n_splits=cv, shuffle=True, random_state=random_state) #StratifiedKFold or #KFold\n    \n#     list_score = []\n#     dict_score = {}\n\n#     i=0\n#     for train_idx, valid_idx in skf.split(x_train, y_bins):\n#         X_train, X_val = x_train.loc[train_idx], x_train.loc[valid_idx]\n#         Y_train, Y_val = y_train.loc[train_idx], y_train.loc[valid_idx]\n        \n#         #reset_index() :\n#         X_train = X_train.reset_index(drop = True)\n#         Y_train = Y_train.reset_index(drop = True)\n#         X_val = X_val.reset_index(drop = True)\n#         Y_val = Y_val.reset_index(drop = True)\n        \n#         # fillna :\n#         X_train, X_val = fillna_non_fix(x_train=X_train, x_test=X_val, fillna_method=fillna_method, groupby_col=groupby_col)\n\n#         # encoding :\n#         if encoding==True :\n#             pre_process = pre_processing()\n#             X_train = pre_process.pre_processing(df=X_train, train=True, categorical_var_OHE=list_cat_col_OHE,\n#                                                  categorical_var_OrdinalEncoding={}, categorical_var_TE=list_cat_col_TE, \n#                                                  target=Y_train, continious_var=[], encoding_type_cont=MinMaxScaler())\n\n#             X_val = pre_process.pre_processing(df=X_val, train=False, categorical_var_OHE=list_cat_col_OHE,\n#                                              categorical_var_OrdinalEncoding={}, categorical_var_TE=list_cat_col_TE, \n#                                              target=Y_train, continious_var=[], encoding_type_cont=MinMaxScaler())\n\n#         # training on train set :\n#         model.fit(X_train, Y_train)\n        \n#         #score :\n#         score_train = mean_absolute_error(Y_train, model.predict(X_train))\n#         score_val = mean_absolute_error(Y_val, model.predict(X_val))\n        \n#         dict_score[f\"cv {str(i)}\"]= {\"train\" : score_train, \"val\" : score_val}\n#         list_score.append(score_val)\n        \n#         #temp :\n#         #print(f\"CV {str(i)} : score_train = {score_train} | score_val : {score_val}\")\n            \n#         i+=1\n\n#     if display_cv :\n#         display(dict_score)\n        \n#     return mean(list_score)","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### A/ Catboost (to review)","metadata":{}},{"cell_type":"code","source":"# # Naiv modelling :\n\n# model = CatBoostRegressor(iterations=150, # Number of boosting iterations\n#                                     depth=6, # Depth of the tree\n#                                     learning_rate=0.1, # Step size shrinkage\n#                                     loss_function='MAE', \n#                                     eval_metric='MAE',\n#                                     cat_features=list_cat_col_OHE + list_cat_col_TE, # Indices of categorical features\n#                                     silent=True)","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# %%time\n\n# MAE_score_CV(x_train=x_train, y_train=y_train, model=model, fillna_method=dict_imputation_non_fix, groupby_col=groupby_col, list_cat_col_OHE=None, \n#              list_cat_col_TE=None, list_cont_col=None, cv=5, random_state=42, encoding=False, display_cv=True)","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Bayesian Optimisation (optuna) :\n\n# def objective(trial):\n#     #hyperparameters :\n#     learning_rate = trial.suggest_float(name='learning_rate', low=0.01, high=1)\n#     colsample_bylevel = trial.suggest_float(name=\"colsample_bylevel\", low=0.3, high=1)\n#     depth = trial.suggest_int(name=\"depth\", low=1, high=4)\n#     reg_lambda = trial.suggest_float(name=\"reg_lambda\", low=0.01, high=10)\n#     iterations = trial.suggest_int(name=\"iterations\", low=5, high=300)\n#     random_strength = trial.suggest_float(name=\"random_strength\", low=0, high=10)\n#     bagging_temperature = trial.suggest_float(name=\"bagging_temperature\", low=0, high=10)\n    \n\n#     # instanciate :\n#     # train on df_train :\n#     model = CatBoostRegressor(silent=True, loss_function='MAE', # Use 'MultiClass' for multi-class classification\n#                             eval_metric='MAE',\n#                             cat_features=list_cat_col_OHE + list_cat_col_TE, \n#                             learning_rate=learning_rate,\n#                             colsample_bylevel=colsample_bylevel, depth=depth, reg_lambda=reg_lambda,\n#                             iterations=iterations, random_strength=random_strength, bagging_temperature=bagging_temperature)\n\n#     # score :\n#     score = MAE_score_CV(x_train=x_train, y_train=y_train, model=model, fillna_method=dict_imputation_non_fix, groupby_col=groupby_col, list_cat_col_OHE=None, \n#              list_cat_col_TE=None, list_cont_col=None, cv=3, random_state=42, encoding=False, display_cv=False)\n\n#     return score\n\n\n# study = optuna.create_study(direction='minimize')\n# study.optimize(objective, n_trials=5)","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# trial = study.best_trial\n# print('score : {}'.format(trial.value)) # replace scoring='accuracy' by \"recall\"  #or auc\n# print(\"Best hyperparameters: {}\".format(trial.params))","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Modelling with best hyperparameters : \n\n# best_cat_model = CatBoostRegressor(silent=True, loss_function='MAE', # Use 'MultiClass' for multi-class classification\n#                                     eval_metric='MAE',\n#                                     cat_features=list_cat_col_OHE + list_cat_col_TE, \n#                                     learning_rate=(trial.params)[\"learning_rate\"],\n#                                     colsample_bylevel=(trial.params)[\"colsample_bylevel\"], depth=(trial.params)[\"depth\"], \n#                                     reg_lambda=(trial.params)['reg_lambda'],\n#                                     iterations=(trial.params)[\"iterations\"], random_strength=(trial.params)[\"random_strength\"], \n#                                     bagging_temperature=(trial.params)[\"bagging_temperature\"])","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# %%time\n\n# MAE_score_CV(x_train=x_train, y_train=y_train, model=best_cat_model, fillna_method=dict_imputation_non_fix, groupby_col=groupby_col, list_cat_col_OHE=None, \n#              list_cat_col_TE=None, list_cont_col=None, cv=5, random_state=42, encoding=False, display_cv=True)","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### B/ Xgboost","metadata":{}},{"cell_type":"code","source":"# Naiv modelling :\n\n# model = XGBRegressor(random_state=42)","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# %%time\n\n# MAE_score_CV(x_train=x_train, y_train=y_train, model=model, fillna_method=dict_imputation_non_fix, groupby_col=groupby_col, list_cat_col_OHE=list_cat_col_OHE, \n#              list_cat_col_TE=list_cat_col_TE, list_cont_col=[], cv=2, random_state=42, encoding=True, display_cv=True)","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Bayesian Optimisation (optuna) :\n\n# def objective(trial):\n#     #hyperparameters :\n#     max_depth = trial.suggest_int('max_depth', 0, 50, step=2) #profondeur\n#     learning_rate = trial.suggest_categorical('learning_rate', [0.01,0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1]) \n#     colsample_bytree = trial.suggest_float('colsample_bytree', 0.1, 1, step=0.1) #min leaf of each tree\n#     n_estimators = trial.suggest_int('n_estimators', 1,1001, step=50) #nb of tree\n    \n#     # instanciate :\n#     # train on train set :\n#     model = XGBRegressor(random_state=42, n_jobs=-1, max_depth=max_depth, learning_rate=learning_rate, colsample_bytree=colsample_bytree,\n#                         n_estimators=n_estimators)  #, tree_method='gpu_hist', predictor=\"gpu_predictor\"\n\n#     # score :\n#     score = MAE_score_CV(x_train=x_train, y_train=y_train, model=model, fillna_method=dict_imputation_non_fix, groupby_col=groupby_col, list_cat_col_OHE=list_cat_col_OHE, \n#              list_cat_col_TE=list_cat_col_TE, list_cont_col=[], cv=5, random_state=42, encoding=True, display_cv=False)\n\n#     return score\n\n\n# study = optuna.create_study(direction='minimize')\n# study.optimize(objective, n_trials=50)","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# trial = study.best_trial\n# print('score : {}'.format(trial.value)) \n# print(\"Best hyperparameters: {}\".format(trial.params))","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Modelling with best hyperparameters : \n\n# best_xgb_model =  XGBRegressor(random_state=42, n_jobs=-1, max_depth=(trial.params)[\"max_depth\"], learning_rate=(trial.params)[\"learning_rate\"], \n#                                   colsample_bytree=(trial.params)[\"colsample_bytree\"], n_estimators=(trial.params)[\"n_estimators\"]) #, tree_method='gpu_hist'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# %%time\n\n# MAE_score_CV(x_train=x_train, y_train=y_train, model=best_xgb_model, fillna_method=dict_imputation_non_fix, groupby_col=groupby_col, list_cat_col_OHE=list_cat_col_OHE, \n#              list_cat_col_TE=list_cat_col_TE, list_cont_col=[], cv=5, random_state=42, encoding=True, display_cv=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model testing (train/val1/val2 method)","metadata":{}},{"cell_type":"markdown","source":"#### 0*/ Data treatment","metadata":{}},{"cell_type":"code","source":"# Split x_val/x_val2 :\ntarget = \"Ewltp (g/km)\"\n\nnum_bins = 8  # Number of bins for stratification\ny_bins = pd.cut(df_val[target], bins=num_bins, labels=False)\ndf_val1, df_val2 = train_test_split(df_val, test_size=0.5, stratify=y_bins, random_state=42)\n\n#reset_index :\ndf_val1 = df_val1.reset_index(drop = True)\ndf_val2 = df_val2.reset_index(drop = True)\n\n#create x_val, y_val, x_val2, y_val2 : \nx_val1, y_val1, x_val2, y_val2 = create_x_y(df_train=df_val1, df_test=df_val2, target=target)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-04-08T19:32:23.540584Z","iopub.execute_input":"2024-04-08T19:32:23.541078Z","iopub.status.idle":"2024-04-08T19:32:29.137557Z","shell.execute_reply.started":"2024-04-08T19:32:23.541047Z","shell.execute_reply":"2024-04-08T19:32:29.136704Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fillna :\nx_train_imp, x_val1_imp = fillna_non_fix(x_train=x_train, x_test=x_val1, fillna_method=dict_imputation_non_fix, groupby_col=groupby_col)\n\n# Encoding/Scaling :\n# pre_process = pre_processing()\n# x_train_process = pre_process.pre_processing(df=x_train_imp, train=True, categorical_var_OHE=list_cat_col_OHE,\n#                                      categorical_var_OrdinalEncoding={}, categorical_var_TE=list_cat_col_TE, \n#                                      target=y_train, continious_var=[], encoding_type_cont=MinMaxScaler())\n\n# x_val1_process = pre_process.pre_processing(df=x_val1_imp, train=False, categorical_var_OHE=list_cat_col_OHE,\n#                                  categorical_var_OrdinalEncoding={}, categorical_var_TE=list_cat_col_TE, \n#                                  target=y_train, continious_var=[], encoding_type_cont=MinMaxScaler())","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-04-08T19:32:29.138820Z","iopub.execute_input":"2024-04-08T19:32:29.139204Z","iopub.status.idle":"2024-04-08T19:34:31.441978Z","shell.execute_reply.started":"2024-04-08T19:32:29.139170Z","shell.execute_reply":"2024-04-08T19:34:31.441163Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\nDowncasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n","output_type":"stream"}]},{"cell_type":"code","source":"# modify Encoding/scaling : ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Warning : Use this function after imputation :\n\n# # create a function that can help us to choose the best encoding method (with spearman correlation method)\n# # apply this only for the variables which containt more than 20 values \n# # Warning : compute the corr differently between regression and classification.\n\n# def encoding_selection(project_type : str, var_to_encode : list, x_train : pd.DataFrame, y_train : pd.DataFrame) : \n#     # project_type = regression or binary_classification\n#     # var_to_encode = list_cat_col_TE + list_cat_col_CE\n#     # x_train = train x set\n#     # y_tran = train y set\n#     # output 0 = list_cat_col_TE \n#     # output 1 = list_cat_col_CE\n    \n#     # Create dict for collecting the corr between cat_var and target :\n#     dict_best_enc = {}\n\n#     # Step 1 : Encode with the count_encoding :\n#     print(\"Step 1 : count_encoding\")\n#     count_encoder = CountEncoder(cols=var_to_encode)\n#     x_train_encoded = count_encoder.fit_transform(x_train)\n    \n#     # Compute the correlation between the col of var_to_encode and the target :    \n#     if project_type==\"regression\":\n#         for cat_col in tqdm(var_to_encode) :\n#             correlation, p_value = spearmanr(x_train_encoded[cat_col], y_train)\n#             dict_best_enc[cat_col] = [correlation, \"count_encoding\"]\n    \n#     elif project_type==\"binary_classification\":\n#         return \"Not working for classif now\"\n    \n#     #temp :\n#     print(f\" count_encoding : {dict_best_enc}\")\n    \n#     # Step 2 : Encode with target_encoding :\n#     print(\"Step 2 : target_encoding\")\n#     pre_process = pre_processing()\n#     x_train_encoded = pre_process.pre_processing(df=x_train, train=True, categorical_var_OHE=[],\n#                                      categorical_var_OrdinalEncoding={}, categorical_var_TE=var_to_encode, \n#                                      target=y_train, continious_var=[], encoding_type_cont=MinMaxScaler())\n    \n#     # Compute the correlation between the col of var_to_encode and the target :\n#     if project_type==\"regression\":\n#         for cat_col in tqdm(var_to_encode) :\n#             correlation, p_value = spearmanr(x_train_encoded[cat_col], y_train)\n            \n#             if abs(correlation) >= abs(dict_best_enc[cat_col][0]) :\n#                 dict_best_enc[cat_col] = [correlation, \"target_encoding\"]\n    \n#     elif project_type==\"binary_classification\":\n#         return \"Not working for classif now\"\n      \n#     list_cat_col_TE = [key for key in dict_best_enc.keys() if dict_best_enc[key][1]==\"target_encoding\"]\n#     list_cat_col_CE = [key for key in dict_best_enc.keys() if dict_best_enc[key][1]==\"count_encoding\"]\n    \n#     print(dict_best_enc)\n#     return list_cat_col_TE, list_cat_col_CE\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # test : \n\n# # Warning : Use this function after imputation :\n# # create a function that can help us to choose the best encoding method (with spearman correlation method)\n# # apply this only for the variables which containt more than 20 values \n# # Warning : compute the corr differently between regression and classification.\n\n# def encoding_selection(project_type : str, list_cat_col_sup20 : list, list_cat_col_OHE : list, x_train : pd.DataFrame, \n#                        y_train : pd.DataFrame, x_test : pd.DataFrame, y_test : pd.DataFrame) : \n#     # project_type = regression or binary_classification\n#     # list_cat_col_sup20 = list_cat_col_TE + list_cat_col_CE\n#     # list_cat_col_OHE = list_cat_col_OHE\n#     # x_train = train x set\n#     # y_tran = train y set\n#     # x_test = test x set\n#     # y_test = test y set\n#     # output 0 = list_cat_col_TE \n#     # output 1 = list_cat_col_CE\n    \n#     # Stpe 0 :\n#     # Encoding with OHE :\n#     pre_process = pre_processing()\n#     x_train_OHE = pre_process.pre_processing(df=x_train, train=True, categorical_var_OHE=list_cat_col_OHE,\n#                                          categorical_var_OrdinalEncoding={}, categorical_var_TE=[], \n#                                          target=y_train, continious_var=[], encoding_type_cont=MinMaxScaler())\n\n#     x_test_OHE = pre_process.pre_processing(df=x_test, train=False, categorical_var_OHE=list_cat_col_OHE,\n#                                      categorical_var_OrdinalEncoding={}, categorical_var_TE=[], \n#                                      target=y_train, continious_var=[], encoding_type_cont=MinMaxScaler())\n\n#     # Create dict for collecting the corr between cat_var and target :\n#     dict_best_enc = {}\n    \n    \n#     # Step 1 : \n#     # Encode with the count_encoding :\n#     print(\"Step 1 : count_encoding\")\n#     count_encoder = CountEncoder(cols=list_cat_col_sup20)\n#     x_train_encoded = count_encoder.fit_transform(x_train_OHE)\n#     x_test_encoded = count_encoder.transform(x_test_OHE)\n    \n#     # Compute the correlation between the col of list_cat_col_sup20 and the target :    \n#     if project_type==\"regression\":  \n#         for cat_col in tqdm(list_cat_col_sup20) :\n#             # Config new data : \n#             x_train_cat_col = pd.concat([x_train_encoded.drop(columns=list_cat_col_sup20),x_train_encoded[cat_col]], axis=1)\n#             x_test_cat_col = pd.concat([x_test_encoded.drop(columns=list_cat_col_sup20),x_test_encoded[cat_col]], axis=1)\n#             # Config model :\n#             model = XGBRegressor(random_state=42, \n#                          objective=\"reg:squarederror\", #reg:squarederror, reg:squaredlogerror, reg:pseudohubererror, reg:absoluteerror, reg:quantileerror\n#                          early_stopping_rounds=20, #int [10;30]\n#                          eval_metric=\"mae\", #MAE, MSE, MAPE, R2\n#                          tree_method=\"auto\", #auto, hist, approx, exact \n#                          n_jobs=-1) #tree_method=\"hist\", device=\"cuda\")\n#             # Training :\n#             model.fit(x_train_cat_col, y_train, eval_set=[(x_test_cat_col, y_test)], verbose=False)\n#             # Score :\n#             score = mean_absolute_error(y_test, model.predict(x_test_cat_col))\n#             # Add score to the dict_best_enc :\n#             dict_best_enc[cat_col] = [score, \"count_encoding\"]\n    \n#     elif project_type==\"binary_classification\":\n#         return \"Not working for classif case now\"\n    \n    \n    \n#     # Step 2 : \n#     # Encode with target_encoding :\n#     print(\"Step 2 : target_encoding\")\n#     pre_process = pre_processing()\n#     x_train_encoded = pre_process.pre_processing(df=x_train_OHE, train=True, categorical_var_OHE=[],\n#                                      categorical_var_OrdinalEncoding={}, categorical_var_TE=list_cat_col_sup20, \n#                                      target=y_train, continious_var=[], encoding_type_cont=MinMaxScaler())\n#     x_test_encoded = pre_process.pre_processing(df=x_test_OHE, train=False, categorical_var_OHE=[],\n#                                  categorical_var_OrdinalEncoding={}, categorical_var_TE=list_cat_col_sup20, \n#                                  target=y_train, continious_var=[], encoding_type_cont=MinMaxScaler())\n    \n#     # Compute the correlation between the col of list_cat_col_sup20 and the target :\n#     if project_type==\"regression\":\n#         for cat_col in tqdm(list_cat_col_sup20) :\n#             # Config new data :\n#             x_train_cat_col = pd.concat([x_train_encoded.drop(columns=list_cat_col_sup20),x_train_encoded[cat_col]], axis=1)\n#             x_test_cat_col = pd.concat([x_test_encoded.drop(columns=list_cat_col_sup20),x_test_encoded[cat_col]], axis=1)\n#             # Config model :\n#             model = XGBRegressor(random_state=42, \n#                          objective=\"reg:squarederror\", #reg:squarederror, reg:squaredlogerror, reg:pseudohubererror, reg:absoluteerror, reg:quantileerror\n#                          early_stopping_rounds=20, #int [10;30]\n#                          eval_metric=\"mae\", #MAE, MSE, MAPE, R2\n#                          tree_method=\"auto\", #auto, hist, approx, exact \n#                          n_jobs=-1) # tree_method=\"hist\", device=\"cuda\")\n#             # Training :\n#             model.fit(x_train_cat_col, y_train,eval_set=[(x_test_cat_col, y_test)], verbose=False)\n#             # Score :\n#             score = mean_absolute_error(y_test, model.predict(x_test_cat_col))\n#             # Add score to the dict_best_enc if score with TE encoding is better :\n#             if score <= dict_best_enc[cat_col][0] :\n#                 dict_best_enc[cat_col] = [score, \"target_encoding\"]\n    \n#     elif project_type==\"binary_classification\":\n#         return \"Not working for classif case now\"\n      \n#     list_cat_col_TE = [key for key in dict_best_enc.keys() if dict_best_enc[key][1]==\"target_encoding\"]\n#     list_cat_col_CE = [key for key in dict_best_enc.keys() if dict_best_enc[key][1]==\"count_encoding\"]\n    \n#     print(dict_best_enc)\n#     return list_cat_col_TE, list_cat_col_CE\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# list_cat_col_TE, list_cat_col_CE = encoding_selection(project_type=\"regression\", list_cat_col_sup20=list_cat_col_sup20, list_cat_col_OHE=list_cat_col_OHE,\n#                                                       x_train=x_train_imp, y_train=y_train, x_test=x_val1_imp, y_test=y_val1)\n\n# print(list_cat_col_TE)\n# print(list_cat_col_CE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# list_cat_col_TE = ['Country', 'VFN', 'Tan', 'T', 'Va', 'Ve', 'Mk', 'Cn', 'IT']\n# list_cat_col_CE = ['Mh', 'Man']\n\nlist_cat_col_TE = []\nlist_cat_col_CE = list_cat_col_sup20","metadata":{"execution":{"iopub.status.busy":"2024-04-08T19:34:31.443140Z","iopub.execute_input":"2024-04-08T19:34:31.443437Z","iopub.status.idle":"2024-04-08T19:34:31.447999Z","shell.execute_reply.started":"2024-04-08T19:34:31.443413Z","shell.execute_reply":"2024-04-08T19:34:31.447063Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"#  Temp :\n\n# Encoding/Scaling :\n\n# Target encoding and OHE : \npre_process = pre_processing()\nx_train_process = pre_process.pre_processing(df=x_train_imp, train=True, categorical_var_OHE=list_cat_col_OHE,\n                                     categorical_var_OrdinalEncoding={}, categorical_var_TE=list_cat_col_TE, \n                                     target=y_train, continious_var=[], encoding_type_cont=MinMaxScaler())\n\nx_val1_process = pre_process.pre_processing(df=x_val1_imp, train=False, categorical_var_OHE=list_cat_col_OHE,\n                                 categorical_var_OrdinalEncoding={}, categorical_var_TE=list_cat_col_TE, \n                                 target=y_train, continious_var=[], encoding_type_cont=MinMaxScaler())\n\n\n# Count encoding : \nfrom category_encoders import CountEncoder\ncount_encoder = CountEncoder(cols=list_cat_col_CE)\nx_train_process = count_encoder.fit_transform(x_train_process)\nx_val1_process = count_encoder.transform(x_val1_process)","metadata":{"execution":{"iopub.status.busy":"2024-04-08T19:34:31.449252Z","iopub.execute_input":"2024-04-08T19:34:31.449866Z","iopub.status.idle":"2024-04-08T19:37:43.924913Z","shell.execute_reply.started":"2024-04-08T19:34:31.449833Z","shell.execute_reply":"2024-04-08T19:37:43.924092Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"`sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### A*/ Catboost (to review)","metadata":{}},{"cell_type":"code","source":"# Pool train and val :\ntrain_pool = Pool(data=x_train_imp, label=y_train, cat_features=list_cat_col_OHE+list_cat_col_TE+list_cat_col_CE)\nval1_pool = Pool(data=x_val1_imp, label=y_val1, cat_features=list_cat_col_OHE+list_cat_col_TE+list_cat_col_CE)","metadata":{"execution":{"iopub.status.busy":"2024-04-08T19:37:43.926039Z","iopub.execute_input":"2024-04-08T19:37:43.926328Z","iopub.status.idle":"2024-04-08T19:38:05.531307Z","shell.execute_reply.started":"2024-04-08T19:37:43.926305Z","shell.execute_reply":"2024-04-08T19:38:05.530228Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# %%time\n# # Naiv modelling :\n\n# # Instanciate :\n# model = CatBoostRegressor(random_state=42, silent=True,\n#                      loss_function=\"RMSE\", # alias objective : MAE, MAPE, RMSE \n#                      early_stopping_rounds=20, #int [10;30]\n#                      eval_metric=\"MAE\", #MAE, MAPE, RMSE, R2\n#                      #cat_features=list_cat_col_OHE + list_cat_col_TE, # Indices of categorical features\n#                      #task_type=\"GPU\"\n#                     ) #task_type=\"GPU\"\n\n# # Training :\n# model.fit(train_pool,\n#             eval_set=val1_pool,\n#             verbose=False, plot=False\n#             ) \n\n\n# # MAE score :\n# print(f\"MAE on train : {mean_absolute_error(y_train, model.predict(x_train))}\")\n# print(f\"MAE on val : {mean_absolute_error(y_val1, model.predict(x_val1))}\") #y_val1, x_val1_process","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Bayesian Optimisation (optuna) :\n\ndef objective(trial):\n    #hyperparameters :\n    learning_rate = trial.suggest_float(name='learning_rate', low=0.001, high=0.3)\n    #colsample_bylevel = trial.suggest_float(name=\"colsample_bylevel\", low=0.5, high=1)\n    depth = trial.suggest_int(name=\"depth\", low=1, high=15)\n    reg_lambda = trial.suggest_float(name=\"reg_lambda\", low=0.01, high=20)\n    #l2_leaf_reg=trial.suggest_int(name=\"depth\", low=1, high=50)\n    #model_size_reg\n    #max_ctr_complexity\n    #ctr_leaf_count_limit\n    iterations = trial.suggest_int(name=\"iterations\", low=5, high=6000)\n    random_strength = trial.suggest_float(name=\"random_strength\", low=0, high=10)\n    bagging_temperature = trial.suggest_float(name=\"bagging_temperature\", low=0, high=10)\n    \n    # train on train set :\n    model = CatBoostRegressor(random_state=42, silent=True,\n                              loss_function=\"RMSE\", # alias objective : MAE, MAPE, RMSE \n                              early_stopping_rounds=40, #int [10;30]\n                              eval_metric=\"MAE\", #MAE, MAPE, RMSE, R2 \n                              learning_rate=learning_rate,\n                              #colsample_bylevel=colsample_bylevel, \n                              depth=depth, reg_lambda=reg_lambda,\n                              iterations=iterations, random_strength=random_strength, bagging_temperature=bagging_temperature, \n                              task_type=\"GPU\", devices='0') #task_type=\"GPU\", devices='0'\n    \n    # Training :\n    model.fit(train_pool,\n            eval_set=val1_pool,\n            verbose=False\n            ) \n    \n    # score :\n    score = mean_absolute_error(y_val1, model.predict(x_val1_imp))\n\n    return score\n\n\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=50)","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Catboost_trial = study.best_trial\nprint('score : {}'.format(Catboost_trial.value)) \nprint(\"Best hyperparameters: {}\".format(Catboost_trial.params))","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Best_hyp_cat = {'learning_rate': 0.15625836094606832, 'depth': 15, 'reg_lambda': 0.4188523993108593, 'iterations': 3668, 'random_strength': 5.401731949242277, 'bagging_temperature': 9.407271200260343}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluate the best model ","metadata":{}},{"cell_type":"code","source":"# Pool train and val :\n#train_pool = Pool(data=x_train_imp, label=y_train, cat_features=list_cat_col_OHE+list_cat_col_TE)\n#val1_pool = Pool(data=x_val1_imp, label=y_val1, cat_features=list_cat_col_OHE+list_cat_col_TE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Modelling with best hyperparameters : \n\nbest_cat_model =  CatBoostRegressor(random_state=42, silent=True,\n                              loss_function=\"RMSE\", # alias objective : MAE, MAPE, RMSE \n                              early_stopping_rounds=40, #int [10;30]\n                              eval_metric=\"MAE\", #MAE, MAPE, RMSE, R2 \n                              learning_rate=(Catboost_trial.params)[\"learning_rate\"],\n                              #colsample_bylevel=(Catboost_trial.params)[\"colsample_bylevel\"], \n                              depth=(Catboost_trial.params)[\"depth\"], \n                              reg_lambda=(Catboost_trial.params)[\"reg_lambda\"], iterations=(Catboost_trial.params)[\"iterations\"], \n                              random_strength=(Catboost_trial.params)[\"random_strength\"], bagging_temperature=(Catboost_trial.params)[\"bagging_temperature\"], \n                              task_type=\"GPU\", devices=\"0\")  #task_type=\"GPU\", devices=\"0\"\n        \n# training on train set :\nbest_cat_model.fit(train_pool,\n            eval_set=val1_pool,\n            verbose=False, plot=False\n            ) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #mean absolute error\n# print(\"MAE\")\n# print(mean_absolute_error(y_train, best_cat_model.predict(x_train_imp))) #, multi_class='ovr'\n# print(mean_absolute_error(y_val1, best_cat_model.predict(x_val1_imp))) #, multi_class='ovr'\n# print(\" \")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Save the model temporary :\n# joblib.dump(value = best_cat_model, filename = './MODEL/Temp/train_val1_val2/best_cat_model.pkl')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# temp : \nBest_hyp_cat = {'learning_rate': 0.15625836094606832, 'depth': 15, 'reg_lambda': 0.4188523993108593, 'iterations': 3668, 'random_strength': 5.401731949242277, 'bagging_temperature': 9.407271200260343}\n\n# Modelling with best hyperparameters : \nbest_cat_model =  CatBoostRegressor(random_state=42, silent=True,\n                              loss_function=\"RMSE\", # alias objective : MAE, MAPE, RMSE \n                              early_stopping_rounds=30, #int [10;30]\n                              eval_metric=\"MAE\", #MAE, MAPE, RMSE, R2 \n                              learning_rate=(Best_hyp_cat)[\"learning_rate\"],\n                              #colsample_bylevel=(Catboost_trial.params)[\"colsample_bylevel\"], \n                              depth=(Best_hyp_cat)[\"depth\"], \n                              reg_lambda=(Best_hyp_cat)[\"reg_lambda\"], iterations=(Best_hyp_cat)[\"iterations\"], \n                              random_strength=(Best_hyp_cat)[\"random_strength\"], bagging_temperature=(Best_hyp_cat)[\"bagging_temperature\"],  \n                              task_type=\"GPU\", devices=\"0\")\n        \n# training on train set :\nbest_cat_model.fit(train_pool,\n            eval_set=val1_pool,\n            verbose=False, plot=False\n            )\n\n# save :\njoblib.dump(value = best_cat_model, filename = 'best_cat_model.pkl')","metadata":{"execution":{"iopub.status.busy":"2024-04-08T17:38:26.001992Z","iopub.execute_input":"2024-04-08T17:38:26.002347Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"Default metric period is 5 because MAE is/are not implemented for GPU\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### B*/ Xgboost","metadata":{}},{"cell_type":"code","source":"# %%time\n# # Naiv modelling :\n\n# # Instanciate :\n# model = XGBRegressor(random_state=42, \n#                      objective=\"reg:squarederror\", #reg:squarederror, reg:squaredlogerror, reg:pseudohubererror, reg:absoluteerror, reg:quantileerror\n#                      early_stopping_rounds=20, #int [10;30]\n#                      eval_metric=\"mae\", #MAE, MSE, MAPE, R2\n#                      #tree_method=\"auto\", #auto, hist, approx, exact \n#                      n_jobs=-1)\n#                      #tree_method=\"hist\", device=\"cuda\")\n\n# # Training :\n# model.fit(x_train_proc_test, y_train, \n#           eval_set=[(x_val1_proc_test, y_val1)],\n#           verbose=False\n#          ) \n\n# # MAE score :\n# print(f\"MAE on train : {mean_absolute_error(y_train, model.predict(x_train_proc_test))}\")\n# print(f\"MAE on val : {mean_absolute_error(y_val1, model.predict(x_val1_proc_test))}\") #y_val1, x_val1_process","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Bayesian Optimisation (optuna) :\n\ndef objective(trial):\n    \n    #hyperparameters :\n    n_estimators = trial.suggest_int('n_estimators', 500,7000, step=10) #nb of tree\n    learning_rate = trial.suggest_float('learning_rate', 0.01, 0.2, step=0.0001) \n    #subsample = trial.suggest_float('subsample', 0.6, 1, step=0.01)\n    colsample_bytree = trial.suggest_float('colsample_bytree', 0.6, 1, step=0.01)\n    reg_lambda = trial.suggest_float('reg_lambda', 0, 20, step=0.1) #lambda penalty\n    min_split_loss = trial.suggest_float('min_split_loss', 0, 20, step=0.1) #Gamma penalty\n    max_depth = trial.suggest_int('max_depth', 21, 51, step=2) #max depth of each trees\n    #max_leaves = trial.suggest_int('max_leaves', 6, 100, step=2) #max leaves of each trees\n    #tree_method = trial.suggest_categorical('tree_method', [\"auto\", \"hist\", \"approx\"]) #auto, hist, approx, exact\n    \n    # instanciate :\n    # train on train set :\n    model = XGBRegressor(random_state=42, \n                     objective=\"reg:squarederror\",\n                     early_stopping_rounds=40, #int [10;30]\n                     eval_metric=\"mae\", #MAE, MSE, MAPE, R2\n                     #tree_method=tree_method, #auto, hist, approx, exact \n                     n_jobs=-1,\n                     n_estimators=n_estimators, learning_rate=learning_rate,\n                     reg_lambda=reg_lambda, min_split_loss=min_split_loss,\n                     max_depth=max_depth, colsample_bytree=colsample_bytree\n                     , tree_method='hist', device=\"cuda\")  #, tree_method='hist', device=\"cuda\"\n    \n    # Training :\n    model.fit(x_train_process, y_train, \n            eval_set=[(x_val1_process, y_val1)],\n            verbose=False\n            )\n    \n    # score :\n    score = mean_absolute_error(y_val1, model.predict(x_val1_process))\n\n    return score\n\n\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=50)","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"XGB_trial = study.best_trial \nprint('score : {}'.format(XGB_trial.value))\nprint(\"Best hyperparameters: {}\".format(XGB_trial.params))","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Modelling with best hyperparameters : \n\nbest_xgb_model =  XGBRegressor(random_state=42, \n                     objective=\"reg:squarederror\",\n                     early_stopping_rounds=40, #int [10;30]\n                     eval_metric=\"mae\", #MAE, MSE, MAPE, R2\n                     #tree_method=(XGB_trial.params)[\"tree_method\"], #auto, hist, approx, exact \n                     n_jobs=-1,\n                     n_estimators=(XGB_trial.params)[\"n_estimators\"], learning_rate=(XGB_trial.params)[\"learning_rate\"],\n                     reg_lambda=(XGB_trial.params)[\"reg_lambda\"], min_split_loss=(XGB_trial.params)[\"min_split_loss\"],\n                     max_depth=(XGB_trial.params)[\"max_depth\"], colsample_bytree=(XGB_trial.params)[\"colsample_bytree\"]\n                     , tree_method='hist', device=\"cuda\")  #, tree_method='hist', device=\"cuda\"\n\n# Training :\nbest_xgb_model.fit(x_train_process, y_train, \n                  eval_set=[(x_val1_process, y_val1)],\n                  verbose=False\n                  )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#mean absolute error\nprint(\"MAE\")\nprint(mean_absolute_error(y_train, best_xgb_model.predict(x_train_process))) #, multi_class='ovr'\nprint(mean_absolute_error(y_val1, best_xgb_model.predict(x_val1_process))) #, multi_class='ovr'\nprint(\" \")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Score on x_val1 : 2.81178","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the model temporary :\n# joblib.dump(value = best_xgb_model, filename = './MODEL/Temp/train_val1_val2/best_xgb_model.pkl')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# temp :\n\n# Modelling with best hyperparameters : \ndict_best_hyp = {'n_estimators': 550, 'learning_rate': 0.021900000000000003, 'colsample_bytree': 0.7, 'reg_lambda': 0.9, 'min_split_loss': 7.300000000000001, 'max_depth': 49}\n\nbest_xgb_model =  XGBRegressor(random_state=42, \n                     objective=\"reg:squarederror\",\n                     early_stopping_rounds=40, #int [10;30]\n                     eval_metric=\"mae\", #MAE, MSE, MAPE, R2\n                     #tree_method=(XGB_trial.params)[\"tree_method\"], #auto, hist, approx, exact \n                     n_jobs=-1,\n                     n_estimators=(dict_best_hyp)[\"n_estimators\"], learning_rate=(dict_best_hyp)[\"learning_rate\"],\n                     reg_lambda=(dict_best_hyp)[\"reg_lambda\"], min_split_loss=(dict_best_hyp)[\"min_split_loss\"],\n                     max_depth=(dict_best_hyp)[\"max_depth\"], colsample_bytree=(dict_best_hyp)[\"colsample_bytree\"]\n                     , tree_method='hist', device=\"cuda\")  #, tree_method='hist', device=\"cuda\"\n\n# Training :\nbest_xgb_model.fit(x_train_process, y_train, \n                  eval_set=[(x_val1_process, y_val1)],\n                  verbose=False\n                  )\n\n# save :\njoblib.dump(value = best_xgb_model, filename = 'best_xgb_model.pkl')","metadata":{"execution":{"iopub.status.busy":"2024-04-08T19:38:05.532476Z","iopub.execute_input":"2024-04-08T19:38:05.532761Z","iopub.status.idle":"2024-04-08T19:45:46.131975Z","shell.execute_reply.started":"2024-04-08T19:38:05.532737Z","shell.execute_reply":"2024-04-08T19:45:46.130982Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"['best_xgb_model.pkl']"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### C*/ Random forest","metadata":{}},{"cell_type":"code","source":"# %%time\n# # Naiv modelling :\n\n# # Instanciate :\n# model = RandomForestRegressor(random_state=42, n_jobs=-1)\n\n# # Training :\n# model.fit(x_train_process, y_train)\n\n# # MAE score :\n# print(f\"MAE on train : {mean_absolute_error(y_train, model.predict(x_train_process))}\")\n# print(f\"MAE on val : {mean_absolute_error(y_val1, model.predict(x_val1_process))}\") #y_val1, x_val1_process","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Bayesian Optimisation (optuna) :\n\n# def objective(trial):\n    \n#     #hyperparameters :\n#     max_depth = trial.suggest_int('max_depth', 21, 50, step=2) #profondeur\n#     max_features = trial.suggest_categorical('max_features', [\"log2\",\"sqrt\",None]) \n#     min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 10, step=1) #min leaf of each tree\n#     max_leaf_nodes = trial.suggest_int('max_leaf_nodes', 1, 10, step=1) #max leaf of each tree\n#     min_samples_split = trial.suggest_int('min_samples_split', 4, 20, step=1)\n#     n_estimators = trial.suggest_int('n_estimators', 40,500, step=5) #nb of tree\n\n    \n#     # instanciate :\n#     # train on train set :\n#     model = RandomForestRegressor(random_state=42, n_jobs=-1, max_depth=max_depth, max_features=max_features, min_samples_leaf=min_samples_leaf, \n#                                 max_leaf_nodes=max_leaf_nodes, min_samples_split=min_samples_split, n_estimators=n_estimators)\n\n#     # Training :\n#     model.fit(x_train_process, y_train)\n    \n#     # score :\n#     score = mean_absolute_error(y_val1, model.predict(x_val1_process))\n\n#     return score\n\n\n# study = optuna.create_study(direction='minimize')\n# study.optimize(objective, n_trials=50)","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# rf_trial = study.best_trial\n# print('score : {}'.format(rf_trial.value)) \n# print(\"Best hyperparameters: {}\".format(rf_trial.params))","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Modelling with best hyperparameters : \n\n# best_rf_model = RandomForestRegressor(random_state=42, n_jobs=-1\n# #                                       , max_depth=(rf_trial.params)[\"max_depth\"], max_features=(rf_trial.params)[\"max_features\"], \n# #                                       min_samples_leaf=(rf_trial.params)[\"min_samples_leaf\"], max_leaf_nodes=(rf_trial.params)[\"max_leaf_nodes\"], \n# #                                       min_samples_split=(rf_trial.params)[\"min_samples_split\"], n_estimators=(rf_trial.params)[\"n_estimators\"]\n#                                      )\n\n# # Training :\n# best_rf_model.fit(x_train_process, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the model temporary :\n#joblib.dump(value = best_rf_model, filename = './MODEL/Temp/train_val1_val2/best_rf_model.pkl')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### D*/ LGBM","metadata":{}},{"cell_type":"code","source":"# %%time\n# # Naiv modelling :\n\n# # Instanciate :\n# model = LGBMRegressor(random_state=42, n_jobs=-1)\n\n# # Training :\n# model.fit(x_train_process, y_train)\n\n# # MAE score :\n# print(f\"MAE on train : {mean_absolute_error(y_train, model.predict(x_train_process))}\")\n# print(f\"MAE on val : {mean_absolute_error(y_val1, model.predict(x_val1_process))}\") #y_val1, x_val1_process","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Bayesian Optimisation (optuna) :\n\ndef objective(trial):\n    max_depth = trial.suggest_int('max_depth', 21, 51, step=2) #max depth of each trees\n    learning_rate = trial.suggest_float('learning_rate', 0.001, 0.3, step=0.001) \n    n_estimators = trial.suggest_int('n_estimators', 400,8000, step=10) #nb of tree\n    #boosting_type = trial.suggest_categorical('boosting_type', [\"gbdt\", \"dart\"])\n    num_leaves = trial.suggest_int('num_leaves', 10,200,step=2)\n    #feature_fraction = trial.suggest_float('feature_fraction', 0.1,0.999)\n    subsample = trial.suggest_float('subsample', 0.1,0.999)\n    reg_alpha = trial.suggest_float('reg_alpha', 0.001,0.999)\n    reg_lambda = trial.suggest_float('reg_lambda', 0.001,0.999)\n\n    model = LGBMRegressor(random_state=42, n_jobs=-1, verbose=-1, objective=\"regression\",\n                          early_stopping_rounds=40, #int [10;30]\n                          eval_metric=\"mae\", #MAE, MSE, MAPE, R2\n                          max_depth=max_depth, learning_rate=learning_rate, n_estimators=n_estimators, num_leaves=num_leaves,\n                          boosting_type=\"gbdt\", subsample=subsample, reg_alpha=reg_alpha, reg_lambda=reg_lambda, device=\"gpu\")\n    \n    # Training :\n    model.fit(x_train_process, y_train, \n                  eval_set=[(x_val1_process, y_val1)])\n    \n    # score :\n    score = mean_absolute_error(y_val1, model.predict(x_val1_process))\n\n    return score\n\n\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=50)","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"LGBM_trial = study.best_trial \nprint('score : {}'.format(LGBM_trial.value)) \nprint(\"Best hyperparameters: {}\".format(LGBM_trial.params))","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Modelling with best hyperparameters : \n\nbest_lgbm_model = LGBMRegressor(random_state=42, n_jobs=-1, verbose=-1, objective=\"regression\",\n                                        early_stopping_rounds=40, #int [10;30]\n                                        eval_metric=\"mae\", #MAE, MSE, MAPE, R2\n                                        max_depth=(LGBM_trial.params)['max_depth'], \n                                        learning_rate=(LGBM_trial.params)['learning_rate'], \n                                        n_estimators=(LGBM_trial.params)['n_estimators'], num_leaves=(LGBM_trial.params)['num_leaves'], \n                                        boosting_type=\"gbdt\", subsample=(LGBM_trial.params)['subsample'], \n                                        reg_alpha=(LGBM_trial.params)['reg_alpha'], reg_lambda=(LGBM_trial.params)['reg_lambda'], device=\"gpu\")\n                                  \n# Training :\nbest_lgbm_model.fit(x_train_process, y_train, \n                  eval_set=[(x_val1_process, y_val1)])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#mean absolute error\nprint(\"MAE\")\nprint(mean_absolute_error(y_train, best_lgbm_model.predict(x_train_process))) #, multi_class='ovr'\nprint(mean_absolute_error(y_val1, best_lgbm_model.predict(x_val1_process))) #, multi_class='ovr'\nprint(\" \")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the model temporary :\njoblib.dump(value = best_lgbm_model, filename = './MODEL/Temp/train_val1_val2/best_lgbm_model.pkl')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# temp :\n\nbest_hyp_lgbm = {'max_depth': 31, 'learning_rate': 0.168, 'n_estimators': 7130, 'num_leaves': 156, 'subsample': 0.794051926923025, 'reg_alpha': 0.6892563729662605, 'reg_lambda': 0.3753105755368744}\n\n# Modelling with best hyperparameters : \n\nbest_lgbm_model = LGBMRegressor(random_state=42, n_jobs=-1, verbose=-1, objective=\"regression\",\n                                        early_stopping_rounds=40, #int [10;30]\n                                        eval_metric=\"mae\", #MAE, MSE, MAPE, R2\n                                        max_depth=(best_hyp_lgbm)['max_depth'], \n                                        learning_rate=(best_hyp_lgbm)['learning_rate'], \n                                        n_estimators=(best_hyp_lgbm)['n_estimators'], num_leaves=(best_hyp_lgbm)['num_leaves'], \n                                        boosting_type=\"gbdt\", subsample=(best_hyp_lgbm)['subsample'], \n                                        reg_alpha=(best_hyp_lgbm)['reg_alpha'], reg_lambda=(best_hyp_lgbm)['reg_lambda']) #,device=\"gpu\")\n                                  \n# Training :\nbest_lgbm_model.fit(x_train_process, y_train, \n                  eval_set=[(x_val1_process, y_val1)])\n\n# save model :\njoblib.dump(value = best_lgbm_model, filename = 'best_lgbm_model.pkl')","metadata":{"execution":{"iopub.status.busy":"2024-04-08T19:45:46.135260Z","iopub.execute_input":"2024-04-08T19:45:46.135609Z","iopub.status.idle":"2024-04-08T19:57:37.879271Z","shell.execute_reply.started":"2024-04-08T19:45:46.135580Z","shell.execute_reply":"2024-04-08T19:57:37.878306Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"['best_lgbm_model.pkl']"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Combinaison testing (train/val1/val2)","metadata":{}},{"cell_type":"markdown","source":"#### 0/ Import all model","metadata":{}},{"cell_type":"code","source":"# Load all model :\nbest_cat_model = joblib.load(filename = './MODEL/Temp/train_val1_val2/best_cat_model.pkl')\nbest_xgb_model = joblib.load(filename = './MODEL/Temp/train_val1_val2/best_xgb_model.pkl')\nbest_rf_model = joblib.load(filename = './MODEL/Temp/train_val1_val2/best_rf_model.pkl')\nbest_lgbm_model = joblib.load(filename = './MODEL/Temp/train_val1_val2/best_lgbm_model.pkl')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### A/ Comb 1 (xgb, Catb, lgbm)","metadata":{}},{"cell_type":"code","source":"# Modifiy comb model :","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Combinaison method 1 :\ny_test_pred_xgb = best_xgb_model.predict(x_val1_process)\n#y_test_pred_cat = best_cat_model.predict(x_val1_imp)\ny_test_pred_lgbm = best_lgbm_model.predict(x_val1_process)\n    \ndef model_predict_comb1(dict_coeff, xgb_pred, lgbm_pred) :  #, cat_pred):\n    y_test_pred = xgb_pred * dict_coeff[\"XGB\"] + lgbm_pred * dict_coeff[\"LGBM\"] #+ cat_pred * dict_coeff[\"CAT\"] \n    return y_test_pred\n","metadata":{"execution":{"iopub.status.busy":"2024-04-08T19:57:37.880764Z","iopub.execute_input":"2024-04-08T19:57:37.881580Z","iopub.status.idle":"2024-04-08T19:58:34.206807Z","shell.execute_reply.started":"2024-04-08T19:57:37.881545Z","shell.execute_reply":"2024-04-08T19:58:34.205992Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"[19:57:38] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\nPotential solutions:\n- Use a data structure that matches the device ordinal in the booster.\n- Set the device for booster before call to inplace_predict.\n\nThis warning will only be shown once.\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# # Naiv modelling :\n# dict_coeff = {\"XGB\" :0.7, \"CAT\":0.15, \"LGBM\":0.15}\n# y_val1_pred_comb = model_predict_comb1(dict_coeff=dict_coeff, xgb_pred=y_test_pred_xgb, cat_pred=y_test_pred_cat, lgbm_pred=y_test_pred_lgbm)\n# print(mean_absolute_error(y_val1, y_val1_pred_comb))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Bayesian Optimisation (optuna) :\n\ndef objective(trial):\n    xgb_coeff = trial.suggest_float('xgb_coeff', 0, 1, step=0.001)\n    cat_coeff = 0 #trial.suggest_float('cat_coeff', 0, 1, step=0.001)\n    lgbm_coeff = 1 - xgb_coeff #trial.suggest_float('lgbm_coeff', 0, 1, step=0.001)\n    \n    # Build dict coeff :\n    dict_coeff = {\"XGB\" : xgb_coeff, \"CAT\":cat_coeff, \"LGBM\":lgbm_coeff}\n\n    # Predict :\n    y_val1_pred_comb = model_predict_comb1(dict_coeff=dict_coeff, xgb_pred=y_test_pred_xgb, lgbm_pred=y_test_pred_lgbm) #, cat_pred=y_test_pred_cat\n    \n    # score :\n    score = mean_absolute_error(y_val1, y_val1_pred_comb)\n\n    return score\n\n\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=5000)","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"comb_trial = study.best_trial\nprint('score : {}'.format(comb_trial.value)) \nprint(\"Best hyperparameters: {}\".format(comb_trial.params))","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-04-08T19:58:34.856887Z","iopub.status.idle":"2024-04-08T19:58:34.857240Z","shell.execute_reply.started":"2024-04-08T19:58:34.857085Z","shell.execute_reply":"2024-04-08T19:58:34.857099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"raw","source":"score : 2.808\nBest hyperparameters: {'xgb_coeff': 0.613, 'cat_coeff': 0.114, 'lgbm_coeff': 0.273}","metadata":{}},{"cell_type":"code","source":"# Create the best dict_coeff :\ndict_coeff = {\"XGB\" :(comb_trial.params)[\"xgb_coeff\"], \"CAT\": (comb_trial.params)['cat_coeff'] , \"LGBM\": (comb_trial.params)['lgbm_coeff']}\ndict_coeff","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save dict and model combinaison :\n\n# dict : \nwith open(r'./MODEL/Temp/Combinaison_model/Comb1/dict_coeff.yaml', 'w') as file:\n    documents = yaml.dump(dict_coeff, file)\n\n# Model the model used for this comb model : \njoblib.dump(value = best_xgb_model, filename = './MODEL/Temp/Combinaison_model/Comb1/best_xgb_model.pkl')\njoblib.dump(value = best_cat_model, filename = './MODEL/Temp/Combinaison_model/Comb1/best_cat_model.pkl')\njoblib.dump(value = best_lgbm_model, filename = './MODEL/Temp/Combinaison_model/Comb1/best_lgbm_model.pkl')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"raw","source":"# If the best model is this combinaison model, then use this to read the dict_coeff file :\n# read dict_coeff : \nwith open(r'./MODEL/Temp/Combinaison_model/Comb1/dict_coeff.yaml') as file:\n    dict_coeff = yaml.load(file, Loader=yaml.FullLoader)","metadata":{"tags":[]}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6/ Choose, Train, Valid, Save the best model : ","metadata":{}},{"cell_type":"markdown","source":"## Choose the model","metadata":{"tags":[]}},{"cell_type":"raw","source":"XGBoost","metadata":{}},{"cell_type":"markdown","source":"## Treat x_train/x_val1/x_val2 for the best model","metadata":{}},{"cell_type":"code","source":"# Fillna :\n\n# x_train and x_val1 :\nx_train_imp, x_val1_imp = fillna_non_fix(x_train=x_train, x_test=x_val1, fillna_method=dict_imputation_non_fix, groupby_col=groupby_col)\n\n# x_val2 :\n_ , x_val2_imp = fillna_non_fix(x_train=x_train, x_test=x_val2, fillna_method=dict_imputation_non_fix, groupby_col=groupby_col)\n\n\n\n# Encoding/Scaling :\npre_process = pre_processing()\n\n# x_train :\nx_train_process = pre_process.pre_processing(df=x_train_imp, train=True, categorical_var_OHE=list_cat_col_OHE,\n                                     categorical_var_OrdinalEncoding={}, categorical_var_TE=list_cat_col_TE, \n                                     target=y_train, continious_var=[], encoding_type_cont=MinMaxScaler())\n\n# x_val1 :\nx_val1_process = pre_process.pre_processing(df=x_val1_imp, train=False, categorical_var_OHE=list_cat_col_OHE,\n                                 categorical_var_OrdinalEncoding={}, categorical_var_TE=list_cat_col_TE, \n                                 target=y_train, continious_var=[], encoding_type_cont=MinMaxScaler())\n\n# x_val2 :\nx_val2_process = pre_process.pre_processing(df=x_val2_imp, train=False, categorical_var_OHE=list_cat_col_OHE,\n                                 categorical_var_OrdinalEncoding={}, categorical_var_TE=list_cat_col_TE, \n                                 target=y_train, continious_var=[], encoding_type_cont=MinMaxScaler())","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#  Temp :\n\n# Fillna :\n\n# x_train and x_val1 :\n#x_train_imp, x_val1_imp = fillna_non_fix(x_train=x_train, x_test=x_val1, fillna_method=dict_imputation_non_fix, groupby_col=groupby_col)\n\n# x_val2 :\n_ , x_val2_imp = fillna_non_fix(x_train=x_train, x_test=x_val2, fillna_method=dict_imputation_non_fix, groupby_col=groupby_col)\n\n\n# Encoding/Scaling :\npre_process = pre_processing()\n\n# x_train :\nx_train_process = pre_process.pre_processing(df=x_train_imp, train=True, categorical_var_OHE=list_cat_col_OHE,\n                                     categorical_var_OrdinalEncoding={}, categorical_var_TE=[], \n                                     target=y_train, continious_var=[], encoding_type_cont=MinMaxScaler())\n\n# x_val1 :\nx_val1_process = pre_process.pre_processing(df=x_val1_imp, train=False, categorical_var_OHE=list_cat_col_OHE,\n                                 categorical_var_OrdinalEncoding={}, categorical_var_TE=[], \n                                 target=y_train, continious_var=[], encoding_type_cont=MinMaxScaler())\n\n# x_val2 :\nx_val2_process = pre_process.pre_processing(df=x_val2_imp, train=False, categorical_var_OHE=list_cat_col_OHE,\n                                 categorical_var_OrdinalEncoding={}, categorical_var_TE=[], \n                                 target=y_train, continious_var=[], encoding_type_cont=MinMaxScaler())\n\n\n\n# Count encoding : \nfrom category_encoders import CountEncoder\ncount_encoder = CountEncoder(cols=list_cat_col_TE)\nx_train_process = count_encoder.fit_transform(x_train_process)\nx_val1_process = count_encoder.transform(x_val1_process)\nx_val2_process = count_encoder.transform(x_val2_process)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load the best model","metadata":{}},{"cell_type":"code","source":"#load the best regression model :\nbest_model_reg = joblib.load(filename = './MODEL/Temp/train_val1_val2/best_xgb_model.pkl')","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Validation","metadata":{}},{"cell_type":"code","source":"# MAE\nprint(f\"MAE on train : {mean_absolute_error(y_train, best_model_reg.predict(x_train_process))}\")\nprint(f\"MAE on val1 : {mean_absolute_error(y_val1, best_model_reg.predict(x_val1_process))}\")\nprint(f\"MAE on val2 : {mean_absolute_error(y_val2, best_model_reg.predict(x_val2_process))}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"raw","source":"MAE on train : 2.5791766318914098\nMAE on val1 : 2.8117780057826676\nMAE on val2 : 2.8112577908373138","metadata":{}},{"cell_type":"code","source":"# temp xgboost :\n\n# MAE\nprint(f\"MAE on train : {mean_absolute_error(y_train, best_xgb_model.predict(x_train_process))}\")\nprint(f\"MAE on val1 : {mean_absolute_error(y_val1, best_xgb_model.predict(x_val1_process))}\")\nprint(f\"MAE on val2 : {mean_absolute_error(y_val2, best_xgb_model.predict(x_val2_process))}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# temp comb :\n\n#MAE :\n\n# Predict :\n# x_train :\n# y_test_pred_xgb = best_xgb_model.predict(x_train_process)\n# y_test_pred_cat = best_cat_model.predict(x_train_imp)\n# y_test_pred_lgbm = best_lgbm_model.predict(x_train_process)\n\n# # x_val1 :\n# y_test_pred_xgb = best_xgb_model.predict(x_val1_process)\n# y_test_pred_cat = best_cat_model.predict(x_val1_imp)\n# y_test_pred_lgbm = best_lgbm_model.predict(x_val1_process)\n\n# x_val2 :\ny_val2_pred_xgb = best_xgb_model.predict(x_val2_process)\ny_test_pred_cat = best_cat_model.predict(x_val2_imp)\ny_test_pred_lgbm = best_lgbm_model.predict(x_val2_process)\n\n\n# Combine prediction : \ndict_coeff = {\"XGB\" :(comb_trial.params)[\"xgb_coeff\"], \"CAT\": (comb_trial.params)['cat_coeff'] , \"LGBM\": (comb_trial.params)['lgbm_coeff']}\ny_val2_pred_comb = model_predict_comb1(dict_coeff=dict_coeff, xgb_pred=y_val2_pred_xgb, cat_pred=y_val2_pred_cat, lgbm_pred=y_val2_pred_lgbm)\n\n# Print score \nprint(mean_absolute_error(y_val2, y_val2_pred_comb))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Save model","metadata":{}},{"cell_type":"code","source":"# Save the best model :\njoblib.dump(value = best_model_reg, filename = './MODEL/best_model/best_model_reg.pkl')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#load model :\nbest_model_reg = joblib.load(filename = './MODEL/best_model/best_model_reg.pkl')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 7/ Analyse output : (useless)","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 8/ Choose best threshold (option) : (useless)","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 9/ Feature impact analysis (model interpretation) :","metadata":{}},{"cell_type":"code","source":"# Identification des variables les plus importantes :\ndef Features_importance(model) -> pd.DataFrame :\n    \"\"\"\n    Calculate and return feature importance scores as a DataFrame.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing feature names and their importance scores in percentage.\n    \"\"\"\n    df_features_importance = (pd.DataFrame({'Features': model.feature_names_in_,\n            'Features importance (in %)': (model.feature_importances_)*100}))\n    \n    return df_features_importance.sort_values(by='Features importance (in %)', ascending=False)\n\nFeatures_importance(model=best_model_reg).head(30)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# SHAP values\n\n#General :\n# compute the SHAP values for the linear model\nexplainer = shap.TreeExplainer(best_model_reg)\nshap_values = explainer.shap_values(x_val2_process)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shap.summary_plot(shap_values, x_val2_process)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 10/ Deployement :","metadata":{}},{"cell_type":"code","source":"# 1/ Open data\n\n# - read_csv :\nx_test = pd.read_csv(\"./data/test.csv\", encoding=\"utf-8\")\n\n# - Take the ID (option) :\nID = x_test[\"ID\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 2/ Data cleaning \n\n\n# - Basic treatment :\nuseless_columns = ['ID', 'Vf', 'De','Ernedc (g/km)','MMS','Mp','Mk','Man','Cn','Date of registration','r','Status']\nx_test = basic_treatment(df=x_test, useless_columns=useless_columns, drop_duplicate=False)\n\n\n# - Data filter : (useless)\n\n\n# - Data transformation : (useless)\n\n\n# - Check/change col type : (good)\n\n\n# - Handle abnormal values : \n# Verif incoherance : \"z (Wh/km)\" \n# Solution : Corriger les lignes incohérentes. Ajouter \"/electric\" à la col \"Ft\" si : z (Wh/km) != NaN, Fuel consumption != NaN, et Ft ne contient pas \"electric\"\nx_test.loc[(x_test['z (Wh/km)'].notna()) & (x_test['Fuel consumption '].notna() & ~(x_test[\"Ft\"].str.contains(\"electric\"))), \"Ft\"] += \"/electric\"\n\n\n# - Impute NaN (delete col, fillna_fix) :\n# A/ Delete columns which contains more than 50% of NaN or useless :\nCol_to_drop = [\"Enedc (g/km)\", \"Erwltp (g/km)\"]\nx_test = x_test.drop(Col_to_drop, axis=1)\n\n#B/ # Impute by fix value :\ndict_imputation_fix = {\"Country\" : \"unknown\", \"z (Wh/km)\": 0,\"Fuel consumption \": 0, \"Electric range (km)\": 0}\nx_test = fillna_fix_value(df=x_test, fillna_value=dict_imputation_fix)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 3/ Feature eng \n\n\n# - Feature creation : (useless)\n\n\n# - Feature selection (x_test = x_test[list(x_train.columns)]) :\nx_test = x_test[list(x_train.columns)]\n\n\n# - Row selection : (useless)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 4/ Prediction\n\n\n# - Impute NaN (fillna_nonfix) :\ngroupby_col = ['Ft']\ndict_imputation_non_fix = {\"VFN\": \"mode\",\"T\": \"mode\", \"Tan\": \"mode\", \"Va\": \"mode\",\"Ve\": \"mode\",\"Ct\": \"mode\",\"m (kg)\": \"median\",\"Mt\": \"median\",\"W (mm)\": \"median\",\"At1 (mm)\": \"median\",\"At2 (mm)\": \"median\",\"Fm\": \"mode\",\n                            \"ec (cm3)\": \"median\",\"ep (KW)\": \"median\", \"z (Wh/km)\" : \"median\",\"IT\": \"mode\",\"Fuel consumption \": \"median\", \"Electric range (km)\":\"median\", 'Cr':\"mode\", 'Mh':\"mode\"}\nx_train_imp, x_test_imp = fillna_non_fix(x_train=x_train, x_test=x_test, fillna_method=dict_imputation_non_fix, groupby_col=groupby_col)\n\n\n# - Encoding/Scaling :\npre_process = pre_processing()\nx_train_pro = pre_process.pre_processing(df=x_train_imp, train=True, categorical_var_OHE=list_cat_col_OHE,\n                                     categorical_var_OrdinalEncoding={}, categorical_var_TE=list_cat_col_TE, \n                                     target=y_train, continious_var=[], encoding_type_cont=MinMaxScaler())\n\nx_test_pro = pre_process.pre_processing(df=x_test_imp, train=False, categorical_var_OHE=list_cat_col_OHE,\n                                 categorical_var_OrdinalEncoding={}, categorical_var_TE=list_cat_col_TE, \n                                 target=y_train, continious_var=[], encoding_type_cont=MinMaxScaler())\n\n\n# - model.predict(x_test) :\ny_test_pred = best_model_reg.predict(x_test_pro)\n\n\n# - Save the prediction (submission.csv) :\nsubmission = pd.DataFrame({'ID': ID, 'Ewltp (g/km)': y_test_pred})\nsubmission.to_csv(r'submission.csv', index=False)\nsubmission","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}