{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dX7x2CBxFYxR",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import io\n",
        "# Compute the correlation matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "import warnings \n",
        "\n",
        "warnings.filterwarnings('ignore') "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "YDUwiN7w576B",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Lire les données à partir du fichier CSV\n",
        "data_train = pd.read_csv(r'train.csv')\n",
        "data_test = pd.read_csv(r'test.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpQEqz3uFYxX"
      },
      "source": [
        "## Traitements des données :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4nj8LCykFYxZ",
        "outputId": "f30eedb0-6295-4c54-eec7-6cfddea1809d"
      },
      "outputs": [],
      "source": [
        "def df_analyse(df, columns, name_df):\n",
        "    \"\"\"\n",
        "    Initial analysis on the DataFrame.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    Args:\n",
        "        df (pandas.DataFrame): DataFrame to analyze.\n",
        "        columns (list): Dataframe keys in list format.\n",
        "        name_df (str): DataFrame name.\n",
        "\n",
        "    Returns:\n",
        "        None.\n",
        "        Print the initial analysis on the DataFrame.\n",
        "    \"\"\"\n",
        "\n",
        "    # Calculating the memory usage based on dataframe.info()\n",
        "    buf = io.StringIO()\n",
        "    df.info(buf=buf)\n",
        "    memory_usage = buf.getvalue().split('\\n')[-2]\n",
        "\n",
        "    if df.empty:\n",
        "        print(\"The\", name_df, \"dataset is empty. Please verify the file.\")\n",
        "    else:\n",
        "        # identifying empty columns\n",
        "        empty_cols = [col for col in df.columns if df[col].isna().all()]\n",
        "        #identifying full duplicates rows\n",
        "        df_rows_duplicates = df[df.duplicated()]\n",
        "\n",
        "        # Creating a dataset based on Type object and records by columns\n",
        "        type_cols = df.dtypes.apply(lambda x: x.name).to_dict()\n",
        "        df_resume = pd.DataFrame(list(type_cols.items()), columns = [\"Name\", \"Type\"])\n",
        "        df_resume[\"Records\"] = list(df.count())\n",
        "        df_resume[\"% of NaN\"] = list(round((df.isnull().sum(axis = 0))/len(df),5)*100)\n",
        "        df_resume[\"Unique\"] = list(df.nunique())\n",
        "\n",
        "\n",
        "        print(\"\\nInitial Analysis of\", name_df, \"dataset\")\n",
        "        print(\"--------------------------------------------------------------------------\")\n",
        "        print(\"- Dataset shape:                 \", df.shape[0], \"rows and\", df.shape[1], \"columns\")\n",
        "        print(\"- Total of NaN values:           \", df.isna().sum().sum())\n",
        "        #print(\"- Percentage of NaN:             \", round((df.isna().sum().sum() / prod(df.shape)) * 100, 2), \"%\")\n",
        "        print(\"- Total of full duplicates rows: \", df_rows_duplicates.shape[0])\n",
        "        print(\"- Total of empty rows:           \", df.shape[0] - df.dropna(axis=\"rows\", how=\"all\").shape[0]) if df.dropna(axis=\"rows\", how=\"all\").shape[0] < df.shape[0] else \\\n",
        "                    print(\"- Total of empty rows:            0\")\n",
        "        print(\"- Total of empty columns:        \", len(empty_cols))\n",
        "        print(\"  + The empty column is:         \", empty_cols) if len(empty_cols) == 1 else \\\n",
        "                    print(\"  + The empty column are:         \", empty_cols) if len(empty_cols) >= 1 else None\n",
        "\n",
        "        print(\"\\n- The key(s):\", columns, \"is not present multiple times in the dataframe.\\n  It CAN be used as a primary key.\") if df.size == df.drop_duplicates(columns).size else \\\n",
        "                    print(\"\\n- The key(s):\", columns, \"is present multiple times in the dataframe.\\n  It CANNOT be used as a primary key.\")\n",
        "\n",
        "        print(\"\\n- Type object and records by columns         (\",memory_usage,\")\")\n",
        "        print(\"--------------------------------------------------------------------------\")\n",
        "        print(df_resume.sort_values(\"Records\", ascending=False))\n",
        "\n",
        "# Analyse df\n",
        "df_analyse(data_train, [\"Ewltp (g/km)\"], \"df\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CiX2v6AaFYxa",
        "outputId": "42b4d378-ba6e-4c69-8a34-471e4e721b4c",
        "tags": []
      },
      "outputs": [],
      "source": [
        "data_train_des = (data_train.describe()).T\n",
        "data_train_des"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Y-Qjh9kTFYxb"
      },
      "outputs": [],
      "source": [
        "def Drop_Columns(column_to_drop, data):\n",
        "    \"\"\"\n",
        "    Drop columns from the DataFrame.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    Args:\n",
        "        column_to_drop (list): List of columns to drop.\n",
        "        X_train (pandas.DataFrame): DataFrame to train.\n",
        "        X_val (pandas.DataFrame): DataFrame to validate.\n",
        "\n",
        "    Returns:\n",
        "        None.\n",
        "        Drop columns from the DataFrame.\n",
        "    \"\"\"\n",
        "\n",
        "    data.drop(column_to_drop, axis=1, inplace=True)\n",
        "\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "IyCyVX5NFYxc"
      },
      "outputs": [],
      "source": [
        "data_train[\"IT\"].fillna(\"Missing\", inplace=True)\n",
        "data_train[\"Mp\"].fillna(\"Missing\", inplace=True)\n",
        "data_train[\"Ct\"].fillna(\"Missing\", inplace=True)\n",
        "\n",
        "column_to_drop = [\"z (Wh/km)\" ,\"Electric range (km)\", \"Enedc (g/km)\" ,\"Vf\" ,\"De\" ,\"Ernedc (g/km)\", \"MMS\", \"Cr\", \"r\", \"Enedc (g/km)\", \"Fm\", \"Status\", \"Date of registration\", \"T\", \"Mk\", \"Va\", \"Tan\", \"Ve\", \"Cn\"]\n",
        "\n",
        "data_train = Drop_Columns(column_to_drop, data_train)\n",
        "col_to_keep = ['ec (cm3)' ,'ep (KW)', 'm (kg)','Mt', 'W (mm)', 'At1 (mm)', 'At2 (mm)',  'Erwltp (g/km)','Fuel consumption ',  'ID', 'Ewltp (g/km)']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qz2LUzyFYxd"
      },
      "source": [
        "### Modèlisation :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Rhtxulw4FYxe"
      },
      "outputs": [],
      "source": [
        "# Splitting data into features (X) and target variable (y)\n",
        "#col_to_keep_cat = ['Country', 'Mh', 'Man', 'VFN', 'Mp', 'IT', 'Ct', 'Ft']\n",
        "#col_to_keep.append( col_to_keep_cat)\n",
        "#flattened_list = []\n",
        "\n",
        "#for item in original_list:\n",
        " #   if isinstance(item, list):\n",
        "#        flattened_list.extend(item)\n",
        "#    else:\n",
        "#        flattened_list.append(item)\n",
        "#col_to_keep = flattened_list\n",
        "X = data_train[col_to_keep]\n",
        "X = X.drop(columns=['Ewltp (g/km)'])\n",
        "y = data_train['Ewltp (g/km)']\n",
        "# Split data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.35, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_XSaXUHFYxe"
      },
      "source": [
        "## Imputation des valeurs manquantes et normalisation :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6nbqzlWFYxe"
      },
      "source": [
        "### Encodage et Normalisation :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "aHhA5z0vFYxe"
      },
      "outputs": [],
      "source": [
        "def Impute_Missing_Value(column_to_impute, X, strategy='median'):\n",
        "    \"\"\"\n",
        "    Impute missing values in the DataFrame.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    Args:\n",
        "        column_to_impute (list): List of columns to impute.\n",
        "        X_train (pandas.DataFrame): DataFrame to train.\n",
        "        X_val (pandas.DataFrame): DataFrame to validate.\n",
        "\n",
        "    Returns:\n",
        "        None.\n",
        "        Impute missing values in the DataFrame.\n",
        "    \"\"\"\n",
        "\n",
        "    # Create a SimpleImputer with the strategy 'median' and apply it only to the specific column\n",
        "\n",
        "    imputer = SimpleImputer(strategy=strategy)\n",
        "    # Fit the imputer to your training data for that specific column\n",
        "    imputer.fit(X[column_to_impute])\n",
        "    # Transform the column for both the training and validation datasets\n",
        "    X[column_to_impute] = imputer.transform(X[column_to_impute])\n",
        "\n",
        "    return X\n",
        "\n",
        "def Label_Encoder(column_to_encode, X):\n",
        "    \"\"\"\n",
        "    Encode labels with value between 0 and n_classes-1.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    Args:\n",
        "        column_to_encode (list): List of columns to encode.\n",
        "        X_train (pandas.DataFrame): DataFrame to train.\n",
        "        X_val (pandas.DataFrame): DataFrame to validate.\n",
        "\n",
        "    Returns:\n",
        "        None.\n",
        "        Encode labels with value between 0 and n_classes-1.\n",
        "    \"\"\"\n",
        "\n",
        "    # Create a LabelEncoder object and apply it to each column of the DataFrame\n",
        "    label_encoder = LabelEncoder()\n",
        "    for col in column_to_encode:\n",
        "        X[col] = label_encoder.fit_transform(X[col])\n",
        "\n",
        "    return X\n",
        "\n",
        "def Standard_Scaler(column_to_scale, X):\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X[column_to_scale] = scaler.fit_transform(X[column_to_scale])\n",
        "\n",
        "    return X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "-aRtAYCmFYxf"
      },
      "outputs": [],
      "source": [
        "X_train_ID = X_train['ID']\n",
        "X_val_ID = X_val['ID']\n",
        "\n",
        "del X_train['ID']\n",
        "del X_val['ID']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "rfdjcSUMFYxf"
      },
      "outputs": [],
      "source": [
        "column_to_impute_scale = ['ec (cm3)',\n",
        " 'ep (KW)',\n",
        " 'm (kg)',\n",
        " 'Mt',\n",
        " 'W (mm)',\n",
        " 'At1 (mm)',\n",
        " 'At2 (mm)',\n",
        " 'Erwltp (g/km)',\n",
        " 'Fuel consumption ']\n",
        "#column_to_encode = ['Country', 'Mh', 'Man', 'VFN', 'Mp', 'IT', 'Ct', 'Ft']\n",
        "#column_to_impute_scale =['m (kg)', 'W (mm)', 'ep (KW)', 'At1 (mm)', 'At2 (mm)', 'Mt', 'ec (cm3)', 'Fuel consumption ', 'Erwltp (g/km)']\n",
        "#column_to_encode =['Country', 'Mh', 'Man', 'VFN', 'Mp', 'IT', 'Ct', 'Ft']\n",
        "\n",
        "\n",
        "X_train = Impute_Missing_Value(column_to_impute_scale, X_train, strategy='median')\n",
        "X_val = Impute_Missing_Value(column_to_impute_scale, X_val, strategy='median')\n",
        "#X_train = Label_Encoder(column_to_encode, X_train)\n",
        "#X_val = Label_Encoder(column_to_encode, X_val)\n",
        "X_train = Standard_Scaler(column_to_impute_scale, X_train)\n",
        "X_val = Standard_Scaler(column_to_impute_scale, X_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "corr = X_train.select_dtypes(exclude=['object']).corr()\n",
        "# Generate a mask for the upper triangle\n",
        "mask = np.zeros_like(corr, dtype=bool)\n",
        "mask[np.triu_indices_from(mask)] = True\n",
        "f, ax = plt.subplots(figsize=(5.5, 4.5))\n",
        "cmap = sns.color_palette(\"RdBu_r\", 11)\n",
        "# Draw the heatmap with the mask and correct aspect ratio\n",
        "_ = sns.heatmap(corr, mask=None, cmap=cmap, vmax=1, center=0,\n",
        "square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
        "\n",
        "# convert correlation to distances\n",
        "d = 2 * (1 - np.abs(corr))\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "clustering = AgglomerativeClustering(n_clusters=6, linkage='single', affinity=\"precomputed\").fit(d)\n",
        "lab=0\n",
        "clusters = [list(corr.columns[clustering.labels_==lab]) for lab in set(clustering.labels_)]\n",
        "print(clusters)\n",
        "reordered = np.concatenate(clusters)\n",
        "\n",
        "R = corr.loc[reordered, reordered]\n",
        "f, ax = plt.subplots(figsize=(5.5, 4.5))\n",
        "# Draw the heatmap with the mask and correct aspect ratio\n",
        "_ = sns.heatmap(R, mask=None, cmap=cmap, vmax=1, center=0,\n",
        "square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcm15VAxFYxf"
      },
      "source": [
        "### Régression linéaire :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9tBNwrXcFYxf",
        "outputId": "513bdda9-9264-4b86-e7ab-ea45858049de"
      },
      "outputs": [],
      "source": [
        "# Create and train a Linear Regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the validation set\n",
        "y_pred = model.predict(X_val)\n",
        "\n",
        "# Evaluate the model\n",
        "mae = mean_absolute_error(y_val, y_pred)\n",
        "print(f'Mean Absolute Error on Validation Set: {mae}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fSL4qtuBFYxg",
        "outputId": "20afa203-d80e-4af0-f8d2-6be9e6e5a13f"
      },
      "outputs": [],
      "source": [
        "# Create and train a Linear Regression model\n",
        "model = Ridge()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the validation set\n",
        "y_pred = model.predict(X_val)\n",
        "\n",
        "# Evaluate the model\n",
        "mae = mean_absolute_error(y_val, y_pred)\n",
        "print(f'Mean Absolute Error on Validation Set: {mae}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJBDMf3gFYxg"
      },
      "source": [
        "### Decision Tree :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9vsd-zAFYxg",
        "outputId": "4a7dc7fc-c0b2-4591-a4f6-118cd68069f2"
      },
      "outputs": [],
      "source": [
        "# Create and train a Linear Regression model\n",
        "model = DecisionTreeRegressor(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the validation set\n",
        "y_pred = model.predict(X_val)\n",
        "\n",
        "# Evaluate the model\n",
        "mae = mean_absolute_error(y_val, y_pred)\n",
        "print(f'Mean Absolute Error on Validation Set: {mae}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jRZlnnKKSGQl"
      },
      "outputs": [],
      "source": [
        "# Create and train a Linear Regression model\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "model = RandomForestRegressor()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the validation set\n",
        "y_pred = model.predict(X_val)\n",
        "\n",
        "# Evaluate the model\n",
        "mae = mean_absolute_error(y_val, y_pred)\n",
        "print(f'Mean Absolute Error on Validation Set: {mae}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2023-10-27 09:45:31,789] A new study created in memory with name: no-name-1aa17c4b-5dac-4b81-b362-8ca288e217e4\n",
            "[I 2023-10-27 09:46:39,268] Trial 0 finished with value: 138.4416988053937 and parameters: {'n_estimators': 235, 'max_depth': 5, 'min_samples_split': 0.7765753519976455, 'min_samples_leaf': 0.37145362227371337, 'max_features': 'sqrt'}. Best is trial 0 with value: 138.4416988053937.\n",
            "[I 2023-10-27 09:47:15,629] Trial 1 finished with value: 138.4492597887468 and parameters: {'n_estimators': 128, 'max_depth': 19, 'min_samples_split': 0.9025875230888355, 'min_samples_leaf': 0.10233559511815185, 'max_features': 'auto'}. Best is trial 0 with value: 138.4416988053937.\n",
            "[I 2023-10-27 09:48:13,789] Trial 2 finished with value: 138.4468655040149 and parameters: {'n_estimators': 207, 'max_depth': 17, 'min_samples_split': 0.28692170320074073, 'min_samples_leaf': 0.4950949499395654, 'max_features': 'auto'}. Best is trial 0 with value: 138.4416988053937.\n",
            "[I 2023-10-27 09:49:01,275] Trial 3 finished with value: 138.44336815047805 and parameters: {'n_estimators': 168, 'max_depth': 17, 'min_samples_split': 0.9176686286204836, 'min_samples_leaf': 0.24202032447557442, 'max_features': 'sqrt'}. Best is trial 0 with value: 138.4416988053937.\n",
            "[I 2023-10-27 09:52:32,365] Trial 4 finished with value: 126.34302180956213 and parameters: {'n_estimators': 98, 'max_depth': 11, 'min_samples_split': 0.4641820638067684, 'min_samples_leaf': 0.24275562169589648, 'max_features': 'auto'}. Best is trial 4 with value: 126.34302180956213.\n",
            "[I 2023-10-27 09:59:40,794] Trial 5 finished with value: 131.41410250451787 and parameters: {'n_estimators': 211, 'max_depth': 2, 'min_samples_split': 0.14981525003320495, 'min_samples_leaf': 0.29280657510101854, 'max_features': 'auto'}. Best is trial 4 with value: 126.34302180956213.\n",
            "[I 2023-10-27 10:00:16,765] Trial 6 finished with value: 138.446590636816 and parameters: {'n_estimators': 126, 'max_depth': 10, 'min_samples_split': 0.8088752073061477, 'min_samples_leaf': 0.43768490250994363, 'max_features': 'sqrt'}. Best is trial 4 with value: 126.34302180956213.\n",
            "[I 2023-10-27 10:03:29,405] Trial 7 finished with value: 127.65007038755861 and parameters: {'n_estimators': 78, 'max_depth': 17, 'min_samples_split': 0.5559932284302883, 'min_samples_leaf': 0.12381197427582258, 'max_features': 'auto'}. Best is trial 4 with value: 126.34302180956213.\n",
            "[W 2023-10-27 10:04:36,506] Trial 8 failed with parameters: {'n_estimators': 87, 'max_depth': 2, 'min_samples_split': 0.16996655142115813, 'min_samples_leaf': 0.16074286526284798, 'max_features': 'auto'} because of the following error: KeyboardInterrupt().\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\NEIL\\anaconda3\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 200, in _run_trial\n",
            "    value_or_values = func(trial)\n",
            "  File \"C:\\Users\\NEIL\\AppData\\Local\\Temp\\ipykernel_15484\\4106586392.py\", line 22, in objective\n",
            "    mae = -cross_val_score(model, X_train, y_train, cv=3, scoring='neg_mean_absolute_error').mean()\n",
            "  File \"c:\\Users\\NEIL\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 515, in cross_val_score\n",
            "    cv_results = cross_validate(\n",
            "  File \"c:\\Users\\NEIL\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 266, in cross_validate\n",
            "    results = parallel(\n",
            "  File \"c:\\Users\\NEIL\\anaconda3\\lib\\site-packages\\sklearn\\utils\\parallel.py\", line 63, in __call__\n",
            "    return super().__call__(iterable_with_config)\n",
            "  File \"c:\\Users\\NEIL\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 1085, in __call__\n",
            "    if self.dispatch_one_batch(iterator):\n",
            "  File \"c:\\Users\\NEIL\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 901, in dispatch_one_batch\n",
            "    self._dispatch(tasks)\n",
            "  File \"c:\\Users\\NEIL\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 819, in _dispatch\n",
            "    job = self._backend.apply_async(batch, callback=cb)\n",
            "  File \"c:\\Users\\NEIL\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
            "    result = ImmediateResult(func)\n",
            "  File \"c:\\Users\\NEIL\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 597, in __init__\n",
            "    self.results = batch()\n",
            "  File \"c:\\Users\\NEIL\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 288, in __call__\n",
            "    return [func(*args, **kwargs)\n",
            "  File \"c:\\Users\\NEIL\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 288, in <listcomp>\n",
            "    return [func(*args, **kwargs)\n",
            "  File \"c:\\Users\\NEIL\\anaconda3\\lib\\site-packages\\sklearn\\utils\\parallel.py\", line 123, in __call__\n",
            "    return self.function(*args, **kwargs)\n",
            "  File \"c:\\Users\\NEIL\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"c:\\Users\\NEIL\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 473, in fit\n",
            "    trees = Parallel(\n",
            "  File \"c:\\Users\\NEIL\\anaconda3\\lib\\site-packages\\sklearn\\utils\\parallel.py\", line 63, in __call__\n",
            "    return super().__call__(iterable_with_config)\n",
            "  File \"c:\\Users\\NEIL\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 1088, in __call__\n",
            "    while self.dispatch_one_batch(iterator):\n",
            "  File \"c:\\Users\\NEIL\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 901, in dispatch_one_batch\n",
            "    self._dispatch(tasks)\n",
            "  File \"c:\\Users\\NEIL\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 819, in _dispatch\n",
            "    job = self._backend.apply_async(batch, callback=cb)\n",
            "  File \"c:\\Users\\NEIL\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
            "    result = ImmediateResult(func)\n",
            "  File \"c:\\Users\\NEIL\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 597, in __init__\n",
            "    self.results = batch()\n",
            "  File \"c:\\Users\\NEIL\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 288, in __call__\n",
            "    return [func(*args, **kwargs)\n",
            "  File \"c:\\Users\\NEIL\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 288, in <listcomp>\n",
            "    return [func(*args, **kwargs)\n",
            "  File \"c:\\Users\\NEIL\\anaconda3\\lib\\site-packages\\sklearn\\utils\\parallel.py\", line 123, in __call__\n",
            "    return self.function(*args, **kwargs)\n",
            "  File \"c:\\Users\\NEIL\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 174, in _parallel_build_trees\n",
            "    sample_counts = np.bincount(indices, minlength=n_samples)\n",
            "KeyboardInterrupt\n",
            "[W 2023-10-27 10:04:36,507] Trial 8 failed with value None.\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\NEIL\\Documents\\M2 MoSEF DATA SCIENCE\\S1\\Data mining\\Projet_Kaggle_Score_3,60.ipynb Cell 23\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/NEIL/Documents/M2%20MoSEF%20DATA%20SCIENCE/S1/Data%20mining/Projet_Kaggle_Score_3%2C60.ipynb#X31sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m mae\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/NEIL/Documents/M2%20MoSEF%20DATA%20SCIENCE/S1/Data%20mining/Projet_Kaggle_Score_3%2C60.ipynb#X31sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m study \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(direction\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mminimize\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/NEIL/Documents/M2%20MoSEF%20DATA%20SCIENCE/S1/Data%20mining/Projet_Kaggle_Score_3%2C60.ipynb#X31sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m study\u001b[39m.\u001b[39;49moptimize(objective, n_trials\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/NEIL/Documents/M2%20MoSEF%20DATA%20SCIENCE/S1/Data%20mining/Projet_Kaggle_Score_3%2C60.ipynb#X31sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m best_params \u001b[39m=\u001b[39m study\u001b[39m.\u001b[39mbest_params\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/NEIL/Documents/M2%20MoSEF%20DATA%20SCIENCE/S1/Data%20mining/Projet_Kaggle_Score_3%2C60.ipynb#X31sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mBest Hyperparameters:\u001b[39m\u001b[39m\"\u001b[39m, best_params)\n",
            "File \u001b[1;32mc:\\Users\\NEIL\\anaconda3\\lib\\site-packages\\optuna\\study\\study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimize\u001b[39m(\n\u001b[0;32m    349\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    357\u001b[0m     show_progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    358\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    359\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    360\u001b[0m \n\u001b[0;32m    361\u001b[0m \u001b[39m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[39m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    450\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 451\u001b[0m     _optimize(\n\u001b[0;32m    452\u001b[0m         study\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m    453\u001b[0m         func\u001b[39m=\u001b[39;49mfunc,\n\u001b[0;32m    454\u001b[0m         n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[0;32m    455\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    456\u001b[0m         n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[0;32m    457\u001b[0m         catch\u001b[39m=\u001b[39;49m\u001b[39mtuple\u001b[39;49m(catch) \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(catch, Iterable) \u001b[39melse\u001b[39;49;00m (catch,),\n\u001b[0;32m    458\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[0;32m    459\u001b[0m         gc_after_trial\u001b[39m=\u001b[39;49mgc_after_trial,\n\u001b[0;32m    460\u001b[0m         show_progress_bar\u001b[39m=\u001b[39;49mshow_progress_bar,\n\u001b[0;32m    461\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\NEIL\\anaconda3\\lib\\site-packages\\optuna\\study\\_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     65\u001b[0m     \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m---> 66\u001b[0m         _optimize_sequential(\n\u001b[0;32m     67\u001b[0m             study,\n\u001b[0;32m     68\u001b[0m             func,\n\u001b[0;32m     69\u001b[0m             n_trials,\n\u001b[0;32m     70\u001b[0m             timeout,\n\u001b[0;32m     71\u001b[0m             catch,\n\u001b[0;32m     72\u001b[0m             callbacks,\n\u001b[0;32m     73\u001b[0m             gc_after_trial,\n\u001b[0;32m     74\u001b[0m             reseed_sampler_rng\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m     75\u001b[0m             time_start\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m     76\u001b[0m             progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[0;32m     77\u001b[0m         )\n\u001b[0;32m     78\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     79\u001b[0m         \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
            "File \u001b[1;32mc:\\Users\\NEIL\\anaconda3\\lib\\site-packages\\optuna\\study\\_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 163\u001b[0m     frozen_trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[0;32m    164\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    165\u001b[0m     \u001b[39m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[39m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     \u001b[39m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    168\u001b[0m     \u001b[39m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    169\u001b[0m     \u001b[39mif\u001b[39;00m gc_after_trial:\n",
            "File \u001b[1;32mc:\\Users\\NEIL\\anaconda3\\lib\\site-packages\\optuna\\study\\_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    244\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mShould not reach.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    246\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    247\u001b[0m     frozen_trial\u001b[39m.\u001b[39mstate \u001b[39m==\u001b[39m TrialState\u001b[39m.\u001b[39mFAIL\n\u001b[0;32m    248\u001b[0m     \u001b[39mand\u001b[39;00m func_err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    249\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    250\u001b[0m ):\n\u001b[1;32m--> 251\u001b[0m     \u001b[39mraise\u001b[39;00m func_err\n\u001b[0;32m    252\u001b[0m \u001b[39mreturn\u001b[39;00m frozen_trial\n",
            "File \u001b[1;32mc:\\Users\\NEIL\\anaconda3\\lib\\site-packages\\optuna\\study\\_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[39mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[39m.\u001b[39m_trial_id, study\u001b[39m.\u001b[39m_storage):\n\u001b[0;32m    199\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 200\u001b[0m         value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[0;32m    201\u001b[0m     \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    202\u001b[0m         \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    203\u001b[0m         state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n",
            "\u001b[1;32mc:\\Users\\NEIL\\Documents\\M2 MoSEF DATA SCIENCE\\S1\\Data mining\\Projet_Kaggle_Score_3,60.ipynb Cell 23\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/NEIL/Documents/M2%20MoSEF%20DATA%20SCIENCE/S1/Data%20mining/Projet_Kaggle_Score_3%2C60.ipynb#X31sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m model \u001b[39m=\u001b[39m RandomForestRegressor(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/NEIL/Documents/M2%20MoSEF%20DATA%20SCIENCE/S1/Data%20mining/Projet_Kaggle_Score_3%2C60.ipynb#X31sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     n_estimators\u001b[39m=\u001b[39mn_estimators,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/NEIL/Documents/M2%20MoSEF%20DATA%20SCIENCE/S1/Data%20mining/Projet_Kaggle_Score_3%2C60.ipynb#X31sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     max_depth\u001b[39m=\u001b[39mmax_depth,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/NEIL/Documents/M2%20MoSEF%20DATA%20SCIENCE/S1/Data%20mining/Projet_Kaggle_Score_3%2C60.ipynb#X31sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     max_features\u001b[39m=\u001b[39mmax_features\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/NEIL/Documents/M2%20MoSEF%20DATA%20SCIENCE/S1/Data%20mining/Projet_Kaggle_Score_3%2C60.ipynb#X31sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/NEIL/Documents/M2%20MoSEF%20DATA%20SCIENCE/S1/Data%20mining/Projet_Kaggle_Score_3%2C60.ipynb#X31sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m# Use cross_val_score to calculate MAE (mean absolute error)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/NEIL/Documents/M2%20MoSEF%20DATA%20SCIENCE/S1/Data%20mining/Projet_Kaggle_Score_3%2C60.ipynb#X31sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m mae \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mcross_val_score(model, X_train, y_train, cv\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m, scoring\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mneg_mean_absolute_error\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39mmean()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/NEIL/Documents/M2%20MoSEF%20DATA%20SCIENCE/S1/Data%20mining/Projet_Kaggle_Score_3%2C60.ipynb#X31sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39mreturn\u001b[39;00m mae\n",
            "File \u001b[1;32mc:\\Users\\NEIL\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515\u001b[0m, in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[39m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[0;32m    513\u001b[0m scorer \u001b[39m=\u001b[39m check_scoring(estimator, scoring\u001b[39m=\u001b[39mscoring)\n\u001b[1;32m--> 515\u001b[0m cv_results \u001b[39m=\u001b[39m cross_validate(\n\u001b[0;32m    516\u001b[0m     estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[0;32m    517\u001b[0m     X\u001b[39m=\u001b[39;49mX,\n\u001b[0;32m    518\u001b[0m     y\u001b[39m=\u001b[39;49my,\n\u001b[0;32m    519\u001b[0m     groups\u001b[39m=\u001b[39;49mgroups,\n\u001b[0;32m    520\u001b[0m     scoring\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mscore\u001b[39;49m\u001b[39m\"\u001b[39;49m: scorer},\n\u001b[0;32m    521\u001b[0m     cv\u001b[39m=\u001b[39;49mcv,\n\u001b[0;32m    522\u001b[0m     n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[0;32m    523\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m    524\u001b[0m     fit_params\u001b[39m=\u001b[39;49mfit_params,\n\u001b[0;32m    525\u001b[0m     pre_dispatch\u001b[39m=\u001b[39;49mpre_dispatch,\n\u001b[0;32m    526\u001b[0m     error_score\u001b[39m=\u001b[39;49merror_score,\n\u001b[0;32m    527\u001b[0m )\n\u001b[0;32m    528\u001b[0m \u001b[39mreturn\u001b[39;00m cv_results[\u001b[39m\"\u001b[39m\u001b[39mtest_score\u001b[39m\u001b[39m\"\u001b[39m]\n",
            "File \u001b[1;32mc:\\Users\\NEIL\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:266\u001b[0m, in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[39m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[39m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[0;32m    265\u001b[0m parallel \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39mn_jobs, verbose\u001b[39m=\u001b[39mverbose, pre_dispatch\u001b[39m=\u001b[39mpre_dispatch)\n\u001b[1;32m--> 266\u001b[0m results \u001b[39m=\u001b[39m parallel(\n\u001b[0;32m    267\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    268\u001b[0m         clone(estimator),\n\u001b[0;32m    269\u001b[0m         X,\n\u001b[0;32m    270\u001b[0m         y,\n\u001b[0;32m    271\u001b[0m         scorers,\n\u001b[0;32m    272\u001b[0m         train,\n\u001b[0;32m    273\u001b[0m         test,\n\u001b[0;32m    274\u001b[0m         verbose,\n\u001b[0;32m    275\u001b[0m         \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    276\u001b[0m         fit_params,\n\u001b[0;32m    277\u001b[0m         return_train_score\u001b[39m=\u001b[39;49mreturn_train_score,\n\u001b[0;32m    278\u001b[0m         return_times\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    279\u001b[0m         return_estimator\u001b[39m=\u001b[39;49mreturn_estimator,\n\u001b[0;32m    280\u001b[0m         error_score\u001b[39m=\u001b[39;49merror_score,\n\u001b[0;32m    281\u001b[0m     )\n\u001b[0;32m    282\u001b[0m     \u001b[39mfor\u001b[39;49;00m train, test \u001b[39min\u001b[39;49;00m cv\u001b[39m.\u001b[39;49msplit(X, y, groups)\n\u001b[0;32m    283\u001b[0m )\n\u001b[0;32m    285\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[0;32m    287\u001b[0m \u001b[39m# For callabe scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[39m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[0;32m    289\u001b[0m \u001b[39m# the correct key.\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\NEIL\\anaconda3\\lib\\site-packages\\sklearn\\utils\\parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     58\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[0;32m     59\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[0;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     61\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[0;32m     62\u001b[0m )\n\u001b[1;32m---> 63\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
            "File \u001b[1;32mc:\\Users\\NEIL\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:1085\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1076\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1077\u001b[0m     \u001b[39m# Only set self._iterating to True if at least a batch\u001b[39;00m\n\u001b[0;32m   1078\u001b[0m     \u001b[39m# was dispatched. In particular this covers the edge\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1082\u001b[0m     \u001b[39m# was very quick and its callback already dispatched all the\u001b[39;00m\n\u001b[0;32m   1083\u001b[0m     \u001b[39m# remaining jobs.\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m-> 1085\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[0;32m   1086\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1088\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n",
            "File \u001b[1;32mc:\\Users\\NEIL\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    899\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 901\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[0;32m    902\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\NEIL\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    818\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[1;32m--> 819\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[0;32m    820\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    821\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    823\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    824\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
            "File \u001b[1;32mc:\\Users\\NEIL\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[0;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
            "File \u001b[1;32mc:\\Users\\NEIL\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[0;32m    595\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    596\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
            "File \u001b[1;32mc:\\Users\\NEIL\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
            "File \u001b[1;32mc:\\Users\\NEIL\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
            "File \u001b[1;32mc:\\Users\\NEIL\\anaconda3\\lib\\site-packages\\sklearn\\utils\\parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    121\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[0;32m    122\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[1;32m--> 123\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\NEIL\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:686\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    684\u001b[0m         estimator\u001b[39m.\u001b[39mfit(X_train, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m    685\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 686\u001b[0m         estimator\u001b[39m.\u001b[39mfit(X_train, y_train, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m    688\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m    689\u001b[0m     \u001b[39m# Note fit time as time until error\u001b[39;00m\n\u001b[0;32m    690\u001b[0m     fit_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n",
            "File \u001b[1;32mc:\\Users\\NEIL\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:473\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    462\u001b[0m trees \u001b[39m=\u001b[39m [\n\u001b[0;32m    463\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_estimator(append\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, random_state\u001b[39m=\u001b[39mrandom_state)\n\u001b[0;32m    464\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    465\u001b[0m ]\n\u001b[0;32m    467\u001b[0m \u001b[39m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    468\u001b[0m \u001b[39m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    469\u001b[0m \u001b[39m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    470\u001b[0m \u001b[39m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    471\u001b[0m \u001b[39m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    472\u001b[0m \u001b[39m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 473\u001b[0m trees \u001b[39m=\u001b[39m Parallel(\n\u001b[0;32m    474\u001b[0m     n_jobs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_jobs,\n\u001b[0;32m    475\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[0;32m    476\u001b[0m     prefer\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mthreads\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    477\u001b[0m )(\n\u001b[0;32m    478\u001b[0m     delayed(_parallel_build_trees)(\n\u001b[0;32m    479\u001b[0m         t,\n\u001b[0;32m    480\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbootstrap,\n\u001b[0;32m    481\u001b[0m         X,\n\u001b[0;32m    482\u001b[0m         y,\n\u001b[0;32m    483\u001b[0m         sample_weight,\n\u001b[0;32m    484\u001b[0m         i,\n\u001b[0;32m    485\u001b[0m         \u001b[39mlen\u001b[39;49m(trees),\n\u001b[0;32m    486\u001b[0m         verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[0;32m    487\u001b[0m         class_weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclass_weight,\n\u001b[0;32m    488\u001b[0m         n_samples_bootstrap\u001b[39m=\u001b[39;49mn_samples_bootstrap,\n\u001b[0;32m    489\u001b[0m     )\n\u001b[0;32m    490\u001b[0m     \u001b[39mfor\u001b[39;49;00m i, t \u001b[39min\u001b[39;49;00m \u001b[39menumerate\u001b[39;49m(trees)\n\u001b[0;32m    491\u001b[0m )\n\u001b[0;32m    493\u001b[0m \u001b[39m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    494\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimators_\u001b[39m.\u001b[39mextend(trees)\n",
            "File \u001b[1;32mc:\\Users\\NEIL\\anaconda3\\lib\\site-packages\\sklearn\\utils\\parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     58\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[0;32m     59\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[0;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     61\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[0;32m     62\u001b[0m )\n\u001b[1;32m---> 63\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
            "File \u001b[1;32mc:\\Users\\NEIL\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1085\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1086\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1088\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[0;32m   1089\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[0;32m   1091\u001b[0m \u001b[39mif\u001b[39;00m pre_dispatch \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   1092\u001b[0m     \u001b[39m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[0;32m   1093\u001b[0m     \u001b[39m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[0;32m   1094\u001b[0m     \u001b[39m# consumption.\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\NEIL\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    899\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 901\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[0;32m    902\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\NEIL\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    818\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[1;32m--> 819\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[0;32m    820\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    821\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    823\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    824\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
            "File \u001b[1;32mc:\\Users\\NEIL\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[0;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
            "File \u001b[1;32mc:\\Users\\NEIL\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[0;32m    595\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    596\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
            "File \u001b[1;32mc:\\Users\\NEIL\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
            "File \u001b[1;32mc:\\Users\\NEIL\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
            "File \u001b[1;32mc:\\Users\\NEIL\\anaconda3\\lib\\site-packages\\sklearn\\utils\\parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    121\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[0;32m    122\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[1;32m--> 123\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\NEIL\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:174\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[0;32m    169\u001b[0m     curr_sample_weight \u001b[39m=\u001b[39m sample_weight\u001b[39m.\u001b[39mcopy()\n\u001b[0;32m    171\u001b[0m indices \u001b[39m=\u001b[39m _generate_sample_indices(\n\u001b[0;32m    172\u001b[0m     tree\u001b[39m.\u001b[39mrandom_state, n_samples, n_samples_bootstrap\n\u001b[0;32m    173\u001b[0m )\n\u001b[1;32m--> 174\u001b[0m sample_counts \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mbincount(indices, minlength\u001b[39m=\u001b[39;49mn_samples)\n\u001b[0;32m    175\u001b[0m curr_sample_weight \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m sample_counts\n\u001b[0;32m    177\u001b[0m \u001b[39mif\u001b[39;00m class_weight \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msubsample\u001b[39m\u001b[39m\"\u001b[39m:\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import optuna\n",
        "from optuna import Trial\n",
        "import optuna.visualization as ov\n",
        "\n",
        "\n",
        "def objective(trial: optuna.Trial):\n",
        "    n_estimators = trial.suggest_int('n_estimators', 50, 250)\n",
        "    max_depth = trial.suggest_int('max_depth', 1, 20)\n",
        "    min_samples_split = trial.suggest_float('min_samples_split', 0.0, 1.0)\n",
        "    min_samples_leaf = trial.suggest_float('min_samples_leaf', 0.1, 0.5)\n",
        "    max_features = trial.suggest_categorical('max_features', ['auto', 'sqrt'])\n",
        "\n",
        "    model = RandomForestRegressor(\n",
        "        n_estimators=n_estimators,\n",
        "        max_depth=max_depth,\n",
        "        min_samples_split=min_samples_split,\n",
        "        min_samples_leaf=min_samples_leaf,\n",
        "        max_features=max_features\n",
        "    )\n",
        "\n",
        "    # Use cross_val_score to calculate MAE (mean absolute error)\n",
        "    mae = -cross_val_score(model, X_train, y_train, cv=3, scoring='neg_mean_absolute_error').mean()\n",
        "    return mae\n",
        "\n",
        "study = optuna.create_study(direction=\"minimize\")\n",
        "study.optimize(objective, n_trials=50)\n",
        "best_params = study.best_params\n",
        "print(\"Best Hyperparameters:\", best_params)\n",
        "\n",
        "# Create a RandomForestRegressor with the best hyperparameters\n",
        "best_model = RandomForestRegressor(\n",
        "    n_estimators=best_params['n_estimators'],\n",
        "    max_depth=best_params['max_depth'],\n",
        "    min_samples_split=best_params['min_samples_split'],\n",
        "    min_samples_leaf=best_params['min_samples_leaf'],\n",
        "    max_features=best_params['max_features'],\n",
        "    criterion=best_params['criterion']\n",
        ")\n",
        "\n",
        "# Train the best model on the training data\n",
        "best_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the validation dataset\n",
        "y_pred = best_model.predict(X_val)\n",
        "\n",
        "# Calculate the MAE on the validation dataset\n",
        "mae = mean_absolute_error(y_val, y_pred)\n",
        "print(\"Mean Absolute Error on Validation Data:\", mae)\n",
        "\n",
        "\n",
        "# Plot the best hyperparameters\n",
        "optuna.visualization.plot_param_importances(study)\n",
        "optuna.visualization.plot_optimization_history(study)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from bayes_opt import BayesianOptimization\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Define the objective function to optimize (in this case, cross-validated MAE)\n",
        "def objective_function(max_depth, min_samples_split, min_samples_leaf):\n",
        "    # Convert hyperparameters to integers when necessary\n",
        "    max_depth = int(max_depth)\n",
        "    min_samples_split = int(min_samples_split)\n",
        "    min_samples_leaf = int(min_samples_leaf)\n",
        "\n",
        "    # Create a DecisionTreeRegressor with the specified hyperparameters\n",
        "    model = DecisionTreeRegressor(\n",
        "        max_depth=max_depth,\n",
        "        min_samples_split=min_samples_split,\n",
        "        min_samples_leaf=min_samples_leaf\n",
        "    )\n",
        "\n",
        "    # Use cross_val_score to calculate the mean absolute error (MAE)\n",
        "    mae = -cross_val_score(model, X_train, y_train, cv=5, scoring='neg_mean_absolute_error').mean()\n",
        "\n",
        "    return mae\n",
        "\n",
        "# Define the hyperparameter search space\n",
        "pbounds = {\n",
        "    'max_depth': (1, 32),                  # Depth of the tree\n",
        "    'min_samples_split': (2, 20),         # Minimum number of samples required to split an internal node\n",
        "    'min_samples_leaf': (1, 10)           # Minimum number of samples required to be at a leaf node\n",
        "}\n",
        "\n",
        "# Create a BayesianOptimization instance\n",
        "optimizer = BayesianOptimization(\n",
        "    f=objective_function,\n",
        "    pbounds=pbounds,\n",
        "    random_state=42  # Set a random seed for reproducibility\n",
        ")\n",
        "\n",
        "# Perform the optimization\n",
        "optimizer.maximize(init_points=5, n_iter=15, acq='ucb', kappa=2)\n",
        "\n",
        "# Get the best hyperparameters\n",
        "best_hyperparameters = optimizer.max\n",
        "print(\"Best Hyperparameters:\", best_hyperparameters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "# Save the model to a file using pickle\n",
        "with open('RandomForest.pkl', 'wb') as model_file:\n",
        "    pickle.dump(model, model_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "# Load the model from the file\n",
        "with open('model.pkl', 'rb') as model_file:\n",
        "    loaded_model = pickle.load(model_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WF-qV4FPFYxj"
      },
      "source": [
        "### Production du résultat (prédiction : )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "5upj_9fJFYxj",
        "outputId": "b60323c6-46c6-490a-a446-be78986c210f"
      },
      "outputs": [],
      "source": [
        "data_test = pd.read_csv('test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NEbk0yLEFYxj"
      },
      "outputs": [],
      "source": [
        "data_test[\"IT\"].fillna(\"Missing\", inplace=True)\n",
        "data_test[\"Mp\"].fillna(\"Missing\", inplace=True)\n",
        "data_test[\"Ct\"].fillna(\"Missing\", inplace=True)\n",
        "column_to_drop = [\"z (Wh/km)\" ,\"Electric range (km)\", \"Enedc (g/km)\" ,\"Vf\" ,\"De\" ,\"Ernedc (g/km)\", \"MMS\", \"Cr\", \"r\", \"Enedc (g/km)\", \"Fm\", \"Status\", \"Date of registration\", \"T\", \"Mk\", \"Va\", \"Tan\", \"Ve\", \"Cn\"]\n",
        "\n",
        "data_test = Drop_Columns(column_to_drop, data_test)\n",
        "data_test = Impute_Missing_Value(column_to_impute_scale, data_test, strategy='median')\n",
        "#data_test = Label_Encoder(column_to_encode, data_test)\n",
        "data_test = Standard_Scaler(column_to_impute_scale, data_test)\n",
        "\n",
        "#column_to_encode = ['Country', 'Mh', 'Man', 'VFN', 'Mp', 'IT', 'Ct', 'Ft']\n",
        "#column_to_impute_scale =['m (kg)', 'W (mm)', 'ep (KW)', 'At1 (mm)', 'At2 (mm)', 'Mt', 'ec (cm3)', 'Fuel consumption ', 'Erwltp (g/km)']\n",
        "#column_to_encode =['Country', 'Mh', 'Man', 'VFN', 'Mp', 'IT', 'Ct', 'Ft']\n",
        "\n",
        "data_test_ID=data_test[\"ID\"]\n",
        "data_test = Drop_Columns([\"Country\",\"VFN\", \"Mp\", \"Mh\", \"Man\", \"Ct\",\"IT\" ,\"Ft\", \"ID\"], data_test)\n",
        "data_test = data_test[X_train.columns]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vTmvpmTdFYxk"
      },
      "outputs": [],
      "source": [
        "test_predictions = model.predict(data_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "s6xNVEm1FYxk",
        "outputId": "77d559ed-db0e-44c5-8941-482a1fffec39"
      },
      "outputs": [],
      "source": [
        "# Prepare a submission file with test predictions\n",
        "submission = pd.DataFrame({'ID': data_test_ID, 'Ewltp (g/km)': test_predictions})\n",
        "submission.to_csv('submission_352.csv', index=False)\n",
        "submission"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
